A Practical Primer on Geostatistics
By Ricardo A. Olea

Open-File Report 2009–1103
Version 1.4, December 2018

U.S. Department of the Interior
U.S. Geological Survey

U.S. Department of the Interior
RYAN K. ZINKE, Secretary
U.S. Geological Survey
James F. Reilly II, Director
U.S. Geological Survey, Reston, Virginia
First release: July 6, 2009 (ver. 1.0), online
Revised: July 10, 2009 (ver. 1.0), online
Revised: January 2010 (ver. 1.1), online
Revised: July 2017 (ver. 1.2), online
Revised: November 2017 (ver. 1.3), online
Revised: December 2018 (ver. 1.4), online

For more information on the USGS—the Federal source for science about the Earth,
its natural and living resources, natural hazards, and the environment—visit
https://www.usgs.gov/ or call 1–888–ASK–USGS (1–888–275–8747).

For an overview of USGS information products, including maps, imagery, and publications,
visit https://store.usgs.gov/.

Any use of trade, firm, or product names is for descriptive purposes only and does not imply
endorsement by the U.S. Government.

Although this information product, for the most part, is in the public domain, it also may
contain copyrighted materials as noted in the text. Permission to reproduce copyrighted items
must be secured from the copyright owner.

Suggested citation:
Olea, R.A., 2018, A practical primer on geostatistics (ver. 1.4, December 2018): U.S. Geological
Survey Open-File Report 2009–1103, 346 p., https://doi.org/10.3133/ofr20091103.

ISSN 2331-1257 (online)

A practical primer
on geostatistics
By Ricardo A. Olea
U.S. Department of the Interior
U.S. Geological Survey
Open-File Report 2009–1103
Version 1.4, December 2018

1

CONTENTS
1. Introduction
3
2. Univariate Statistics 14
3. Bivariate Statistics 53
4. SGeMS Software
67
5. Spatial Statistics
85
6. Transformations
98
7. Semivariogram
108
8. Simple Kriging
127
9. Ordinary Kriging
155
10. Universal Kriging
192
11. Block Kriging
211

12. Cokriging
225
13. Crossvalidation
240
14. Critical Review
254
15. Sequential Gaussian
266
Simulation
16. Simulated Annealing 282
17. Filter Simulation
298
18. Reliability
318
19. Navigation Charts
333
Selected Bibliography 340
Index
342
2

1. INTRODUCTION

3

THE CHALLENGE

• Most geological phenomena are extraordinarily
complex in their interrelationships and vast in their
geographical extension.
• Ordinarily, engineers and geoscientists are faced with
corporate or scientific requirements to properly
prepare geological models with measurements
involving a small fraction of the entire area or volume
of interest.
• Exact description of a system such as an oil reservoir
is neither feasible nor economically possible.
• The results are necessarily uncertain.
Note that the uncertainty is not an intrinsic property
of the systems; it is the result of incomplete knowledge
by the observer.

4

THE AIM OF GEOSTATISTICS
• The main objective of geostatistics
is the characterization of spatial
Two-dimensional example
systems that are incompletely
known, systems that are common
in geology.
• A key difference from classical
statistics is that geostatistics uses
the sampling location of every
measurement.
Unless the measurements show spatial correlation, the
application of geostatistics is pointless.
Ordinarily the need for additional knowledge goes
beyond a few points, which explains the display of results
graphically as fishnet plots, block diagrams, and maps.
0.86

0.63

Northing

0.49

?

0.32

0.51

0.77

0.45

Data

How much?

Easting

5

GEOSTATISTICAL METHODS
Geostatistics is a collection of numerical techniques
for the characterization of spatial attributes using
primarily two tools:
• probabilistic models, which are used for spatial data
in a manner similar to the way in which time-series
analysis characterizes temporal data, or
• pattern recognition techniques.
The probabilistic models are used as a way to
handle uncertainty in results away from sampling
locations, making a radical departure from alternative
approaches like inverse distance estimation methods.
6

On dealing with time-series
analysis, users frequently
concentrate their attention
on extrapolations for
making forecasts.

Attribute

Although users of
geostatistics may be
interested in extrapolation,
the methods work at their
best interpolating.

Attribute

DIFFERENCES WITH TIME SERIES

Time

0

?

Distance

This simple difference has significant methodological
implications.
7

HISTORICAL REMARKS
• As a discipline, geostatistics was firmly established
in the 1960s by the French engineer Georges
Matheron, who was interested in the appraisal of ore
reserves in mining.
• Geostatistics did not develop overnight. Like other
disciplines, it has built on previous results, many of
which were formulated with different objectives in
various fields.

8

PIONEERS
Seminal ideas conceptually related to what today we
call geostatistics or spatial statistics are found in the
work of several pioneers, including:
• 1940s: A.N. Kolmogorov in turbulent flow and N.
Wiener in stochastic processing
• 1950s: D. Krige in mining
• 1960s: B. Mathern in forestry and L.S. Gandin in
meteorology

9

CALCULATIONS
Serious applications of geostatistics require the use of
digital computers.
Although for most geostatistical techniques
rudimentary implementation from scratch is fairly
straightforward, coding programs from scratch is
recommended only as part of a practice that may help
users to gain a better grasp of the formulations.

10

SOFTWARE
For professional work, the reader should employ
software packages that have been thoroughly tested to
handle any sampling scheme, that run as efficiently as
possible, and that offer graphic capabilities for the
analysis and display of results.
This primer employs primarily the package Stanford
Geomodeling Software (SGeMS)—recently developed at
the Energy Resources Engineering Department at
Stanford University—as a way to show how to obtain
results practically.
This applied side of the primer should not be
interpreted as the notes being a manual for the use of
SGeMS. The main objective of the primer is to help the
reader gain an understanding of the fundamental
concepts and tools in geostatistics.
11

ORGANIZATION OF THE PRIMER
The chapters of greatest importance are those
covering kriging and simulation. All other materials
are peripheral and are included for better
comprehension of these main geostatistical
modeling tools.
The choice of kriging versus simulation is often a
big puzzle to the uninitiated, let alone the different
variants of both of them. Chapters 14, 18, and 19
are intended to shed light on those subjects.
The critical aspect of assessing and modeling
spatial correlation is covered in chapter 7.
Chapters 2 and 3 review relevant concepts in
classical statistics.
12

COURSE OBJECTIVES
This course offers stochastic solutions to common
problems in the characterization of complex geological
systems.
At the end of the course, participants should have:
• an understanding of the theoretical foundations of
geostatistics;
• a good grasp of its possibilities and limitations; and
• reasonable familiarity with the SGeMS software,
thus opening the possibility of practically applying
geostatistics.

13

2. UNIVARIATE
STATISTICS

14

EVERYTHING AND A PIECE
In statistics, a population is the collection of all possible
outcomes or individuals composing the complete system
of interest; for example, all people in the United States.
Populations may be hard or impossible to analyze
exhaustively. In statistics, a limited collection of
measurements is called a sample; for example, a Gallup
Poll.
Unfortunately, the term “sample” is employed with
different meanings in geology and statistics.
Geology
collection
sample

Statistics
sample
observation

The statistical usage of the term “sample” is observed in
what follows.
15

Frequency

RANDOM VARIABLE

Variate

A random variable or
variate is a quantity that
may take any of the values
within a given set with
specified relative
frequencies.

The concept is used in geostatistics extensively to
characterize a population or convey the unknown value
that an attribute may take at any spatiotemporal
location.
16

DESCRIPTIVE ANALYSIS
• A sample of an attribute ordinarily comprises several
measurements, which are best understood when
organized in some way. This is an important aspect
of statistics.
• The number of measurements in a sample is the
sample size.
• There are multiple options to make the data more
intelligible. Some of these are more convenient than
others, depending on factors such as the sample size
and the ultimate objectives of the study.

17

SAMPLE VISUALIZATION

18

FREQUENCY TABLE
Given some numerical information, if the interval
of variation of the data is divided into class
intervals—customarily of the same lengths—and
all observations are assigned to their
corresponding classes, the result is a count of
relative frequency of the classes.

19

UNCF EXAMPLE FREQUENCY TABLE
1
2
3
4
5
6
7
8
9
10
11
12
13
Total

Class
7,680-7,710
7,710-7,740
7,740-7,770
7,770-7,800
7,800-7,830
7,830-7,860
7,860-7,890
7,890-7,920
7,920-7,950
7,950-7,980
7,980-8,010
8,010-8,040
8,040-8,070

Count
1
1
1
2
5
6
10
11
13
7
7
4
2
70

Frequency, %
1.43
1.43
1.43
2.86
7.14
8.57
14.29
15.71
18.57
10.00
10.00
5.71
2.86
100.00

This example from a major oil company relates to depth in
feet to an unconformity (UNCF) in an undisclosed area.
It will be used as a common reference to graphically
illustrate other definitions in this chapter.
20

HISTOGRAM
20

Frequency, percent

16

12

8

4

0
7695

7755

7815

7875

7935

7995

8055

Depth, ft

A histogram is a graphical representation of a
frequency table.
21

CUMULATIVE FREQUENCY
Summaries based on frequency tables depend on the
selection of the class interval and origin.
Given a sample of size n, this
drawback is eliminated by
displaying each observation zi
versus the proportion of the
sample that is not larger than zi.
Each proportion is a multiple of
100/n. The vertical axis is
divided in n intervals and the
data are displayed at the center
of the corresponding interval.
Customarily, the vertical axis is scaled so that data from
a normal distribution (page 49) display as a straight line.
22

SUMMARY STATISTICS

23

SUMMARY STATISTICS
Summary statistics are complementary or alternative
descriptors to histograms and cumulative
distributions.
A statistic is a synoptic value that is calculated from
a sample of observations, which is usually, but not
necessarily, an estimator of some population
parameter.
Generally, summary statistics are subdivided into
three categories:
• Measures of location or centrality
• Measures of spread or dispersion
• Measures of shape
24

MEASURES OF LOCATION
Measures of location give an idea about the central
tendency of the data. They are:
• mean
• median
• mode

25

MEAN
The arithmetic mean or
simply the mean, m̂ , of a
sample of size n is the
additive average of all
the observations, zi:
1 n
ˆ = ∑ zi .
m
n i =1

Mean

The mean of the UNCF
sample is 7,912.2 ft.

26

MEDIAN
The median, Q2, of a
sample is the value that
evenly splits the number
of observations zi into a
lower half of smaller
observations and an
upper half of larger
measurements.
If zi is sorted by
increasing values, then

Median

z(n +1) / 2 , if n is odd,
Q2 = 
0.5 ⋅ (zn / 2 + z(n / 2 )+1 ), if n is even .
The median of the UNCF sample is 7,918 ft.
27

MODE
Mode

20

Frequency, percent

16

12

8

4

0
7695

7755

7815

7875
Depth, ft

7935

7995

8055

The mode of a sample is
the most probable or
frequent value, or,
equivalently, the center
point of the class containing
the most observations.
For the UNCF sample,
the center of the class with
the most observations is
7,935 ft.

28

ROBUSTNESS
Robustness denotes the ability of statistical methods
to work well not only under ideal conditions but also in
the presence of data problems, mild to moderate
departures from assumptions, or both.
For example, in the presence of large errors, the
median is a more robust statistic than the mean.

29

MEASURES OF SPREAD
Measures of spread provide an idea of the
dispersion of the data. The most common measures
are:
• variance
• standard deviation
• extreme values
• quantiles
• interquartile range

30

VARIANCE
The variance, σˆ 2, is the average squared dispersion
around the mean:

1 n
1 n 2
2
2
σ = ∑ (zi − m ) = ∑ zi − n ⋅ m  ,
n i =1
n  i =1

2

expressions that are commonly restricted to estimate
variances of finite populations.
When dealing with samples, the denominator is
often changed to n - 1.
Because this is a quadratic measure, it is less
robust than most other measures of spread.
The variance of the UNCF sample is 5,474 sq ft.
31

STANDARD DEVIATION
The standard deviation is
the positive square root
of the variance.
It has the advantage of
being in the same units
as the attribute.
The standard deviation
of the UNCF sample is
74.5 ft.
σˆ σˆ σˆ σˆ
According to Chebyshev’s
theorem, for any sample and t > 1, the proportion of data
that deviates from the mean m̂ at least t ⋅ σˆ is at most t −2:
1
ˆ
Prop( X − m ≥ t ⋅ σˆ ) ≤ 2
t
32

EXTREME VALUES
The extreme values
are the minimum and
the maximum.
For the UNCF
sample, the minimum
value is 7,696 ft and
the maximum value is
8,059 ft.
This measure is not
particularly robust,
especially for small samples.
33

QUANTILES
The idea of the median splitting the ranked sample
into two equal-size halves can be generalized to any
number of partitions with equal numbers of
observations. The partition boundaries are called
quantiles or fractiles. The names for the most
common quantiles are:
• Median, for 2 partitions
• Quartiles, for 4 partitions
• Deciles, for 10 partitions
• Percentiles, for 100 partitions
The number of boundaries is always one less than
the number of partitions.
34

UNCF QUARTILES

Q1 = 7,871.75 ft
Q2 = 7,918 ft
Q3 = 7,965.75 ft

Q3
Q2
Q1

Q2 coincides with the median.
35

INTERQUARTILE RANGE
The interquartile range,
iqr, is the difference
between the upper and
the lower quartiles

Q1

Q3

iqr = Q3 − Q1 ,

thus measuring the
central spread of the
data.
For the UNCF sample,
the interquartile range is
iqr
94 ft.
The interquantile range is more robust than the variance
but insensitive to values in the lower and upper tails.
36

OUTLIER
Outliers are values so markedly different from the rest of the
sample that they raise the suspicion that they may be from a
different population or that they may be in error, doubts that
frequently are hard to clarify. In any sample, outliers are
3iqr
1.5iqr
always few, if any.
mild
mild
A practical rule of thumb is to
regard any value deviating
more than 1.5 times the
interquartile range, iqr, from the
median as a mild outlier and a
value departing more than 3
times iqr as an extreme outlier.
20

Frequency, percent

16

12

8

4

82
35

81
75

81
15

80
55

79
95

79
35

78
75

78
15

77
55

76
95

0

Depth, ft

For the UNCF sample, all mild outliers seem to be legitimate
values, while the extreme outlier of 8,240 ft is an error.
37

BOX-AND-WHISKER PLOT
UNCF
8100

Max.
8000

Q3

Depth, ft

The box-and-whisker plot is a
simple graphical way to summarize
several of the statistics:
• Minimum
• Quartiles
• Maximum
• Mean
Variations in this presentation
style abound. Extremes may
exclude outliers, in which case
the outliers are individually plotted
as open circles. Extremes
sometimes are replaced by the
5th and 95th percentiles.

7900

Mean

+

Q2
Q1

7800

7700

Min.

7600

38

MEASURES OF SHAPE
The most commonly used measures of shape in
the distribution of values are:
• Coefficient of skewness
• Quartile skew coefficient
• Coefficient of kurtosis

39

COEFFICIENT OF SKEWNESS
The coefficient of skewness is a measure of asymmetry
of the histogram. It is given by:
n

∑

σ

20

B1 = -0.38

3

If : B1 < 0, the left tail is
longer;
B1 = 0 , the distribution is
symmetric;
B1 > 0 , the right tail is
longer.

16

Frequency, percent

B1 =

1
(zi − m )3
n i =1

12

8

4

0
7695

7755

7815

7875

7935

7995

8055

Depth, ft

The UNCF coefficient of skewness is -0.38.
40

QUARTILE SKEW COEFFICIENT
The quartile skew coefficient serves the same purpose as
the coefficient of skewness, but it is more robust, yet only
sensitive to the central part of the distribution. Its
definition is:

(
Q3 − Q2 ) − (Q2 − Q1 )
qs =

qs=0.02
16

Frequency, percent

iqr

20

12

If : qs < 0, the left tail is longer;
qs = 0 , the distribution is
symmetric;
qs > 0 , the right tail is longer.
The UNCF quartile skew coefficient is 0.02.
8

4

0

7695

7755

7815

7875

7935

7995

8055

Depth, ft

41

COEFFICIENT OF KURTOSIS
This statistic measures the concentration of values around
the mean. Its definition is:
Gaussian distribution

The UNCF coefficient of kurtosis
is 3.08.

0.3

f(x)

σ4
If : B2 < 3, the distribution is more
peaked than the
Gaussian distribution;
B2 = 3 , it is as peaked as
the Gaussian;
B2 > 3 , it is less peaked
than the Gaussian.

B2 = 3

0.4

0.2
0.1
0
-5

-2.5

0

2.5

5

x
20

B2=3.08
16

Frequency, percent

B2 =

0.5

1 n
4
(
)
−
z
m
∑ i
n i =1

12

8

4

0
7695

7755

7815

7875

7935

7995

8055

Depth, ft

42

MODELS

43

PROBABILITY
Probability is a measure of the likelihood that an event, A,
may occur. It is commonly denoted by Pr[A].
• The probability of an impossible event is zero, Pr[A] = 0.
It is the lowest possible probability.
• The maximum probability is Pr[A] = 1, which denotes
certainty.
• When two events A and B cannot take place
simultaneously, Pr[A or B] = Pr[A] + Pr[B].
• Frequentists claim that Pr[A] = NA/N, where N is total
number of outcomes and NA the number of outcomes of
A. The outcomes can be counted theoretically or
experimentally.
• For others, a probability is a degree of belief in A, even
if no random process is involved nor a count is possible.
44

JOINT AND CONDITIONAL PROBABILITIES
Let A and B be two random events. The probability of their
joint (simultaneous) occurrence, Pr[ A and B ] , is
Pr[ A and B ] = Pr[ A ∩ B ] = Pr[ A | B ] ⋅ Pr[B ]

where Pr[ A | B ] is the conditional probability of A given that
B has occurred.
If Pr[ A | B ] ≠ Pr [A], then the result of one event affects the
probability of the other. Such types of outcomes are said to
be statistically dependent. Otherwise, they are statistically
independent. For example:
• Heads and tails in successive flips of a fair coin are
independent events.
• Being dealt a king from a deck of cards and having two
kings on the table are dependent events.
45

BAYES’S THEOREM

This is a widely used relationship for the calculation of
conditional probabilities. Given outcomes A and B, because
.Pr [A ∩ B ] = Pr [B ∩ A] :
Pr [B | A]
Pr [A | B ] =
Pr [A]
Pr [B ]
Example
Number of balls
Suppose there are two boxes.
Box Blue Red Total
A blue ball is drawn (event B).
5
#1
20
25
What is the probability the ball
#2
12
18
30
came from box #1 (event A)?
32
23
55
• If one only knows that there are 2 boxes, Pr[A] = ½ = 0.5.
• Now, if the table is available, Pr[B|A] = 20/25 = 0.8.
• Pr[B] = 32/55 = 0.59. Hence:
0 .8
Pr [A | B ] =
0.5 = 0.69
0.59
46

PROBABILITY FUNCTIONS
Analytical functions approximating experimental
fluctuations are the alternative to numerical descriptors
and measures. They provide approximations of
general conditions. Their drawback is the loss of fine
detail in favor of simpler models.
Models approximating histograms are called
probability density functions.
Variations in the parameters of a probability density
function allow the generation of a family of distributions,
sometimes with radically different shapes.
The main subdivision is into discrete and continuous
distributions, of which the binomial and normal
distribution, respectively, are typical and common
examples.
47

BINOMIAL DISTRIBUTION

.

f (x ; 0.1, 12)

0.3
0.2
0.1
0

0 1

2 3

4

5

6 7 8

9 10 11 12

Number of successes

0.4
Probability

This is the discrete
probability density function,
f (x; p, n ), of the number of
successes in a series of
independent (Bernoulli)
trials, such as heads or
tails in coin flipping. If the
probability of success at
every trial is p, the
probability of x successes
in n independent trials is

Probability

0.4

f (x ; 0.5, 12)

0.3
0.2
0.1
0

0 1

2

3

4

5 6

7

8

9 10 11 12

Number of successes

n!
n−x
f (x; p, n ) =
p x (1 − p ) , x = 0, 1, 2, , n
x! (n − x )!

48

NORMAL DISTRIBUTION
0.5

f(x; 0,1)

0.4
0.3
0.2
0.1
0
-5

-2.5

0

x

2.5

5

The most versatile of all
continuous models is the normal
distribution, also known as the
Gaussian distribution. Its
parameters are µ and σ, which
coincide with the mean and the
standard deviation.

1
f (x; µ,σ ) =
e
σ 2π

2
(
x −µ )
−

2σ 2

,− ∞ < x < ∞

If X = log(Y) is normally distributed, Y is said to follow
a lognormal distribution. Lognormal distributions are
positively defined and positively skewed.
49

PROBABILITY FROM MODELS
x1

Prob[X ≤ x1 ] = ∫ f (x )dx
−∞

1.17

Prob[X ≤ x1 ] = F (x1 )

Examples:

Prob[X ≤ 1.17] = ∫ Normal(x;0,1)dx
−∞

= 0.88

Prob[X ≤ 1.17] = F (1.17 )
= 0.88
50

EXPECTED VALUE
Let X be a random variable having a probability distribution
f (x ) and let u (x ) be a function of x. The expected value of
u (x ) is denoted by the operator E[u (x )] and it is the
probability weighted average value of u (x ).
If X is continuous, such as temperature:
.
.

E[u (x )] = ∫ u (x )f (x )dx,
∞

−∞

and if it is discrete, like in coin flipping:

E[u (x )] = ∑ u (x )f (x ) .
x

In the latter case, for the trivial example of u (x ) = x, if all
values are equally probable, the expected value turns into
1
E[x ] = ∑ x,
n x
which is exactly the definition of the mean.
51

MOMENT
Moment is the name given to the expected value
when the function of the random variable, if it exists,
k
takes the form (x − a ) , where k is an integer larger
than zero, called the order.
If a is the mean, then the moment is a central
moment.
The central moment of order 2 is the variance. For
an equally probable discrete case,
2 1 n
2
M 2 = σ = ∑ (x i − m )
n i =1
52

3. BIVARIATE STATISTICS

53

TOOLS
Frequently there is interest in comparing two or more
measurements made for the same object or site.
Among the most common classical alternatives, we
have:
• Scatterplot
• Correlation coefficient
• Regression
• Quantile-quantile plot
• Probability-probability plot
Some of these concepts can be generalized to deal
with more than two variables.
54

SCATTERPLOT
A bivariate scatterplot is
a Cartesian posting in
which the abscissa and
the ordinate are any two
variables consistently
measured for a series of
objects.
Scatterplots are
prepared for exploring
or revealing form,
direction, and strength
of association between
two attributes.

Mecklenburg Bay sea floor, Germany

55

COVARIANCE
The covariance is a measure of joint variation.
Given two random variables X and Y with means m x and
mY , their covariance is the expected value:
.

Cov X ,Y  E X  m X Y  mY 
The covariance estimator when using a sample of point
measurements is:
n
n
1 n
1
Côv X ,Y 
xi  y i 
xi   y i


n  1 i 1
n  n  1 i 1
i 1

56

CORRELATION COEFFICIENT
This coefficient is the number most
commonly used to summarize bivariate
comparisons. If σ X and σ Y are the standard
deviations for two variables, their correlation
coefficient, ρ, is given by:

Cov X ,Y
ρ=
σ X ⋅σY

• It only makes sense to employ ρ for
assessing linear associations.
• ρ varies continuously from -1 to 1:
1, perfect direct linear correlation
0, no linear correlation
-1, perfectly inverse correlation

1

0

-0.5

-1

57

REGRESSION
Regression is a method for establishing analytical
dependency of one or more variables on another
mainly to determine the degree of their dependency,
to estimate values not included in the sample, or to
summarize the sample.
• The variable in the abscissa is called the
regressor, independent, or explanatory variable.
• The variable in the ordinate is the regressed,
dependent, or response variable.
• In many studies, which variable goes into which
axis is an arbitrary decision, but the result is
different. Causality or the physics of the process
may help in resolving the indetermination.
58

REGRESSION MODEL
• The model is:

Response variable (y )

y i = f (x i ; θ ) + ε i

• f (x i ; θ ) is any continuous
function of x that is judiciously
selected by the user. θ are
εi
unknown parameters.
• Term ε is a random variable
f (x ; θ )
accounting for the error.
• Having selected f (x i ; θ ),
parameters θ are calculated by
Explanatory variable (x)
minimizing total error, for which
there are several methods.
Avoid applying the model outside the extreme values
of the explanatory variable.
i

59

LINEAR REGRESSION

The simplest regression
case is one in which the
parameters are estimated
by minimizing the mean
square error, r 2, associated
with a linear model,

Mecklenburg Bay seafloor, Germany

ρ = 0.94

1 n 2
r = ∑εi
n i =1
2

In this special situation, ρ 2
accounts for the proportion
of variation accounted for by
the regression.
In the example, ρ = 0.94.
Hence, in this case, the
linear regression explains 88% (100 ⋅ ρ 2 ) of the variation.
60

NONLINEAR REGRESSION
Linear

r 2 = 25.47

Cubic polynomial

r 2 = 20.33

Quadratic polynomial

r 2 = 20.56

Sixth degree polynomial

r 2 = 19.98

• In theory, the
higher the
polynomial degree,
the better the fit.
• In practice, the
higher the
polynomial, the less
robust the solution.
• Overfitting may
capture noise and
not systematic
variation.
61

IMPLICATIONS
Countries with a population of
more than 20 million in 1990

ρ = -0.81

80

Life expectancy, year

75
70
65
60
55
50
1

10

100

Persons per television set

1000

High to good correlation:
• allows prediction of one
variable when only the other
is known, but the inference
may be inaccurate,
particularly if the correlation
coefficient is low;
• means the variables are
related, but the association
may be caused by a
common link to a third
lurking variable, making the
relationship meaningless;
• does not necessarily imply
cause and effect.
62

QUANTILE-QUANTILE PLOT
• A quantile-quantile or Q-Q plot is a
scatterplot based on ranked data.
• The pairing is independent of the
object or site where the
observations were taken. The first
pair of coordinates has the minimum
value for each attribute, the second
pair is made of the second smallest
readings, and so on until finishing
with the two maximum values.
Interpolations are necessary for
different size samples.
• Q-Q plots are sensitive to a shift and
scaling of the distributions.
63

STANDARDIZED VARIATE
If X is a random variable
with mean µ and
standard deviation σ,
the standardized
variate, Z, is the
transformation:

Z=

Z=

X −µ

σ

X −µ

σ

A standardized variate always has a mean of zero and
a variance of one.
A standardized Gaussian distribution is a called a
standard normal distribution. It is often denoted by N (0,1).
64

PROBABILITY-PROBABILITY (P-P) PLOT

Density
Velocity

A P-P plot is another scatterplot prepared by extracting
information from the cumulative distributions of two variates.
• If the variates are in different units, preliminary
standardization is necessary.
• For given thresholds, the axes show the cumulative
probabilities for the two distributions being compared.
65

Q-Q AND P-P PLOTS
Main use of the Q-Q and P-P plots is
as a quick way to decide the degree of
similarity between two distributions.
• If the distributions are the same, the
points align along the main diagonal.
• There is no statistic or level of
significance for evaluating the results.
• P-P plots are insensitive to shifting
and scaling, and the vertical scale is in
probability units.
• The Q-Q plot in the example illustrates
its potential for calling the user’s
attention to the fact that normal
distribution with the same parameters
can take negative values.

West Lyons field, Kansas

Q-Q

P-P

66

4. SGeMS SOFTWARE

67

GENERAL DESCRIPTION
The Stanford Geostatistical Modeling Software (SGeMS)
is a general-purpose, user-friendly, state-of-the-art
geostatistical software package.
The software code is in the public domain,
downloadable from http://sgems.sourceforge.net/
(accessed in October 2017).
The code is in C++ and runs interactively under
Windows.
A manual published by Cambridge University Press is
listed in the selected bibliography.
SGeMS is an outgrowth of the Stanford Geostatistical
Library (GSLIB), with which it shares several features.
For details about GSLIB, please consult the book listed
in the bibliography.
68

MAIN MENU

69

OBJECTS
Objects are files with numerical information that can
be of two types:
A. Data, when each record specifies location.
B. Grid, when locations are in a regular rectangular
(usually square) or parallelepipedal (usually cubic)
pattern. Location is implicit and specified in a
separate file. A grid file contains attribute values only.
Typically they are the result of some calculation, but
they can be used as input for subsequent processing.
Evidently, no process is possible without prior
loading of an object, which is done by:
• clicking on <Objects> in the upper left corner of
main menu, or
• dragging the file icon into the Object area of the
main menu.
70

A. DATA FILES
Data may be up to three-dimensional and have to be
coded into plain ASCII files. No special formatting is
necessary, yet they must observe the following
structure:
• The first record is intended to be a header to identify
the data. It is often automatically used to label the
output.
• The second record must be the number of fields per
data entry.
• Starting with the third record, there should be one
record per attribute containing a short description. A
logical choice is to provide attribute type and unit of
measurement, if any.
71

DATA FILES
• If the file has v attributes, the numerical values for the
sample start at record v + 3, one for each site, under
the following restrictions:
o The entries must be strictly numeric. No letters or
special symbols are allowed.
o There must be exactly as many columns as
indicated in the second record. A special code
chosen by the user, such as -999, should be used
if there are missing values.
o It is required to provide at least two Cartesian
coordinates specifying location.
• The number of records is unlimited.
72

EXAMPLE OF DATA FILE
UNCF unconformity
4
Identification
Easting, m
Northing, m
Depth, ft
1
2
3
6200
6202

32000
38600
44000
85050
49400



87015
87030
86400

7888
8020
7949

87000
84000

8003
8032

73

DATA LOADING
Simply:
• Drag the file icon into the object area of the main
menu
or:
• Click on <Objects>.
• Click on <Load Object> at new menu.
• Search for the file of interest.
• Highlight file name and click on <OK>.
The top of the file will be displayed. Finish by:
• Choosing <Point Type> at <Select object type>.
• Clicking on <Next>.
74

1. COMPLETING DATA LOADING
• Type an object name at
<Pointset name>.
• Indicate columns for
easting and northing
and also depth in threedimensional studies.
• Specify missing data
code, if any.
• Click on <Finish>.
The loading has been
completed and the data
file will show in the object
panel of the main menu.
75

B. GRID LAYOUT
134 135

1

2

Results from modeling of
spatial attributes come in
the form of arrays of
values at regular
intervals.
Coordinates for the nodes
are not printed; they are
implicit in the regular
arrangement.

76

IMPLICIT ARRANGEMENT
• In three-dimensional modeling, layer values go from
the lowermost layer to the uppermost layer.
• Within a layer, the first value to go into the file is the
one in the lower left corner.
• Then come the rest of the values for the lowermost
row, from left to right.
• Within a layer, last value to enter the file is the one
in the extreme upper right corner.
• Specification of grid geometry goes into a separate
file.

77

GRID SPECIFICATION FILE
Remember that grids
must be specified
before attempting
any processing that
will result in the
generation of a grid,
such as estimation
or simulation.
For specifying a
grid, click <Object>
and then select
<New Cartesian
Grid>.
78

EXAMPLE OF GRID
To minimize storage space, SGeMS stores grid values in
binary form. Grids can only be displayed in graphical form.
Numerical display of a grid requires saving it in ASCII
format and further viewing it with a text editor. To do such
saving:
• Click on <Object>.
• Select <Save Object>.
• Name the file.
• Select place for storage
at <Look in>.
• Make sure to select
GSLIB as the format.
• Click on <Save>.
79

VIEW OF GRID WITH TEXT EDITOR
UNCFgrid (211x281x1)
2
OK
OK_krig_var
7773.04 2075.6
7766.52 1902.38
7763.12 1745.86
7759.76 1595.19
7756.46 1450.97
..
.
8017.93 2229.09
8018.47 2313.04
8018.98 2404.05
8019.46 2501.91
8019.92 2606.35

SGeMS automatically
creates the first record,
which includes the
number of columns,
rows, and layers.

80

OBJECT SAVE
In SGeMS, all objects in a
session make up a project.
The user can save all
objects in a project for
archiving or further use.
To do that:
• Click on <File> at the upper right corner of the
SGeMS screen;
• Click on <Save Project>. This is all that it is
necessary to do for an existing project.
• If the project is new, navigate directories to locate a
storage folder, select name for the project file, and
click on <Save>.
81

HISTOGRAM PREPARATION
As a first practical application of SGeMS, let us prepare
a histogram of the object previously loaded. For that:
•
•
•
•
•
•

Click on <Data Analysis> at main menu.
In the new menu, click on <Histogram>.
Select the object.
Choose the property.
Set the number of classes.
If there is a need to change the axis extreme values,
click on <Display Option> and enter the settings.

82

HISTOGRAM DISPLAY

83

SAVING IMAGE
• Click on <Save as
Image>.
• Select a folder.
• Give a name to the
file.
• Select type.
• Decide on grid and
summary statistics.
• Click on <Save>.
The histogram graphical file is now at the specified folder.

84

5. SPATIAL STATISTICS

85

RANDOM FUNCTIONS

t
s

A random function is a collection of random variables,
one per site of interest in the sampling space.
A realization is the set of values that arises after
obtaining one outcome for every distribution.
Geostatistics relies heavily on random functions to
model uncertainty.
86

MAIN DIFFERENCES BETWEEN CLASSICAL
GEOSTATISTICS AND CLASSICAL STATISTICS
• Geographical location is an integral part of any sample.
• Measurements have an associated volume called support.
For example, in the case of density, the support is the
specimen volume; a depth measurement has a point support.
• Geostatistics does not assume that the variables are
independent and identically distributed.
• A spatial sample is regarded as a single realization of a
random function, instead of multiple outcomes of a single
random variable.
• Most geostatistical formulations do not require any particular
probability distribution, although some work better under
normality conditions.
• The most efficient forms of spatial sampling are those
following a regular pattern—cubic or hexagonal, for example.
87

ERGODIC ASSUMPTION
We have seen that stochastic methods regard data as
one of an infinite number of possible realizations.
Ordinarily, one needs multiple realizations to infer
properties of the complete ensemble.
The ergodic assumption basically states that just
one realization is indeed sufficient to make reliable
assessments about ensemble properties.
It is an assumption because it is not possible to test it.

88

STATIONARITY
This is another fundamental assumption, presuming
invariance through space of properties of the random
function.
The most general case assumes that the joint
probability distribution of any number of random
variables is the same.
Most geostatistical formulations require
independence of location only for moments and just
up to order two.

89

BIAS
Bias in sampling denotes preference in taking the
measurements.
Bias in an estimator implies that the calculations are
preferentially loaded relative to the true value, either
systematically too high or too low.
• When undetected or not compensated, bias induces
erroneous results.
• The opposite of the quality of being biased is to be
unbiased.

90

PREFERENTIAL SAMPLING
Although estimation and simulation methods are
robust to clustered preferential sampling, parameters
that need to be inferred from the same sample prior
to estimation or simulation can be seriously distorted,
especially for small samples.
The solution to preferential sampling is preparation
of a compensated sample to eliminate the clustering,
for which there are several methods.
Declustering is important for the inference of global
parameters, such as any of those associated with the
histogram.

91

EXAMPLE OF SAMPLE WITH
PREFERENTIAL CLUSTERING

92

DECLUSTERING (1)

Detect presence of clusters by preparing a cumulative
distribution of distance to nearest neighbor.
93

DECLUSTERING (2)

S1

S2

Decompose the clustering.
94

DECLUSTERING (3)
Histograms of the attribute by distance class

Preferential sampling shows as poor overlapping
between the two histograms.
95

S1

DECLUSTERING (4)
One possibility is to obtain the declustered subset (S4) by
expanding the subset without clusters (S1) by transferring
a few observations (S3) from the
clustered subset (S2).

S3

• Transfer observations from
(S2) by decreasing distance to
nearest neighbor in S4.
• Stop transferring points when
the distribution of distances for
the original subset S1 is about
the same as that for the
transferred observations (S3)
within S4.
96

POSTING OF A
DECLUSTERED SAMPLE (S4)
(S3)

(S3)

(S1)

(S1)

(S1)

Best results are obtained by randomly repeating the
drawing from S2 to generate multiple declustered sets.
The final answer is obtained by taking median values
from the cumulative distributions of all declustered sets.

97

6. TRANSFORMATIONS

98

COMPARING DISTRIBUTIONS

The distribution followed by some data and the one that may
be required by a method may be two different distributions.
If a method requires a particular distribution, such
distribution becomes
mandatory.
The simplest way to
compare a sample, Fn (x ),
to an hypothesized
distribution, F (x ), is to
superimpose their
cumulative distributions.
By transforming the
data, one can force the
data to follow almost any
other distribution.
99

KOLMOGOROV-SMIRNOV TEST
The maximum discrepancy, D, is the measure of similarity in
the Kolmogorov-Smirnov test.
The significance of the discrepancy depends on the sample
size and on whether the parameters of the hypothesized
distribution are known. A statistic called p-value, commonly
read from tables, is used to decide the significance of the
discrepancy. Customarily, if the p-value is above 0.1 or 0.05,
the equality of the distributions is accepted.
Standard tables available to run the significance of the
discrepancy are not valid in geostatistics because the tables
do not take into account spatial dependence.
It has been found, however, that the standard tables may
be a limiting case for the situation considering spatial
correlation. In that case, each time two distributions pass the
Kolmogorov-Smirnov test without considering spatial
correlation, they also pass it considering that correlation.
The same is not true in case of rejection.
100

WEST LYONS FIELD, KANSAS
Remember that the cumulative
D = 0.090
p = 0.14
probability of any normal
distribution follows a straight
line in normal probability scale.
At 10 ft, the thickness
distribution of Lyons field
reaches a maximum
discrepancy of 0.09 against a
normal distribution with the
same mean and standard deviation as the sample. The
p-value without considering spatial correlation, pu, is 0.14.
Because the pu-value is already above the threshold of
0.05–0.1, it is not necessary to go into the complications of
considering spatial correlation. Without further ado, it can
be accepted that the thickness is normally distributed.
u

101

NORMAL SCORES
In geostatistics, as in classical statistics, the most
versatile distribution is the normal distribution.
If the sample strongly deviates from normality, it may
be convenient or required to transform the data to follow
a normal distribution, so as to properly apply a specific
method.
The transformation is fairly straightforward. It is
performed by assigning to the cumulative probability of
every observation the value of the standard normal
distribution for the same cumulative probability.

102

NORMAL SCORES
For the value zi of
the attribute, the
normal score is 1.45.
The transformation
also can be used
backwards to bring
calculated values in
the normal score
space to the original
attribute space.

103

LOGNORMAL TRANSFORMATION
• This is a special case of normal score transformation.
The transformed value is simply the logarithm of the
original observation.
• If the original distribution is exactly lognormal, the
distribution of the transformed values will be exactly
normal. Because this is never the case, the
distribution for a transformed variable is only
approximately normal.
• Most minerals and chemical elements have
distributions of concentration close to lognormal,
which explains the great attention paid to this
distribution since the early days of geostatistics.
104

INDICATORS
Given a continuous random function Z (s ) , where s
denotes geographical location, its indicator I (s, u ) is
the binary transformation:

0, if Z (s ) > u
I (s, u ) = 
1, if Z (s ) ≤ u

The potential of indicators lies in the possibility to
handle imprecise data and the fact that the expected
value of an indicator is equal to the cumulative
probability for the threshold value:

E [I (s, u | (n ))] = Prob[Z (s ) ≤ u | (n )]
= F (s, u | (n ))

105

INDICATOR CODING FOR DIFFERENT
TYPES OF CONTINUOUS DATA

Courtesy of A. Journel, Stanford University

106

DISCRETE INDICATORS
The indicator I (u, k ) of a categorical attribute is:
1, if u ∈ k
I (u, k ) = 
0, otherwise
In this case, the indicator is now equal to the
probability of belonging to class k.
E [I (u, k | (n ))] = Prob[u ∈ k | (n )]
In this case, the indicator transformation offers the
possibility of digital processing for information that is
commonly qualitative.

107

7. SEMIVARIOGRAM

108

THE OBJECTIVE

The objective of structural
analysis is to capture the
style of the fluctuations
through an analytical
function.
109

CONTINUITY
Spatial continuity involves the concept that small
values of an attribute are in geographical proximity
to other small values, while high values are close to
other high values. Transitions are gradual.
Assessment of covariance or its close equivalent,
the semivariogram, has been the classical way in
geostatistics to measure spatial correlation.

110

RELEVANCE
•
•

The semivariogram or the covariance are
necessary in the formulation and application of
most estimation and simulation methods.
Semivariogram analysis leads to some standalone applications, such as sampling design.

111

DEFINITIONS

Given a spatial distance h involving magnitude and
direction, the semivariogram γ (h) is:

1
γ (h) = Var [Z (s ) − Z (s + h)]
2

and the spatial covariance is:
Cov (h) = E[(Z (s ) − m )(Z (s + h) − m )] ,
both with a constant mean m

m = E[Z (s )] = E[Z (s + h)].

Different from the classical covariance (page 56), the
spatial covariance is grouped by distance and applies to
the same attribute. Therefore, a more descriptive yet
seldom used term for the covariance is autocovariance.
112

EQUIVALENCE

Second order moment

If the mean is constant and
the covariance is
independent of location,
Cov(0)
then always
γ (h) = Cov (0 ) − Cov (h),
which makes it immaterial
Semivariogram
which one to use.
Covariance
In general, what is
estimated is the
semivariogram because its
0
estimation does not require
a
Lag
knowledge of the mean.
Lag a is the range and the semivariogram asymptote
is called the sill, which is equal to the variance Cov(0).
113

SEMIVARIOGRAM ESTIMATOR
1 n (h )
2
[
]
(
)
(
)
γˆ(h) =
z
z
s
s
h
−
+
∑ i
i
2n (h) i =1

γˆ (h) : estimated or experimental semivariogram;
h : lag;
n (h) : number of pairs h units apart;
z(s i ) : observation at site s i .
Always remember that for the estimator to be valid, the
mean must be constant; hence, there must be no trend.
For any direction, it is recommended that n (h) ≥ 30 and
the estimation be restricted to h no more than half the
extension of the sampling domain in such direction.
114

TRIVIAL EXAMPLE
si

z (s i )

∆20

∆20.5

∆21.0

∆21.5

∆22.0

∆22.5

∆23.0

∆23.5

∆24.0

∆24.5

7.0
7.5
8.0
8.5
9.0
9.5
10.0
10.5
11.0
11.5

3.2
4.3
5.0
6.5
7.9
8.1
7.5
7.3
6.7
5.8

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

1.21
0.49
2.25
1.96
0.04
0.36
0.04
0.36
0.81

3.24
4.84
8.41
2.36
0.16
0.64
0.64
2.25

10.89
12.96
9.61
1.0
0.36
1.96
2.89

22.09
14.44
6.25
0.64
1.44
5.29

24.01
10.24
5.29
0.04
4.41

18.49
9.00
2.89
0.49

16.81
5.76
0.64

12.25
2.25

3.76

0.0

7.52

22.74

38.97

50.15

43.99

30.87

23.21

14.50

6.76

10

9

8

7

6

5

4

3

2

1

0.0

0.42

1.42

2.78

4.18

4.40

3.85

3.86

3.62

3.38

∑∆

2
h

n (h)
λ (h)

5

∆ 1 .0

∆ 1 .5

Semivariogram

∆ 0 .5

4
3
2
1
0
0

1

2

3
Lag

4

5

115

CASE OF IRREGULAR PATTERNS
If the observations, z (s ), are not regularly spaced, they are
grouped into distance classes of equal radial thickness,
which customarily is set equal to twice t h .
Commonly used
s
parameters are:
t h : lag tolerance
δ : angular tolerance
t b : bandwidth
s
t b only operates when the
cone is wider than twice t b .
Otherwise, δ prevails.
Easting
Each class contributes one value to the experimental

semivariogram, γ (h).
Northing

i

j

116

MOST COMMON PERMISSIBLE MODELS

Permissible semivariogram models are a class of models
guaranteeing a unique solution to an estimation system of
equations and a nonnegative estimation variance.
Spherical

  3 h 1  h 3 
C 
    , if h  a
 h    2 a 2  a  

C , if h  a

Exponential
 h  C[1  e

h
3  
a

Gaussian

 h  C[1  e

a

]

h
3  
a

2

]

A sum of permissible models is also permissible.
117

ASSESSMENT OF SPATIAL
CORRELATION
Proof of spatial correlation

Semivariogram

Semivariogram

Spatially correlated attributes have
gradually increasing semivariograms that
reach the range at a distance clearly
Nugget
greater than zero.
{
effect
Lag
If the semivariogram does not seem to
be zero for a lag close to zero, the value
Lack of spatial correlation
of convergence is called nugget effect.
Yet,  (0 ) = 0 .
Lack of spatial correlation is revealed by
a semivariogram that remains constant,
following what is called a pure nuggetLag
effect model, which is permissible.
Mere assessment of spatial correlation does not require
semivariogram modeling.
118

ESTIMATION BASIC STEPS
A. Make sure that the data are error free.
B. Correct preferential clustering, if any.
C. Perform transformations if required or desired.
D. Estimate semivariograms along at least 3 different
directions.
E. Detect trend. In the presence of trend, the
experimental semivariogram is an artifact that
increases monotonically, reaching values orders of
magnitude higher than the sample variance.
F. If necessary, model semivariogram along the trendfree direction.
When the attribute is isotropic, parameters do not change
with h. A semivariogram model prepared disregarding
azimuthal variation is said to be omnidirectional.
119

A. DATA LOADING

If data have not yet been loaded into SGeMS, load
them as explained in chapter 4 (page 74).
120

B. MODELING WITH SGeMS

First click on <Data Analysis> and then on <Variogram>.
See page 187 for more information about this dataset.
121

C. SELECT FOLDER AND ATTRIBUTE

• Select <Head Property> equal to <Tail Property>.
• After making selections, click on <Next>.
122

D. DIRECTIONS AND LAGS
For grids, the azimuth is specified by directional cosines.
For example, north in a horizontal place is (0, 1, 0).
For datasets, azimuth is specified as clockwise degrees
from the north; so, for example, NE is 45o.
Upon
specifying
lags and
directions,
click on
<Next>.

123

E – F. TREND DETECTION AND MODELING
• Hide the main
menu screen;
• expand sideways
semivariogram
menu;
• click on
<Window>, and
then click on
<Tile>;
• pick best fitting
model parameters
by trial and error.
Here there is no trend because the semivariogram tends
to stabilize at a value close to the sample variance. See
pages 202–203 for modeling of a nonstationary dataset.

124

SAVING SEMIVARIOGRAMS
•
•
•
•
•
•
•

Click on the <File> option of semivariogram modeling.
Pick an option, for example, <Export Plot as Images>.
Check <Show Grid> if it is desired to display it.
Click on the three dots <…>.
Select storage folder.
Give generic name to semivariograms.
Click on <Save> in both menus.

125

OTHER DIRECT APPLICATIONS

Semivariogram

Direct applications of the semivariogram, not necessarily
requiring to go through modeling, include:
• Detection of anisotropy by
finding out if the
semivariogram is different
along different directions.
• Comparative studies of
origins in the processes
behind attributes based on
the premise that different
geneses lead to different
semivariograms.
• Sampling design. In any direction, sampling interval
should not exceed half the range.
Direction 1
Direction 2

Direction 3
Direction 4

Lag

126

8. SIMPLE KRIGING

127

KRIGING
Kriging is a form of generalized
s
s
linear regression for the
s
formulation of an optimal spatial
estimator in a minimum means
square-error sense.
s
s
Given some observations at
s
s
locations si, kriging provides an
estimate and a standard error at
s
locations, such as s0, typically
not considered in the sampling.
In contrast to classical linear regression, kriging takes
into account observation volume and stochastic
dependence among data.
The method works best inside the convex hull determined
by the peripheral data.
1

2

3

0

4

5

6

7

8

128

VARIANTS
There are several forms of kriging, each making
different assumptions to adjust to different styles of
attribute fluctuations.
• Simple kriging
• Ordinary kriging
• Universal kriging
• Lognormal kriging
• Indicator kriging
The last two types are simply any of the first three
forms of kriging applied to transformed data as
explained in chapter 6.
129

SIMPLE KRIGING
Simple kriging is the estimation of
*
SK

z

where:

n

(s0 ) = m + ∑ λi ⋅ (z(si ) − m ),
i =1

*
(s0 ) is the estimator at geographical location s0 ;
zSK
s i denotes the location of measurement i ;
n is the number of observations to consider;
m is the mean of Z (s );
λi is a real number, commonly referred to as weight.

130

NORMAL EQUATIONS
The originality of kriging is the use of weights, λi , such
that they minimize the estimation error variance. Such
weights turn out to be the solution of the following system
of equations, called the normal system of equations:


Cov (s0 , s1 )
i =1

k

(
)
(
)
Cov
s
,
s
Cov
s
,
s
λ
=
∑
i
i
2
0
2

i =1
  
k

λi Cov (s i , s k ) = Cov (s0 , s k )
∑

i =1
k

∑ λi Cov (si , s1 )

=

131

DERIVATION OF NORMAL EQUATIONS
The steps are:
A. Make necessary assumptions.
B. Calculate error variance.
C. Express error variance in terms of covariances.
D. Find the weights that minimize the error variance.
E. Calculate the minimum error variance.

132

A. ASSUMPTIONS

1. The sample is a partial realization of a random
function Z (s ) , where s denotes spatial location.
2. Z (s ) is second order stationary, which implies:

E[Z (s )] = m
E[(Z (s ) − m )(Z (s + h) − m )] = Cov (h),
where E[]
⋅ denotes expected value, m is a scalar
constant, and h is a vectorial distance.

3. Unique to simple kriging is the assumption that the
mean is not only constant but known.
These assumptions make simple kriging the most
restricted form of kriging.
The weakness of this simplicity is the same as with any
simple model: limitation in its applicability or suboptimal
results if its use is forced.
133

B. ESTIMATION VARIANCE
By definition, the estimation variance is:
*
(s0 ) − Z (s0 )].
σ 2 (s0 ) = Var [ZSK

We know neither the random function Z (s 0 ) nor the
*
(s0 ) , so we cannot directly calculate the
estimator ZSK
variance of the difference.
The way to solve this conundrum is to transform the
expression for the variance to have it in terms of
quantities that we know.

134

C. ALGEBRAIC MANIPULATIONS (1)
The residual Y s of a random function is the difference
between the random function and its expected value. In
this case:
Y s  Z s  EZ s  Z s  m
Expressing this estimation variance in terms of the
residuals, we have:
k
k




2
 s0   Var  iY si   Y s0   Var  iY si 
 i 1

 i 0

if one defines 0  1.

135

ALGEBRAIC MANIPULATIONS (2)
From the properties of the variance (pages 31 and 52):
2
2
k
k


 



2
σ (s0 ) = E ∑ λiY (s i )  − E ∑ λiY (s i )  .

    i =0
 i =0

Expanding the squares and introducing the expectations
inside the summations:
σ (s0 ) = ∑∑ λi λ j E[Y (s i )Y (s j )]
2

k

k

i =0 j =0

− ∑∑ λi λ j E[Y (s i )]E[Y (s j )].
k

k

i =0 j =0

136

ALGEBRAIC MANIPULATIONS (3)
Grouping the terms by coefficients:

σ (s0 ) = ∑∑ λi λ j {E[Y (s i )Y (s j )] − E[Y (s i )]E[Y (s j )]}.
k

2

k

i =0 j =0

Backtransforming to the random function and expanding:
σ (s0 ) = ∑∑ λi λ j {E[m 2 − mZ (s i ) − mZ (s j ) + Z (s i )Z (s j )]
k

2

k

i =0 j =0

}

− m 2 + m E[Z (s i )] + m E[Z (s j )] − E[Z (s i )]E[Z (s j )] .

Because of the cancelation of terms,

σ (s0 ) = ∑∑ λi λ j {E[Z (s i )Z (s j )]− E[Z (s i )]E[Z (s j )]},
2

k

k

i =0 j =0

137

ALGEBRAIC MANIPULATIONS (4)
which is the same as:
k

k

 2 s0    i  j Covsi , s j .
i 0 j 0

Finally, remembering that 0  1:
k

 s0   Covs0 , s0   2 i Covs0 , si 
2

i 1

k

k

  i  j Covsi , s j ,
i 1 j 1

which is an expression in terms of covariance, a
measurable property, hence a useful expression.
138

D. OPTIMAL WEIGHTS (1)
The expression for σ 2 (s0 ) is valid for any weights. Among

the infinite weight combinations, we are interested in those
that minimize the estimation variance associated with the
*
(s0 ).
estimator ZSK
The expression for σ 2 (s0 ) is quadratic in the unknowns,
the weights. Under these circumstances, according to an
operations research theorem, the necessary and sufficient
condition to have a unique global nonnegative minimum is
that the quadratic term must be larger than or equal to zero.
This implies that the covariance function must be such that
any combination, like the one in the general expression for
σ 2 (s0 ), cannot be negative. In mathematics, such is the
property of being positive definite. This is the condition a
covariance model has to satisfy to be permissible.
.

139

OPTIMAL WEIGHTS (2)
The optimal weights are those that make zero all partial
derivatives of σ 2 (s0 ) with respect to the weights:
k
∂σ 2 (s0 )
= −2 Cov (s0 , s j ) + 2∑ λi Cov (s i , s j ) = 0
∂λ j
i =1
for all locations j , j = 1, 2, , k.
Cancellation of factor 2 and rearrangement give:


λi Cov (s i , s1 ) = Cov (s0 , s1 )
∑
i =1

k
λi Cov (s i , s 2 ) = Cov (s0 , s 2 )
∑

i =1
  
k

λi Cov (s i , s k ) = Cov (s0 , s k )
∑

i =1
k

140

E. OPTIMAL ESTIMATION VARIANCE
Multiplying each optimality condition by λi and adding
them together, we have a new optimality relationship:
k

k

k

∑ λ ∑ λ Cov (s , s ) = ∑ λ Cov (s , s ).
i =1

i

j =1

j

i

j

i =1

i

0

i

Introducing λi under the second summation and
replacing the resulting double summation in the general
expression for σ 2 (s0 ) :

σ

2
SK

k

(s0 ) = Cov (s0 , s0 ) − ∑ λi Cov (s0 , si ).
i =1

141

SUMMARY
Simple kriging normal equations for optimal weights λi :

Cov (s0 , s1 )
i =1

k
λi Cov (s i , s 2 ) = Cov (s0 , s 2 )
∑

i =1
  
k

λi Cov (s i , s k ) = Cov (s0 , s k )
∑

i =1
k

∑ λi Cov (si , s1 )

=

Simple kriging estimation:
*
SK

z

n

(s0 ) = m + ∑ λi (z(si ) − m )
i =1

Simple kriging mean-square-error
variance:
k
2
(s0 ) = Cov (s0 , s0 ) − ∑ λi Cov (s0 , si )
σ SK
i =1

142

MATRICES
A matrix is a rectangular
array of numbers, such as A.
When n = m, A is a square
a11 a12  a1m 
matrix of order n.

a
a

a
22
2m 
Transposing all rows and
A =  21
    
columns in A is denoted as AT .


a
a

a
nm 
 n1 n 2
Matrices are a convenient
notation heavily used in
T
B = [b1 b2  bm ]
dealing with large systems of
T
linear equations, which
X = [x1 x 2  x m ]
notably reduce in size to just
A X = B or X = A-1B, where
A-1 denotes the inverse of matrix A.
The main diagonal of a square matrix is the sequence
of elements a11, a22, …, ann from upper left to lower right.
143

SIMPLE KRIGING MATRIX FORMULATION
 Cov (s1, s1 )  Cov (s k , s1 )  λ1   Cov (s0 , s1 ) 

   = 






  

Cov (s1, s k )  Cov (s k , s k ) λk  Cov (s0 , sK )
 Z (s1 ) − m   λ1 
*
(s0 ) = m +      
ZSK
Z (s k ) − m  λk 
T

 Cov (s0 , s1 )  λ1 
 
2
(s0 ) = Cov (s0 , s0 ) − 
σ SK

  
Cov (s0 , s k ) λk 
T

144

EXERCISE

300

2

SAMPLE

Northing

250

Index
1
2
3
4
?

200
?

150

3

4

100
50

1

Northing
20
280
130
120
120

Attribute
40
130
90
160

m=110

0
0

Easting
10
30
250
360
180

100

200

300

400

Cov (h) = 2000e −h / 250

Easting

145

EXERCISE
For the previous configuration, using simple kriging:
A. Discuss the weight values.
B. Find the estimate.
C. Compute the estimation variance.
D. Calculate the estimation weights.

146

A. CALCULATION OF WEIGHTS (1)
Distance matrix among observations:
 0

260.8

0


264.0 266.3

0


364
.
0
366
.
7
110
.
4
0



Distance from estimation location to each observation:

[197.2 219.3 70.7 180.0]T

147

CALCULATION OF WEIGHTS (2)
Weights:
 2000

704.8 2000



695.6 689.4 2000



466
.
4
461
.
2
1285
.
8
2000



−1

 908.7   0.185 
 831.8   0.128 


=
1507.2  0.646 


 
973
.
6
0
.
001
−


 

148

B. REMARKS

300

2
0.128

Northing

250
200
150

3

4

100

0.646

-0.001

50

1
0.185

0
0

100

200
Easting

300

400

• Weight 4 is almost zero
despite being closer to
the estimation location
than weight 1. This
reduction known as
screen effect is caused
by the intermediate
location of weight 3.
• Weight 4 is negative.
• Negative weights open
the possibility to have
estimates outside the
extreme values of the
data (nonconvexity).
149

C. ESTIMATE
 Z (s1 ) − m   λ1 
*
(s0 ) = m +      
zSK
Z (s k ) − m  λk 
T

T

 40 − 110   0.185 
130 − 110   0.128 
*

 
(180,120 ) = 110 + 
zSK
 90 − 110   0.646 

 

0
.
001
160
110
−
−


 
*
(180,120 ) = 86.7
zSK

150

D. ESTIMATION VARIANCE
 Cov (s0 , s1 )  λ1 
 
2
(s0 ) = Cov (s0 , s0 ) − 

σ SK
  
Cov (s0 , s k ) λk 
T

T

 908.6   0.185 
 831.8   0.128 
2

 
(180,120 ) = 2000 − 
σ SK
1507.2  0.646 


 
973
.
6
0
.
001
−

 

2
(180,120 ) = 752.9
σ SK

151

PROPERTIES OF SIMPLE KRIGING
•
•
•
•
•
•
•
•
•
•

Nonconvexity
Screen effect
Declustering
Global unbiasedness
Minimum mean-square-error optimality
Exact interpolator
When interpolation is exact, the error variance is 0
Intolerance to duplicated sites
The estimate is orthogonal to its error
Independence to translation of Cartesian system
152

PROPERTIES OF SIMPLE KRIGING
• Independence of estimator to covariance scaling
• Scaling of covariance changes the estimation
variance by same factor
• Estimation variance depends on data configuration
• Estimation variance does not depend directly on
individual sample values
• If the weights sum up to 1, the estimator is
independent of the value of the mean

153

MULTIGAUSSIAN KRIGING
• This is kriging of normal scores.
• The univariate marginal distribution of a multivariate
normal distribution is normal, yet the converse is not true.
• Under multivariate normality, simple kriging is the best of
all possible estimators, linear or nonlinear, biased or
unbiased.
• Multivariate normality cannot be verified; at most, it
cannot be rejected.
• Considering that the mean of normal scores is zero, if
they are second order stationary, they provide a rare
opportunity to apply simple kriging.
• Backtransformation of estimated normal scores outside
the interval of variation for the normal scores of the data
is highly uncertain.
154

9. ORDINARY KRIGING

155

BRILLIANT TRICK
One can rewrite the simple kriging estimator as
n
 n

Z (s0 ) = m1 − ∑ λi  + ∑ λi Z (s i ).
i =1
 i =1

*
SK

If the weights sum up to 1, then the estimator is
independent of m, thus resulting in a more general
formulation applicable to samples with constant, but
unknown mean (no trend).
It can be proved than the constraint has the additional
benefit of making the estimator unbiased.
Ordinary kriging was a first formulation of improved
simple kriging. Simple kriging roots predate
geostatistics. We will see that one can still find weights
minimizing the mean square estimation error.
156

ORDINARY KRIGING
The ordinary kriging estimator is:
k
∑ λi Z (s i )
*
(s0 ) =  i =k1
ZOK
 ∑ λi = 1,
 i =1

where:

*
(s0 ) denotes estimation at geographic location s0 ;
zOK
s i is the location of measurement i ;
k denotes the number of observations to consider;
m is the mean of Z (s ); and
λi is a real weight.

157

DERIVATION OF EQUATIONS
The whole purpose of ordinary kriging is to find
optimal weights that minimize the mean square
estimation error. The steps are the same as those
required to formulate simple kriging:
A. Make necessary assumptions;
B. Find expression for estimation error variance;
C. Modify estimation error variance equation to have
it in terms of covariances;
D. Out of all possible weights, find those that
minimize the error variance.
E. Replace the optimal weights in the expression for
error variance. This is the kriging variance.
158

A. STRONGER ASSUMPTIONS
1. The sample is a partial realization of a random
function Z (s ) , where s denotes spatial location.
2. Z (s ) is second order stationary, that is:
E[Z (s )] = m
E[(Z (s ) − m )(Z (s + h) − m )] = Cov (h),
⋅ denotes expected value, m is a scalar
where E[]
constant, and h is a vectorial distance.
As in simple kriging, the mean is still constant, but
now it can be unknown.

159

WEAKER ASSUMPTIONS
The following is a set of slightly weaker assumptions,
in the sense that they do not require that the variance
be finite.
1. The sample is a partial realization of a random
function Z (s ) , where s denotes spatial location.
2. Z (s ) honors the intrinsic hypothesis, meaning that:

E[Z (s )] = m
Var [Z (s ) − Z (s + h)] = 2γ (h),
where γ (h) is the semivariogram.

The mean must be constant, but because it will
not enter into the calculations, it can be unknown.
160

B. ESTIMATION VARIANCE
The estimation variance is the error variance:
*
(s0 ) − Z (s0 )],
σ 2 (s0 ) = Var [ZOK

where Z (s0 ) is the random function at the estimation
*
location s0 and ZOK
(s0 ) is the ordinary kriging estimator.
We know neither of these; thus, it is not possible to
calculate σ 2 (s0 ) . The way out is to transform the
expression for the variance to have it in terms of
quantities that we can calculate. This will require even
more laborious algebraic manipulations than for simple
kriging. Nothing comes for free!
161

LEMMA 1 (1)
Let Z (s i ) be a random variable of a continuous random
function. Then for any coefficient λi :
2
 k
  k k
E ∑ λi Z (s i )  = ∑∑ λi λ j E Z (s i )Z (s j ) .
  i =1 j =1
 i =1
Proof:
One can regard λi λ j as a new constant and

[

]

Z (s i )Z (s j ) as a new variable.

162

LEMMA 1 (2)
Then, by the distributive and commutative properties of
the expectation:
2
k
 k
 
E ∑ λi Z (s i )  = λ1λi ∑ E[Z (s1 )Z (s i )]
i =1
 
 i =1
k

k

i =1

i =1

+ λ2 λi ∑ E[Z (s 2 )Z (s i )] +  + λk λi ∑ E[Z (s k )Z (s i )]
and the proof follows by condensing the expression
using a second summation index.

163

LEMMA 2 (1)
Let Z (s i ) be a random variable of a continuous random
function. Then for any coefficient λi :
2
 k
  k k
Var ∑ λi Z (s i )  = ∑∑ λi λ j Cov [s i , s j ].
  i =1 j =1
 i =1

Proof:
By the definition of variance (pages 31 and 52):
2
2
k
k


 





Var ∑ λi Z (s i ) = E ∑ λi Z (s i )  − E ∑ λi Z (s i )  .

 i =i

    i =1
 i =1
k

164

LEMMA 2 (2)
In the right side, by Lemma 1 and expanding second the term:

k
 k k
Var ∑ λi Z (s i ) = ∑∑ λi λ j E[Z (s i )Z (s j )]
 i =1
 i =1 j =1
− ∑∑ λi λ j E[Z (s i )]E[Z (s j )].
k

k

i =1 j =1

Factoring,

k
 k k
Var ∑ λi Z (s i ) = ∑∑ λi λ j {E[Z (s i )Z (s j )]
 i =1
 i =1 j =1
− E[Z (s i )]E[Z (s j )]}.
The fact that the difference is equal to the covariance of
Z (s i ) and Z s j proves the lemma.

( )

165

LEMMA 3 (1)
Let Z (s ) be an intrinsic random function. Then:

γ (s i − s j ) = γ (s i − s ) + γ (s j − s )

− Cov (s i − s, s j − s ).

Proof:
By the stationarity assumption:

[

]

1
2
γ (s i − s j ) = E {Z (s i ) − Z (s j )} ,
2

which does not change by adding and subtracting a third
variate

[

]

1
2
γ (s i − s j ) = E {{Z (s i ) − Z (s )} − {Z (s j ) − Z (s )}} .
2

166

LEMMA 3 (2)
Expanding:

[

]

[

1
1
2
2
 (s i − s j ) = E {Z (s i ) − Z (s )} + E {Z (s i ) − Z (s )}
2
2
− E[Z (s i ) − Z (s ), Z (s j ) − Z (s )],

]

and again, by the stationarity assumption:

 (s i − s j ) =  (s i − s ) +  (s j − s )

− E[Z (s i ) − Z (s ), Z (s j ) − Z (s )].

The proof follows from the definition of the covariance
because of the special fact that the mean of the differences
is zero because the mean is constant.
167

C. NEW ERROR VARIANCE EXPRESSION (1)
Replacing the estimator by its definition in the expression
for the error variance:

[

]

k


2
*
σ (s0 ) = Var ZOK (s0 ) − Z (s0 ) = Var ∑ λi (Z (s i ) − Z (s0 ))
k
 i =1

because ∑ λi = 1. From Lemma 2,
i =1

k

k

σ (s0 ) = ∑∑ λi λ j Cov (s i − s0 , s j − s0 ),
2

i =1 j =1

which, by Lemma 3, is:
k

k

σ (s0 ) = ∑∑ λi λ j {γ (s i − s0 ) + γ (s j − s0 ) − γ (s i − s j )}.
2

i =1 j =1

168

NEW ERROR VARIANCE EXPRESSION (2)
Expanding, because of the constraint
k

k

i =1

j =1

k

∑ λ = 1:
i =1

i

σ (s0 ) = ∑ λi γ (s i − s0 ) + ∑ λ j γ (s j − s0 )
2

k

k

− ∑∑ λi λ j γ (s i − s j ).
i =1 j =1

The first two terms in the right-hand side of the equation
are the same, thus:
k

k

k

i =1

i =1 j =1

σ 2 (s0 ) = 2∑ λi γ (s i − s0 ) − ∑∑ λi λ j γ (s i − s j ).
169

D. OPTIMAL WEIGHTS
The expression for σ 2 (s0 ) is quadratic in the unknowns,
the weights. In such a case, the necessary and sufficient
condition to have a unique global minimum is that the
quadratic term must be equal to or larger than zero, which
implies that the semivariogram must be negative definite.
Thus, we need to employ only permissible models.
The expression for σ 2 (s0 ) is valid for any weights.
Among the infinite weight combinations, we are interested
in those minimizing the estimation variance associated
*
with the estimator ZOK (s0 ) . This is a constrained
operations research problem, best solved by the Lagrange
method of multipliers.
170

LAGRANGE METHOD
The Lagrange method of multipliers is a procedure for the
optimization of a function of several variables
f (y 1, y 2 , , y n ) constrained by φ1, φ2 , , φm .
The solution is found by equating to 0 the n partial
derivatives of an unconstrained auxiliary function
m

f (y 1, y 2 , , y n ) + ∑ µ i φi
i =1

with respect to all y i , regarding the Lagrange multipliers,
µ i , as constants.
The resulting n equations plus the original m constraints
provide a system of equations from which the unknown y i
and the auxiliary variables, µ i , can be calculated.
171

TRIVIAL LAGRANGE EXAMPLE
To find the maximum area of a rectangle whose perimeter
is the given value p , it is necessary to find the maximum
value of the product of the sides a ⋅ b, subject to the
constraint 2a + 2b − p = 0.
The auxiliary function is in this case
u = ab + µ (2a + 2b − p ),
which after differentiation with respect to the unknowns a
and b gives
a + 2µ = 0 

b + 2µ = 0 
2a + 2b = p 

whose solution is a = b = 0.25 p, µ = −0.125.
172

NORMAL EQUATIONS

The Lagrange auxiliary function in this case is:


 k
L = 2∑ λ j γ (s j , s0 ) − ∑∑ λi λ j γ (s i , s j ) + 2µ  ∑ λ j − 1 .
j =1
i =1 j =1

 j =1
k

k

k

The derivatives with respect to the unknowns are:

k
∂L
= 2γ (s j − s0 ) − ∑ λi γ (s i − s j ) + 2µ = 0, for j = 1, , k .
∂λ j
i =1

Thus


λi γ (s1, s i ) − µ = γ (s1, s0 ) 
∑
i =1

 

k
λi γ (s k , s i ) − µ = γ (s k , s0 )
∑

i =1
k


λi = 1
∑

i =1
k

173

E. OPTIMAL ESTIMATION VARIANCE
Multiplying each optimality condition by λi and adding them
together, we have the new optimality relationship:
k

k

k

k

∑ λ ∑ λ γ (s , s ) = ∑ λ γ (s , s ) + µ ∑ λ .
i =1

i

j =1

j

i

j

i =1

i

i

0

i =1

i

Because the weights sum up to 1, introducing λi under the
second summation and replacing the resulting double
summation in the general expression for the estimation
variance, one obtains:
k

k

i =1

i =1

σ 2 (s0 ) = 2∑ λi γ (s i , s0 ) − ∑ λi γ (s i , s0 ) − µ
OK

or

k

σ 2 (s0 ) = ∑ λi γ (s i , s0 ) − µ .
OK

i =1

174

SUMMARY
Ordinary kriging normal equations for optimal weights λi :
k

λi γ (s1, s i ) − µ = γ (s0 , s1 ) 
∑
i =1

 

k
λi γ (s k , s i ) − µ = γ (s0 , s k )
∑

i =1
k


λi = 1
∑

i =1
Ordinary kriging estimate:
n
*
zOK (s0 ) = ∑ λi z(s i )
i =1

Ordinary kriging mean-square-error variance:
k

σ 2 (s0 ) = ∑ λi γ (s i ,s0 ) − µ
OK

i =1

175

INTRINSIC MATRIX FORMULATION
 γ (s1, s1 )  γ (s k , s1 ) 1  λ1   γ (s0 , s1 ) 


   




1

  = 

γ (s1, s k )  γ (s k , s k ) 1  λk  γ (s0 , sK )


  
1
1
0 − µ  
1

 1
T
 Z (s1 )  λ1 
     
*
  
(s0 ) = 
ZOK
Z (s k )  λk 

  
0

 − µ 
 γ (s0 , s1 )  λ1 

   

2
  
(s0 ) = 
σ OK
γ (s0 , s k )  λk 

  
1

 − µ 
T

176

STATIONARY MATRIX FORMULATION
 Cov (s1, s1 )  Cov (s k , s1 )





Cov (s1, s k )  Cov (s k , s k )

1
1
1


1  λ1   Cov (s0 , s1 ) 


1    

=
1 λk  Cov (s0 , sK )

  
0  µ  
1


 Z (s1 )  λ1 
   
*
  
(s0 ) = 
ZOK
Z (s k ) λk 

  
0

 µ 
T

T
(
)
 Cov s0 , s1   λ1 

 


2
  
(s0 ) = Cov (s0 , s0 ) − 
σ OK
Cov (s0 , s k ) λk 

  
1

 µ 
177

EXERCISE

300

2

SAMPLE

Northing

250

Index
1
2
3
4
?

200
?

150

3

4

100
50

Easting
10
30
250
360
180

Northing
20
280
130
120
120

Attribute
40
130
90
160

1

0
0

100

200

300

400

Cov (h) = 2000e −h / 250

Easting

178

EXERCISE
Apply ordinary kriging to the previous sample to do
the following:
A. Find the estimation weights.
B. Note any interesting peculiarities in the weight
values.
C. Use the data and weights to evaluate the
estimate.
D. Calculate the estimation variance.
Compare to the results obtained using simple
kriging.

179

A. ORDINARY KRIGING WEIGHTS
Distance matrices:
 0

260.8

0



264.0 266.3
0


364
.
0
366
.
7
110
.
4
0



[197.2 219.3 70.7 180.0]T
Weights and Lagrange multiplier:
−1

 2000
  908.7   0.198 

 
 

704
.
8
2000
831
.
8
0
.
141

 
 

695.6 689.4 2000
 1507.2 =  0.650 

 
 

466
.
4
461
.
2
1285
.
8
2000
973
.
6
0
.
011

 
 

 1
1
1
1 0  1  − 42.714 
180

B. WEIGHT COMPARISON

300

2
0.141

Northing

250

Weight
1
2
3
4
Sum

200
150

3

100

0.650

50

4
0.011

SK
0.185
0.128
0.646
-0.001
0.958

OK
0.198
0.141
0.650
0.011
1.000

1
0.198

0
0

100

200

300

400

Easting

181

C. ESTIMATES
Using the subscripts OK to denote ordinary kriging and
SK for simple kriging,
T
 Z (s1 )  λ1 
     
  
z (s0 ) = 
Z (s k )  λk 

  
0

 − µ 
*
OK

T

 40  0.198 
130  0.141
*

(180,120 ) =   
zOK
 90  0.650 


 
160
0
.
011


 
*
(180,120 ) = 86.6
zOK
*
(180,120 ) was 86.7
zSK

182

D. ESTIMATION VARIANCE

T

 908.6   0.198 

 

831
.
8
0
.
141

 

2
 OK (180,120 ) = 2000 − 1507.2  0.650 

 

973
.
6
0
.
011

 

 1  − 42.714 
2
(180,120 ) = 754.7
 OK

2
(180,120 ) was 752.9
 SK

 (−42.714 ) provides the variation in 

2

per unit of change in
the constraint. The change in the sum of weights from OK to SK
is 0.042. Hence the reduction of −1.8 (752.9 − 754.7) agrees
with the fluctuation in  2 (0.042·(−42.724)).
183

PROPERTIES
Ordinary kriging shares the same properties with simple
kriging. In addition,
• σ OK (s0 ) ≥ σ SK (s0 )
2

2

.

• For the trivial case of considering only one observation
in the calculation of the estimate,
2
(s0 ) = 2 ⋅ γ (h01 ) ,
σ OK

where h01 is the distance from the observation, z(s1 ) ,
to the estimation location, s0 .

184

SEARCH NEIGHBORHOOD
• Because of the screen
effect and numerical
instabilities, it is
recommended that only
the closest observations
to the estimation location
be used.
• Three observations are a
reasonable bare minimum
and 25 are more than
adequate.
• Use octant search to
further ensure good radial
distribution.
185

SUBSAMPLE SIZE SENSITIVITY FOR THE
CASE OF THE UNCF DATASET

k

k
5
7
8
10
15
20
25

λ1

0.351
0.358
0.356
0.355
0.355
0.355
0.355

λ2

0.311
0.328
0.325
0.324
0.324
0.324
0.324

λ3

0.043
0.047
0.050
0.050
0.051
0.052
0.052

λ4

0.157
0.157
0.178
0.178
0.178
0.178
0.178

λ5

0.137
0.137
0.165
0.170
0.169
0.169
0.169

∑λ
i =6

i

---0.047
-0.074
-0.077
-0.077
-0.078
-0.078

*
zOK

0.766
0.750
0.767
0.756
0.753
0.754
0.758

*
σ OK

0.159
0.159
0.158
0.158
0.158
0.158
0.158

186

THE ELY WEST SAMPLE

This is a semi-artificial, exhaustive
dataset that will be used extensively to
illustrate and compare several methods.
The data are transformed digital
elevations in an area in Nevada with
extreme cases of continuity: flat dry
lakes surrounded by rugged sierras
running roughly north-south.
The sampling is perfectly regular on a 286 by 286 grid,
making 81,796 observations with a Gaussian anisotropic
semivariogram (p. 124): γ E (h) = 0.1 + 11⋅ Gaussian (N10 E,80,34 ).
The exhaustive sample will be used solely for comparisons.
All calculations will be done employing the same subset of
size 40. Based partly on crossvalidation, its semivariogram is
γ (h) = 0.1 + 13 ⋅ Gaussian (N15 E,80,35 ).
187

ORDINARY KRIGING WITH SGeMS
A. LOAD SAMPLE

Load data according to instructions in
chapter 4.
If data have been previously loaded
and saved:
• click on <File>;
• click on <Open Project>;
• select folder and click <OK>.

Loading is complete.
To post data, operate on
<Object> and <Preferences>.
188

B. CREATE GRID FOR STORING RESULTS

• Click on <Objects>;
• click on <New
Cartesian Grid>;
• fill in options;
• click on <Create
Grid>.

189

C. SET PARAMETERS
• Click on
<Estimation> and
then on <kriging>.
• Complete the form.
To be able to do
that, it is required to
previously have
modeled the
semivariogram.
• Click on <Run
Algorithm> when
ready.
You may want to click on <Save> to store all parameters
as *.par file for further reference.
190

D. RESULTS

*
OK

z

2
σ OK

Click on camera icon ( ) at lower right corner of screen
to save maps as electronic files.
191

10. UNIVERSAL KRIGING

192

INTRODUCTORY REMARKS
There are several instances of attributes—water depth
near the shore, temperature in the upper part of the
Earth’s crust—that have a clear, systematic variation.
Hence, models that presume constancy of the mean are
inadequate for the characterization of such attributes.
Universal kriging is a full generalization of simple
kriging that takes to the limit the ordinary kriging
improvement. Universal kriging removes both the
requirement that the attribute must have a constant mean
and that the user must know this constant mean.
As the model complexity goes up, unfortunately so does
uncertainty. Thus, universal kriging should not be used
indiscriminately.
Universal kriging is also known as kriging with trend.
193

ESTIMATOR
The universal kriging estimator has the same form as that
for ordinary kriging:
k
∑ λi Z (s i )
*
(s0 ) =  i =k1
ZUK
 ∑ λi = 1,
 i =1
where
s denotes spatial location,
λi are real weights,
k is the number of observations to consider, and
Z (s i ) is a variate at site s i .
194

ASSUMPTIONS

1. The residuals Y (s ) are a partial realization of a
random function Y (s ) = Z (s ) − mZ (s ) , where mZ (s ) is
the trend or drift of the random function Z (s ) .
2. As in ordinary kriging, one can select second order
stationarity or intrinsic hypothesis, depending on
whether the derivations will be done on terms of
covariances or semivariograms of the residuals.
3. The mean is now neither known nor constant.
4. The trend in the mean is considered a deterministic
component amenable to analytical modeling. The
most common practice, yet not the only alternative, is
to employ polynomials of the geographical
coordinates. The coefficients become additional
unknowns in the trend expression.
195

TREND (DRIFT)

mZ (s ) = a0 +

p

∑a f
i =1

i i

196

NORMAL EQUATIONS
p

k



∑ λi CovY (si , s1 ) + µ0 + ∑ µl fl (s1 ) = CovY (s0 , s1 ) 



λi CovY (s i , s 2 ) + µ 0 + µ l fl (s 2 ) = CovY (s0 , s 2 )
i =1
l =1

 
p
k

λi CovY (s i , s k ) + µ 0 + µ l fl (s k ) = CovY (s0 , s k )
i =1
l =1

k

λi = 1


i =1
k

λi f1 (s i ) = f1 (s0 )

i =1

k

λi f2 (s i ) = f2 (s0 )

i =1



k

λi f p (s i ) = f p (s0 )

i =1

i =1
k

l =1
p

∑

∑

∑

∑

∑

∑

∑

∑

Note that the covariance is that of the residuals.
197

ESTIMATION ERROR VARIANCE

σ

2
UK

k

p

i =1

j =1

(s0 ) = CovY (0) − ∑ λi CovY (si , s0 ) − µ0 − ∑ µ j f j (s0 )

198

SOME MATRIX DEFINITIONS
 CovY (s1, s1 )



CovY (s1, s k )

1
K=

f1 (s1 )




f p (s1 )


 CovY (s k , s1 )


 CovY (s k , s k )
1


f1 (s k )



f p (s k )

f1 (s1 )  f p (s1 )

  
 
1 f1 (s k )  f p (s k )

0
0
0 

0
0
0 


  
 
0
0
0 

1

[

]

k = CovY (s 0 , s1 )  CovY (s 0 , s k ) 1 f1 (s 0 )  f p (s 0 )

[

x = λ1  λk

− µ0

− µ1  − µ p

T

]

T

Z = [Z (s1 )  Z (s k ) 0 0  0]

T

199

MATRIX FORMULATIONS
Normal equations

Kx = k
Estimator
*
(s0 ) = Z T x
ZUK

Estimation variance
2
= CovY (0 ) − kT x
σ UK

200

NUMERICAL EXERCISE
Elevation of water table in northwestern Kansas is a
clear example of an attribute with a linear trend. Use
the sample to:
A. Apply the semivariogram estimator on page 114 along
different directions.
B. Obtain the semivariogram for the residuals by fitting a
model to the experimental semivariogram along the
trend-free direction—in this case, the only
experimental semivariogram that is not an artifact.
C. Use the closest observations to estimate the weights
and Lagrange multipliers at location (60, 193).
D. Find the estimate and its kriging variance.
E. Produce a map of water table elevations using
SGeMS.
201

A. EXPERIMENTAL SEMIVARIOGRAM
FOR THE ATTRIBUTE

202

B. SEMIVARIOGRAM MODEL ALONG
TREND-FREE DIRECTION (N20W)

 (h) = 119 + Gaussian(sill = 3948, range = 29.8 )

A low angular tolerance of 10° was used to properly
filter out the trend.

203

C. WEIGHTS
199

993

Data

Northing, mi

197
Well
993
1002
1003
1502
1504
1505

1002

195

1003

193
191

1504

Easting
mi
61.56
62.94
55.68
64.96
54.80
59.12
60.0

Northing
mi
197.85
194.81
193.56
189.77
190.60
189.47
193.0

Elevation
ft
3065.0
3099.4
3200.0
3114.9
3217.1
3189.7
?

1502

1505
189
187
54

56

58

60

62

64

66

Easting, mi

204

DISTANCES
Observation–observation:
 0
3.34

7.27

8.77
 9.91

8.72

0
7.37
5.44
9.17
6.57





0

10.03
0


3.09 10.20
0

5.34 5.85 4.47 0

Estimate–observation:

[5.09 3.45 4.36 5.92 5.73 3.64]T

205

COVARIANCES AND UNKNOWNS
3710.5
3489.4

 2970.1

 2707.1
K = 2494.0

2716.0
 1

 61.56
187.85


3710.5
2954.3
3250.3
2633.9
3084.4
1
62.94
194.81

3710.5
2471.3
3511.0
3262.7
1
55.68
193.56






3710.5


2438.4 3710.5

3192.0 3372.7 3710.5


1
1
1
0

64.96 54.80 59.12 0 0 
189.77 190.60 189.47 0 0 0

k = [3296.1

3478.8 3385.5 3181.3 3209.4 3461.4 1 60.0 193.0]

x = [0.073

0.348 0.274 0.012 − 0.056 0.350 33.95 0.27 0.53]

T

T

206

D. ESTIMATE AND VARIANCE
T

 3296.1  0.073 
3065.0  0.073 
3478.8  0.348 
3099.4  0.348 

 


 

3385.5  0.274 
3200.0  0.274 

 


 

3181
.
3
0
.
012
3114
.
9
0
.
012

 


 

2
*
(60, 193 ) = 3209.4 − 0.056
(60, 193) =  3217.1 − 0.056 σ UK
zUK

 


 

3461.4  0.350 
3189.7  0.350 
 1   33.95 
 0   33.95 

 


 

 60.0   0.27 
 0   0.27 
193.00   0.53 
 0   0.53 

 


 

T

*
(60, 193 ) = 3149.4
zUK

2
(60, 193 ) = 111.71
σ UK

207

WEIGHTS, ESTIMATES, AND VARIANCES
199

3065
0.073

Northing, mi

197

3099

195

3200

193
191

0.348

3149.6

0.274

111.7

3217
3115

3189

-0.056

189

0.012

0.350

187
54

56

58

60

62

64

66

Easting, mi
208

E. WATER TABLE ELEVATION WITH SGeMS
• Click on
<Estimation and
<kriging>;
• Fill in form and
modify default
values. Trend
model is:

m(s ) = a + b ⋅ X + c ⋅ Y

where:
m(s ) : trend
a, b, c: coefficients
X: easting
Y: northing
• Click on <Run
Algorithm>.
209

MAPS
ft

*
ZUK

sq ft

2
σ UK

210

11. BLOCK KRIGING

211

MOTIVATION
• Block kriging is the generic name given to any form of
kriging in which one is interested in estimates of the
linear average of an attribute inside supports that are
intermediate in size between the support of the
observations and the sampling domain.
• This formulation was originally the most widely used
form of kriging in mining applications.
• Any form of point kriging is a limiting form of an
equivalent formulation of block kriging.
• Ordinary block kriging under second order stationarity
is the most common form of block kriging.
212

LINEAR AVERAGE
Let Z (s ) be a point support random function in D and
let S (s 0 ) be a support of finite size S centered around
s0 . The true block average ZS (s0 ) at location s0 is:
.

1
ZS (s0 ) =
ZS (s ) ds .
∫
S S (s0 )

213

ESTIMATOR
The purpose of block kriging is to produce direct block
averages from point or quasi-point measurements, not
by averaging point estimates.
Let Z (s i ) be k variates of a random function and let
S
. (s0 ) ⊂ D be the support for the estimator ZS* (s0 ) at
site s 0 . Then the ordinary block kriging estimator is:
*
OBK

Z


(s0 ) = ∑ λi Z (si )

i =1

k

λi = 1
∑

i =1

k

214

POINT TO BLOCK COVARIANCE
The main peculiarity of block kriging is the use of
supports of two different sizes for the observations and
the estimator, and hence covariances between two
different volume sizes.

Let Cov (s0 , s ) be covariances between the variate Z (s0 )
and all the variates Z (s ) inside the block S. Then the
covariance Cv (s0 , S ) between the site and the block is:
1
Cv (s0 , S ) = ∫ Cov (s0 , s ) ds .
SS

215

BLOCK TO BLOCK COVARIANCE
Let Cov (s,u) be covariances between a variate Z (s )
in block Si and variates Z (u) inside the block S j . Then
the block covariance CV Si , S j between two blocks is:

(

)

1
CV (Si , S j ) =
Cov (s,u) ds du.
∫
∫
Si S j S i S j

216

ASSUMPTIONS
The minimum set of assumptions required to formulate
ordinary block kriging are the same as those for ordinary
kriging:
1. The sample is a partial realization of a random
function, Z (s ) , where s denotes spatial location.
2. Z (s ) is second order stationary, which implies:

E[Z (s )] = m

E[(Z (s ) − m )(Z (s + h) − m )] = Cov (h) ,

where E[⋅] denotes expected value; m is a scalar,
unknown constant; and h is a vectorial distance.
3. The mean is constant, but unknown.
The SGeMS implementation requires that the block size
be of the same dimensions as the grid cell size.
217

NORMAL EQUATIONS
In an entirely similar way to the derivation of the normal
equations for point ordinary kriging, one can obtain the
following system of equations that provides the ordinary
block kriging weights λi :
k

λi Cov (s i , s1 ) + µ = Cv (s1, S ) 
∑
i =1

k
λi Cov (s i , s 2 ) + µ = Cv (s 2 , S )
∑

i =1

 
k

λi Cov (s i , s k ) + µ = Cv (s k , S )
∑
i =1

k

λ
1
=
∑
i

i =1
218

ESTIMATION VARIANCE
The minimum mean square error for ordinary block
2
, is:
kriging, σ OBK

σ

2
OBK

k

(s0 ) = CV (S,S ) − ∑ Cv λi (si ,S ) − µ
i =1

The form of the expression remains the same as the one
for point ordinary kriging, but all point covariances have
been replaced by block-to-block or point-block
covariances.

219

POINT COVARIANCE MODELING
As in other forms of kriging, performance of block kriging
presumes knowledge of some covariance terms, which in
this case, however, are of different types.
The solution to this requirement is not different from what
we have seen before: model the covariances and ignore
the effects that the use of models instead of the true
covariances have in the normal equations. The common
practice is to first model the semivariogram and then use
Cov (h) = Cov (0 ) − γ (h)

to obtain the point-point covariance model. The blockblock and point-block covariances are derived from the
point-point covariance.
220

NON-POINT COVARIANCE MODELING
In early days of geostatistics there was great effort
devoted to solve exactly the integrals for point-block and
block-block covariances for multiples shapes of blocks.
Today the simplest and prevailing practice is to calculate
the integrals by numerical approximation. The block is
tessellated into smaller units that are treated as points and
then the block covariance is calculated as the average
between all possible point-point covariances. The
following table gives the recommended minimum number
of subdivisions.
Dimension
1
2
3

Subdivisions
10
6 by 6
4 by 4 by 4
221

EXAMPLE OF NUMERICAL
CALCULATION

When the point-point covariance is the exponential
model e-h/0.8, the values show the point covariance
between the point on the left and the center of squares
0.05 miles on the side. Averaging the 25 numbers
gives a value of 0.682 for Cv(0.3).
222

AVERAGE POROSITY
WEST LYONS FIELD, KANSAS
Ordinary kriging
Cell size: 0.05 mi

Ordinary block kriging
Cell size: 0.25 mi

223

STANDARD ERROR OF POROSITY
Ordinary kriging
Cell size: 0.05 mi

Ordinary block kriging
Cell size: 0.25 mi

224

12. COKRIGING

225

INTRODUCTION
• Simple, ordinary, and universal kriging are not
multivariate models in the usual statistical sense of the
term. Despite the fact that they employ random function
models comprising infinite numbers of random variables,
they are employed in the modeling of one attribute at a
time.
• The main purpose of cokriging is to work as a true
multivariate estimation method able to deal
simultaneously with two or more attributes defined over
the same domain, which is called a coregionalization.
• Not all attributes must be measured at all locations, but a
bare minimum of collocated measurements per pair of
attributes is necessary for structural analysis.
226

VECTORIAL NOTATION
Like in spatial univariate geostatistics, geographical
location is the vectorial term:
s i = [(easting i , northing i , elevation i )] .
T

Cokriging makes heavy use of vectorial and matrix
notation for conciseness in the notation and similarity with
the univariate formulations. Let us start with the definition
of a vectorial random function.

If Z1 (s ), Z2 (s ),, Zk (s ), , Z p (s ) is a set of p random
functions defined over the same domain, then the vectorial
random function Z(s ) is the vectorial matrix:
T
(
)
[
(
)
(
)
(
)
(
)
]
Z s = Z1 s Z2 s  Zk s  Z p s .

227

COEFFICIENT MATRIX
The coefficient matrix, Λ i , is the square matrix:
i
i
 λ11
λ12
 λ1i p 
 i
i
i 
λ21 λ22  λ2 p 

Λi =
 


 
 i
i
i 
λp1 λp 2  λpp 

where λ jk are real numbers and p is the number of
attributes to model.
i

228

ORDINARY COKRIGING ESTIMATOR
The objective of cokriging is to find the minimum meansquare-error weights for the linear estimator, Z * (s0 ) :
n

Z (s0 ) = ∑ Λ i Z(s i )
*

or

i =1

 Λ1   Z(s1 )
 Λ   Z(s )
2 
Z * (s0 ) =  2  
     

  
Λ n  Z(s n )
T

both subject to

n

∑ Λ = I.
i =1

i

229

VECTORIAL MOMENTS
For the purpose of listing the assumptions, we need to
define the vector of means, m :

m = [m1 m2  mp ]
and the matrix covariance

T

 Cov 11 (Z1 (s ), Z1 (s + h))  Cov 1p (Z1 (s ), Z p (s + h)) 





Cov(Z(s ), Z(s + h)) = 
.
Cov p1 (Z p (s ), Z1 (s + h))  Cov pp (Z p (s ), Z p (s + h))



230

ORDINARY COKRIGING ASSUMPTIONS
The minimum set of assumptions required to formulate
ordinary cokriging under second order stationarity are:
1. The sample is a partial realization of a vectorial
random function, Z(s ) .
2. Z(s ) is second order stationary, which implies:

E[Z(s )] = m

E[(Z(s ) − m)(Z(s + h) − m)] = Cov(h) .

3. Each term in m is unknown, but constant.
4. None of the variables is a linear combination of the
others:
Zk (s ) ≠ a0 + ∑ a j Z j (s ), for k = 1, 2, , p .
j ≠k

231

NORMAL EQUATIONS

Λ Cov(s i , s1 ) + μ = Cov(s1, s0 ) 
∑
i =1

k
'

(
)
(
)
,
,
Cov
s
s
μ
Cov
s
s
Λ
+
=
∑
i
2
2
2
0

i =1

 
k

'
Λ p Cov(s i , s p ) + μ = Cov(s p , s0 )
∑
i =1

n

Λi = I
∑

i =1
where μ is a square matrix of Lagrange multipliers of the
same order p as the number of attributes in the
coregionalization.
k

'
1

232

ESTIMATION VARIANCE
If

[

W= Λ

'
1

'

'

Λ2  Λn

]

μ

T

k = [Cov(s1, s0 )  Cov(s n , s0 ) I] ,
T

2
then the estimation variance for ordinary cokriging σ OC
(s0 )
is:
2
(s0 ) = diagonal(Cov(0) − kT W ).
σ OC

233

STRUCTURAL ANALYSIS
The structural analysis required by cokriging is much
more demanding than the one for kriging because:
• Besides the p covariances, the user must model
additional p(p − 1) / 2 cross covariances.
• Cov(⋅) must be positive definite to properly solve the
normal equations. Individual selection of permissible
models for all covariances and cross covariances is
not sufficient to produce a positive definite Cov(⋅).
-

The common way to conduct structural analysis is
through the application of the so-called linear
coregionalization model.
234

LINEAR COREGIONALIZATION MODELING
Linear coregionalization modeling is a solution based on
the decomposition of all structural functions in terms of a
basic set of permissible models, first done in terms of
semivariograms and cross semivariograms and then
converted to covariances and cross covariances.
L

Γ(h) = ∑ Bl γ l (h)
l =1

For all structural functions to be permissible, all coefficient
matrices Bl of order p must be positive semidefinite. If a
matrix is positive semidefinite, its determinant, all its
principal minor determinants, and all its eigenvalues are
nonnegative. A cross semivariogram, γ ij (h) , is the moment:
1
γ ij (h) = E[{Zi (s ) − Zi (s + h)}{Z j (s ) − Z j (s + h)}].
2
235

EXAMPLE OF LINEAR
COREGIONALIZATION MODEL
The following would be an example of isotropic linear
coregionalization for the case of two attributes considering
nugget effect, one exponential model with a range of 200,
and one spherical model with a range of 150:
3
3h

 1 1 3 h

2 1 4 2
1
h


200


h  

1 e

 
 , 1 .



 1 2 2 150 2  150  
1 3 2 3




This notation implies, for example, that the cross
semivariogram between attributes 1 and 2 is:
3
3h


 3 h

h
1


200

 12 h  1  21  e
 
 , 1 .

  2 150 2  150  

 

236

EFFECTIVE APPLICATION OF COKRIGING
Abundant information on additional variables is not a
guarantee of superior results of multivariate cokriging
over just kriging for the variable of interest. The
following are two prerequisites:
• The secondary variables must be significantly
correlated to the primary variable.
• A significant proportion of secondary measurements
must be at locations other than those of the primary
variable.
A corollary is that when all attributes are measured at
the same sites without missing values, cokriging provides
similar results to kriging of individual attributes.
237

POROSITY OF SEA FLOOR,
MECKLENBURG BAY, GERMANY
Sample

Kriging estimation

238

COKRIGING FOR SEA-FLOOR POROSITY
MECKLENBURG BAY, GERMANY

Sample for grain size median

Cokriging porosity map using porosity
and grain size median data

239

13. CROSSVALIDATION

240

ACCOUNTABILITY
• Upon selecting any estimator plus its parameters and
producing estimations for a given spatial attribute,
crossvalidation addresses the curiosity and need to know
about the quality of the job done.
• The ultimate approach would be to run an exhaustive
sampling and then assess the quality of the estimation by
comparing the new data with the collocated estimates
produced with the original data. Unquestionably, the best
estimator would be the one with the best statistics on the
basis of a previously agreed criterion.
• As conclusive as this approach may be, its
implementation ordinarily is either impossible to execute
or prohibitively expensive and defeats the purpose of the
estimation in practical situations.
241

ALTERNATIVE EVALUATION METHOD
Crossvalidation is an ingenious alternative to exhaustive
sampling.
• Given a sample of size n, crossvalidation is based on the
dropping of each observation with replacement. For
each ignored observation, an estimate is computed at
the location of the discarded observation by using at
most the remaining n − 1 measurements.
• By pretending that a drawn observation was never taken,
one can genuinely attempt to produce an estimate, and
by comparing it to the deleted measurement, one can
have an instant and cost free error measurement.
• The discarded observation is replaced in the sample
before continuing with the analysis.
242

WORD OF CAUTION
Crossvalidation is a fast and inexpensive way for
indirectly testing anything about kriging or the data.
However, one must apply crossvalidation with caution.
• Crossvalidation does not indicate whether an
observation, estimate, parameter, or assumption is
inadequate.
• Complex interdependencies among the errors have
precluded finding their distribution, thus hindering a
rigorous analysis or testing of the results.
• Nonetheless, crossvalidation remains a useful tool
primarily to dispel blunders and assist in drawing
honest conclusions by comparison, instead of making
arbitrary decisions or assumptions.
Crossvalidation is not part of SGeMS, but it can be
found in GSLIB.
243

UNFAVORABLE CIRCUMSTANCES
Special circumstances deserving extra caution are:
• In the case of kriging, the estimate is insensitive to
scaling of the semivariogram, thus making the
estimates insensitive to proportional changes in both
sill and nugget.
• Irregular sampling patterns may lead to
unrepresentative errors. A typical situation is the
sampling in clusters, which results in abnormally low
errors at all locations making the clusters. For better
results, decluster first, then crossvalidate.

244

DIAGNOSTIC STATISTICS
Comparisons or remedial actions based on crossvalidation
are more straightforward and conclusive if the collection of
errors is reduced to some key statistics such as:
• Some quantiles of the error distribution, such as the 5th
percentile, the median, and the 95th percentile.
• The mean and standard deviation for the square of the
errors.
• The slope and intercept of the regression line between the
measured and estimated values.
• The correlation coefficient between the measured and
estimated values.
There is plenty of redundancy in the statistics. In addition,
all statistics do not always point in the same direction.
245

CRITERIA
Guidelines to follow based on the statistics include:
• The ideal estimation method should result in no errors
and a correlation coefficient between measurements and
estimates of 1.
• Large individual errors may be the effect of blunders or
outliers. Data preparation and coding, including
geographical location, should be checked carefully.
• A slope different from 45o in the regression line between
measurement and estimates denotes conditional bias.
• Ideally, errors should be spatially uncorrelated, resulting
in a pure nugget effect semivariogram.
• If the z-scores—the ratio of the error over the standard
error—follow a standard normal distribution, one cannot
discard the possibility that the errors are multinormally
distributed.
246

PRACTICAL EXAMPLE: UNCF
A. The sample size is too
small for a directional study
of the semivariogram.
Employ crossvalidation for
investigating anisotropy.
B. Utilize optimal parameters
to run ordinary kriging.
C. For an 8-km search radius,
employ crossvalidation for
finding the optimal number
of estimation points.
D. Analyze crossvalidation
errors.
247

A. OPTIMAL NUMBER ESTIMATION
POINTS
For an 8-km search radius, the mean square error of
ordinary kriging is minimum when the maximum
number of estimation points is 11.
Mean square error
sq ft
589.6
632.1
658.7
657.4
593.9
545.2
539.5
451.0
361.2
376.0
385.1

700
650

Mean square error, ft

Maximum number of
observations
3
4
5
6
7
8
9
10
11
12
13

600
550
500
450
400
350
300
2

4

6

8

10

12

14

Maximum number of observations

248

B. INVESTIGATION OF ANISOTROPY
For anisotropic semivariograms, the practice is to vary
the range as the length of a cord from the center to the
perimeter of an ellipse. Below are the results of an
analysis keeping the average of the axes equal to
isotropic range.
Isotropic semivariogram

 3h  

−


γ (h) = 10 + 5.641 − e  8.11  


2

Axes size
m
8107 8107
8500 7714
9000 7214
9300 6914
9400 6810
9500 6714
9600 6614
9400 6814
9400 6814

Major axis
orientation
N
N
N
N
N
N
N
N20E
N10E

Mean square
error, sq ft
361.2
302.8
257.7
247.0
246.0
246.2
247.5
253.8
243.6
249

C. OPTIMAL ORDINARY KRIGING
PARAMETERS
•
•
•
•

Minimum number of observations: 3
Maximum number of observations: 11
Search neighborhood radius: 8,000 m
Best semivariogram model:
Type:
Gaussian
Nugget:
10 sq ft
Sill:
5,640 sq ft
Major range: 9,400 m
Direction: N10E
Minor range: 6,814 m
250

ORDINARY KRIGING MAPS

m

m

m

m

Estimated depth

Kriging standard deviation
251

D. CROSSVALIDATION ERRORS (1)

• Errors are slightly positive.
• Distributions for estimated
and true values are
extraordinarily similar.
• Crossvalidation errors are
spatially correlated.
252

CROSSVALIDATION ERRORS (2)
D = 0.09
pu = 0.59

• The Kolmogorov-Smirnov pu-value is 0.59, which is
significantly larger than 0.1. Thus, according to the criteria
in p. 100, one cannot reject the normality of errors
independently of the influence of the spatial correlation.
• The larger the standard error, the larger the error
dispersion, but without being correlated.
253

14. CRITICAL REVIEW

254

SAMPLING
• Minimum mean square error methods—kriging
included—produce realistic estimations when the
average sampling interval is one order of magnitude
smaller than the minor axis of the semivariogram
range.
• As the sampling becomes sparser, maps based on
kriging show increasing smoothing, which can be
appreciated in various ways.
We explore here the weak sides of kriging as a
motivation to go into stochastic simulation.

255

EVALUATION OF ORDINARY KRIGING
THROUGH THE EXHAUSTIVE SAMPLE
AT ELY WEST

Exhaustive sample
81,796 measurements

Ordinary kriging map
40 measurements
256

CROSSVALIDATION
Estimated values
are close to being
pure noise, with
estimates for low
measurements
above the main
diagonal and
conversely for the
highest data values.
This a form of
smoothing.
257

SEMIVARIOGRAMS OF
ORDINARY KRIGING MAP

There is wide disagreement between the actual
semivariogram and that of the ordinary kriging map.
This is a consequence of smoothing.
258

HISTOGRAMS

The ordinary kriging histogram is not bimodal like the
histogram of the exhaustive sample, which is also a
consequence of smoothing.
259

CUMULATIVE DISTRIBUTIONS

Discrepancy in the proportion of zeros results in
clearly different distributions.
260

ERRORS ACCORDING TO
EXHAUSTIVE SAMPLE

In agreement with the global unbiasedness of kriging, the
errors are symmetric around a value approximately equal
to zero.
261

SEMIVARIOGRAM OF ERRORS
ACCORDING TO EXHAUSTIVE SAMPLE

Errors are spatially correlated, with a range remarkably
similar to that of the sample, but with a significantly
smaller sill.
262

A VIEW AT ERRORS AND
KRIGING STANDARD DEVIATIONS

3

2

2

4

Errors

3

4

Kriging standard deviation

Most kriging standard deviations are no
more than twice the error, but there is
no correlation, nor the type of assurance
provided by the Chebyshev’s theorem.
263

REMEDIAL ALTERNATIVES
There have been efforts along two different lines to
eliminate smoothing:
• Compensation procedures
• Simulation
Success of compensation procedures has been
limited.
By far the bulk of the effort and results have been in
the generation of equiprobable realizations using
simulation.
When simulation is used, uncertainty is modeled by
generating several realizations. The wider the
discrepancies, the larger the uncertainty.
264

SIMULATION METHODS
The most common methods are:
• Turning bands method
• Lower-upper triangular method
• Truncated plurigaussian methods
• Object oriented methods
• Sequential Gaussian simulation
• Simulated annealing
• Filter simulation
When a realization by any method reproduces the
value and location of all observations, the procedure is
said to be a conditional simulation. Otherwise, it is an
unconditional simulation.
265

15. SEQUENTIAL GAUSSIAN
SIMULATION

266

BACKGROUND
Sequential Gaussian simulation is a stochastic method that
is basically multigaussian kriging with feedback.
Let Z (s ) be a subset of N variates of a random function
and let z(s ) be a sample of size n . By the multiplication
rule in p. 45, one can break a bivariate cumulative
frequency distribution function into the product of two
univariate cumulative frequency distribution functions:
F (s1, s 2 , t1, t 2 | n ) = F (s1, t1 | n ) ⋅ F (s 2 , t 2 | n + 1)
where F (s 2 , t 2 | n + 1) is the probability Pr (Z (s 2 ) ≤ t 2 ),
conditional to the original sample plus a value z(s1 ) drawn
from the distribution F (s1, t1 | n ).

267

EXTENSION OF THE DECOMPOSITION
By extension, the decomposition of a bivariate distribution
can be generalized to any number of random variables:

F (s1, s 2 ,, sN , t1, t 2 ,, t N | n ) =

F (s1, t1 | n ) ⋅ F (s 2 , t 2 | n + 1) ⋅  ⋅ F (sN , t N | n + N − 1).

Sequential Gaussian simulation is a numerical
application of this recursive decomposition for the special
case when the joint distribution F (s1, s 2 ,, sN , t1, t 2 ,, t N | n )
is multivariate normal, in which case the marginal univariate
distribution F (s1, t N | n ) and all the conditional univariate
distributions F (s i , t i | n + i − 1) are also normal.
In practice, the locations are the N nodes of a regular grid.
.

268

TEN-STEP APPROACH (1)
To generate a partial realization of a random function at the
nodes of a regular grid, do the following:
1. In case the sample is not univariate normal, transform
the data into normal scores. SGeMS can do it, going to
<Algorithms> then <Utilities> then <transf>.
2. Using standard semivariogram modeling techniques,
find the best semivariogram model for the transformed
data.
3. Define a grid, pick nodes at random, and stack them all
in a visitation queue.
4. For faster execution, move the data to the closest node.
In case two or more observations land in the same
node, implement a criterion to end up with one value
per node, such as averaging or discarding.
269

TEN-STEP APPROACH (2)
5. From the node sequence queue, draw the next location
s i due for the calculation of a simulated value. Employ
multigaussian kriging to find the estimate z * (s i ) and the
corresponding estimation variance σ 2 (s i ) using an
expanded sample comprising all n original data plus all
values that may have been already simulated.
.

z*

z( s,x)

6. Use z * (s i ) and σ 2 (s i ) to
define a normal distribution with mean z * (s i )
and variance σ 2 (s i ),
N (z * (s i ),σ 2 (s i )).

σ

x

270

TEN-STEP APPROACH (3)
Draw at random a value,
zs (s i ), from N z * (s i ),σ 2 (s i ) .
The number zs (s i ) is the
simulated value at location
.

si .

(

z*

)

z( s,x)

7.

σ

.

8.

Add zs (s i ) to the expanded
sample.

x

If s i is not the last node in the queue, then go back to
Step 5.
10. In case Step 1 was necessary, backtransform all the
values in the partial realization to the original sampling
space.
9.

271

EXERCISE
Use SGeMS and the Ely West sample of size 40 (page 188)
to do the following:
A. Generate 100 realizations by sequential Gaussian
simulation; compare the first one to the exhaustive sample.
B. Examine the semivariograms for the first realization along
the north-south and east-west directions and compare
them to the exhaustive semivariogram.
C. Compare the histograms for the exhaustive sample and
the first realization.
D. Prepare a Q-Q plot to compare the same distributions.
E. Use the exhaustive sample to obtain the errors for the first
realization. Map them and study the semivariogram.
F. Investigate errors in the first realization and the standard
deviation that results from all realizations at every node.
272

A. SGeMS CODE FOR GENERATING 100
SEQUENTIAL GAUSSIAN SIMULATIONS
• Note that the
attribute is the
data, not the
normal scores.
• However, the
semivariogram
is for the
normal scores,
not for the
attribute.
• No backtransformation is
necessary.
273

TWO REALIZATIONS FOR ELY WEST

These are conditional realizations honoring the same 40
data. Thus, although hard to note, the values of both
realizations at those 40 nodes are the same.
274

BACK TO THE EXHAUSTIVE CASE

Exhaustive sample
81,796 measurements

First realization
40 measurements

275

B. FIRST REALIZATION
SEMIVARIOGRAMS

Agreement is not perfect but is significantly better
than in the case of kriging.
276

C. HISTOGRAMS

The realization follows the sample histogram, not that of
the exhaustive sample.
277

D. CUMULATIVE DISTRIBUTIONS

According to the Q-Q plot, agreement between distributions
is also better than in the case of ordinary kriging.
278

E. ERRORS ACCORDING TO
EXHAUSTIVE SAMPLE

Errors are unbiased but are larger than those for
kriging.
279

SEMIVARIOGRAM FOR ERROR IN
FIRST REALIZATION

The error semivariogram resembles that of the
attribute.
280

F. A LOOK AT FIRST REALIZATION ERRORS
AND NODAL STANDARD DEVIATIONS

3

Errors

Nodal standard deviation

2

2

4

3

4

Given a node, the nodal standard deviation
is the standard deviation of the values from
all realizations at that node.
Larger standard deviations are associated
with large errors, but there is no correlation.
281

16. SIMULATED ANNEALING

282

THE GENERAL IDEA
Simulated annealing was one of the first methods for the
modeling of spatially correlated attributes that moved away
from generating values for uninformed nodes by combining
the sample values.
Simulated annealing started as an approach to model the
cooling of a piece of hot metal, but it is now more widely
used as an optimization tool instead.
In geostatistics, simulated annealing is used to move
around node values in order to have a realization with
some property as close as possible to a target function,
which commonly is the semivariogram.
Its main feature is the lack of any distributional
assumptions.
283

“COOLING” A GRID
A simulation grid is populated by placing each sample
value at the collocated or closest node. The unsampled
nodes are given values randomly taken from a probability
distribution, typically that of the sample.
By so doing, the realizations honor the data and the
selected histogram, but in a typical situation, unless the
sample is large relative to the grid size, the grid is totally
chaotic and lacking spatial correlation.
The aim of simulated annealing is to transform the grid
to gain additional properties, customarily to be spatially
correlated by following a semivariogram model.
284

OBJECTIVE FUNCTION
When the target function is a semivariogram model γ (h), the
objective function G is:
2
[
γ G (h) − γ (h)]
G=∑
2
(
)
γ
h
h

where γ G (h) is the semivariogram of the realization.
Reduction in the value of G is achieved through swapping
pairs of values at nodes which are not sampling locations.
Key to a successful process is not to reject 100 percent of
the exchanges of nodal values that do not result in an
improvement of the objective function G.
The cooling schedule controls the proportion of
unfavorable swaps as a function of total swaps attempted.
285

COMPONENTS OF AN
ANNEALING SCHEDULE
 1, i f Gnew ≤ Gold
Pr [accept swap ] =  Gold −Gnew
e t , otherwise .

t is the temperature, starting at t 0 .
Other critical components are (p. 289):
λ temperature reduction factor;
K a is the maximum number of all swaps for a given
temperature, after which it is reduced by λ ;
K is the maximum number of accepted swaps for a given
temperature, after which it is reduced by λ ;
eK a is the maximum number of times for reaching K a ;
Gmin (Omin ) is the target value for the objective function.

286

ANNEALING IN ACTION

G=1
Swaps=0

G=0.0001
Swaps=10,895,372

287

FEATURES
• No computational power at all is used to calculate node
values.
• As all nodal values are drawn from the sample
distribution, its representativity is more critical than for
the other simulation methods.
• All efforts are devoted to moving around node values.
• Given a sample, processing time is in general the
longest among simulation methods, which increases
more steeply than linearly with the number of grid nodes.
• Despite its novel computational approach, simulated
annealing continues to rely on a sample and global
properties that can be derived from the sample, most
commonly the semivariogram.
• The most salient property is independence from any
distributional assumptions.
288

REPETITION OF PAGE 272 EXERCISE
NOW FOR SIMULATED ANNEALING
The method is not part of SGeMS, but it is in GSLIB.
Parameters for SASIM (1992)
***************************
START OF PARAMETERS:
s40.dat
1
2
0
4
-1.0e21
1.0e21
0
s40.dat
4
0
0
14.0
1
1.0
4
2.0
sasim.out
sasim.var
3
500000
sasim.dbg
1
1.0 0.12 4000000 400000
2
112063
100
286 0 1
286 0 1
1
0.0
1.0
150
1
0.1
0
3
45.0
13.0
15.0 0.0 0.0 0.45 0.0

5

0.0001

\conditioning data (if any)
\columns: x,y,z,vr
\data trimming limits
\0=non parametric; 1=Gaussian
\non parametric distribution
\columns: vr,wt
\minimum and maximum data values
\lower tail option and parameter
\upper tail option and parameter
\output File for simulation
\output File for variogram
\debug level, reporting interval
\output file for debugging
\annealing schedule? (0=auto)
\manual schedule: t0,lambda,ka,k,e,Omin
\1 or 2 part objective function
\random number seed
\number of simulations
\nx,xmn,xsiz
\ny,ymn,ysiz
\nz,zmn,zsiz
\max lags for conditioning
\nst, nugget, (1=renormalize)
\it,aa,cc:
STRUCTURE 1
\ang1,ang2,ang3,anis1,anis2:

289

A. TWO REALIZATIONS FOR ELY WEST

290

BACK TO THE EXHAUSTIVE CASE

Exhaustive sample
81,796 measurements

First realization
40 measurements
291

B. SEMIVARIOGRAMS FOR
THE FIRST REALIZATION

Semivariogram reproduction is good.
292

C. HISTOGRAMS

Exhaustive sample

First realization

There is also good reproduction of the sample histogram.
293

D. CUMULATIVE DISTRIBUTIONS

Agreement between the distributions of realizations
and the population is limited only by the unbiasness
of the empirical sample.
294

E. ERRORS IN FIRST REALIZATION
ACCORDING TO EXHAUSTIVE SAMPLE

Errors are comparable to those of sequential Gaussian
simulation.
295

ERROR SEMIVARIOGRAMS
FOR FIRST REALIZATION

Errors have a semivariogram resembling the
semivariogram of the attribute.
296

F. TRYING TO RELATE ERRORS AND
NODAL STANDARD DEVIATIONS
The tendency of errors to stay within
four nodal standard deviations is still
true, but it is less pronounced than
for kriging and sequential Gaussian
simulation.
3

2

2

4

4

Errors in first
realization

Nodal standard deviation

3

297

17. FILTER SIMULATION

298

CLASSICAL GEOSTATISTICS
• Until recently, geostatistics had evolved to provide
modeling tools relying on methods typical of
stochastic processing.
• Central to past developments is the heavy use of
moments up to order 2, particularly the
semivariogram or its equivalent the covariance.

299

MULTIPLE POINT STATISTICS
• The first attempts to expand classical geostatistics
aimed to calculate statistics involving three or more
points.
• These efforts have had limited success because of
the high demands in terms of sample size required
for the estimation of higher order moments.

300

MULTIPLE POINT RELATIONSHIPS
• However, the idea of considering relations among
several points has stayed.
• The concept of moment has received less
attention than techniques related more to image
analysis and pattern recognition, such as filtering
and cluster analysis.
Filter simulation is a member of the multipoint
statistics family of simulation methods. It is based on
pattern recognition and does not require the
semivariogram at all, even though one can still
estimate it for purpose of evaluation or comparison.
301

MULTIPOINT SOURCES
Courtesy of A. Journel, Stanford University

Sources of multiple point
relations are:
• Either images borrowed
from sites considered
analogous to the one of
interest or
• Synthetic models
generated on the basis of
fundamental geological
principles.
These pictures are called
training images. They are
absolutely critical to the
success of the simulation.
302

FILTERING
Filters are used to extract essential pattern information,
such as average, gradient, and curvature. SGeMS
employs six filters for two-dimensional patterns and nine
in three-dimensional cases.
pattern

kth filter weights
applied
applied

generate
results

toto

in
a pattern

Courtesy of A. Journel,
Stanford University

Filter weights

kth score value

a score

303

CLUSTER ANALYSIS

The general idea is to group objects
in attribute space into clusters as
B
internally homogeneous as possible
and as different from the other
C
clusters as possible.
A
Different types of approaches,
distances, and proximity criteria have
resulted in several methods. Filter
Score 1
simulation offers two:
• Cross partition, where classes are defined by the
quantiles of each filter score. Two patterns belong to the
same cluster if they are bounded by the same quantiles
for all filters. Results depends on the number of quantiles.
• K-means, which operates by trial and error. Cluster
membership evidently varies with the number of clusters.
10

4

5

Attribute
Score 22

8

3

6

6

4

2

2

0

1

0

2

4

6

8

10

Attribute 1

304

EXAMPLE OF CATEGORICAL CLUSTER
Prototype
Courtesy of A. Journel, Stanford University

• The prototype here
is the average of 56
template patterns
from a training
image involving two
lithologies.
• By the nature of
cluster analysis, all
the patterns are
quite similar.

305

FILTER SIMULATION
Filter simulation is closer to putting together a puzzle than
to estimation. The steps to generate a realization are:
1. Define a grid and select a training image at least as large
as the study area.
2. Tessellete the training image into small templates.
3. Use different filters to generate scores to characterize
template patterns.
4. Use the filtering multivariate information to classify the
templates into clusters that group pattern styles.
5. Define a random visitation sequence.
6. Select at random a template from the cluster best
matching the conditioning data or select a template at
random if there are no conditioning data.
7. Paste the central portion of the template (inner patch) in
the realization.
8. Go back to Step 5 until the modeling is completed.
306

DIAGRAMMATIC GENERATION OF AN
UNCONDITIONAL REALIZATION
Courtesy of A. Journel, Stanford University

Training image

Note that the progression is by patches, not by pixels.
307

BACK TO THE ELY WEST EXAMPLE
Training image

The training image must at least extend over the area
of interest.
308

PAGE 272 EXERCISE ONCE MORE, NOW
EMPLOYING FILTER SIMULATION

309

A. TWO REALIZATIONS FOR ELY WEST

310

COMPARISON TO EXHAUSTIVE SAMPLE

Exhaustive sample
81,796 measurements

First realization, based on
40 measurements plus
the training image
311

B. SEMIVARIOGRAMS
FOR FIRST REALIZATION

Reproduction of exhaustive semivariogram is not as
good as that of simulated annealing, but comparable
to the overall approximation by sequential Gaussian
simulation.
312

C. HISTOGRAMS

Filter simulation does a superior job approximating the
histogram of the exhaustive sample because of the
positive influence of the training image.
313

D. Q-Q PLOT

The distribution for the first realization is in close
agreement with that of the exhaustive sample.
314

E. ERRORS IN FIRST REALIZATION
ACCORDING TO EXHAUSTIVE SAMPLE

Errors are unbiased and the lowest among the simulation
methods considered in this primer.
315

ERROR SEMIVARIOGRAMS

In common with all other methods, errors here are
correlated. The semivariogram resembles the one for
the attribute being modeled.
316

F. A VIEW OF ERRORS AND NODAL
STANDARD DEVIATIONS

3

2

2

5

5

Error

3

Nodal standard deviation

Above a nodal standard deviation of
1.5, the dispersion of errors tends to
be the same regardless of the value of
the nodal standard deviation.
317

18. RELIABILITY

318

From the outset, the
quantitative
assessment of the
uncertainty associated
with the resulting
models has been an
important objective of
geostatistics.

z( s,x)

UNCERTAINTY

x

Basic to the analysis is having information about
the probability distribution of the errors.
This chapter explores the different ways in which
geostatistics uses results from previous methods for
uncertainty assessment.
319

THE RANDOM FUNCTION MODEL
By collecting the values of multiple realizations at the
same node, it is possible in practice to model the
concept of multiple variates over a sampling domain.

t
s

Diagrammatic idea

Actual implementation:
central node, Ely West

320

MAPS OF MOMENTS
Although nodal distributions have all the information for
assessing uncertainty, customarily there are so many
nodes that their individual inspection is impractical,
creating, once more, a need to prepare summaries.
The easiest solution is to map some consistent
summary statistic for the nodal distributions. The most
common options are:
A. Nodal mean, also known as E-type
B. Nodal standard deviation
Both options share the advantage of reducing all
information to one map, no matter how many the
realizations.
321

A. NODAL MEAN MAP

• E-type maps tend to remind us of kriging maps, sharing
their smooth appearance.
• As in kriging, smoothing disqualifies E-type maps from
being realistic models.
• In the event that an E-type map is a good model, it is
always less demanding to prepare it by using kriging.
The link between E-type maps and kriging is an interesting
fact to remember.

Ordinary kriging estimation

Sequential Gaussian simulation

322

B. NODAL STANDARD DEVIATION MAPS

Ordinary kriging estimation

Sequential Gaussian simulation

• The resemblance between standard deviations from
simulation and kriging can be good, but never as good as
the one for the means.
• Although nodal standard deviation provides a semiquantitative feeling about uncertainty, other summary
statistics are more efficient for summarizing the
information contained in the realizations.
323

NORMALITY
z
Kriging can be used in reliability
estimation, but it is necessary to
assume the form of the nodal
σ
distributions. The common
practice is to assume normality,
with a mean equal to the
x
estimate, z * , and a standard
*
σ
deviation equal to the kriging standard error, .
When the errors indeed follow a multivariate normal
distribution reasonably closely, results can be quite
reliable, as we will see for the case of the UNCF sample.
Whereas kriging may not be great at characterizing the
true attribute, it can do a much better job, comparable to
that of simulation, when it comes to assessing confidence
intervals.
z( s,x)

*

*

324

BEST DISPLAYS FOR SUMMARIZING
UNCERTAINTY

A. The probability distribution for some global attribute
after reducing each realization to one single number,
which can be of the same nature or different from
the sampled attribute.
B. The kth percentile map displaying the threshold for
which there is a probability 0 ≤ k / 100 ≤ 1 that the
true value is less than or equal to the threshold. The
units of this type of map are the same as those of
the attribute.
C. A probability map showing the chances that the true
value be equal to or less than a given threshold.
D. Interval probability map displaying fluctuations in the
chances that the true value be between a pair of
fixed values.
325

A. SUMMARY DISTRIBUTIONS
ASSESSING GAS POTENTIAL IN
GREATER NATURAL BUTTES, UTAH

326

B. BACK TO THE UNCF EXAMPLE.
10th PERCENTILE MAP
Prob[Z (s ) ≤ node _ value(s )] = 0.10

Sequential Gaussian simulation

Ordinary kriging estimation
327

C. PROBABILITY MAP FOR TWO
THRESHOLDS USING SEQUENTIAL
GAUSSIAN SIMULATION

Prob[Z (s ) ≤ 7872]

Prob[Z (s ) ≤ 7966 ]
328

PROBABILITY MAP FOR TWO
THRESHOLDS UNDER NORMALITY

Prob[Z (s ) ≤ 7872]

Prob[Z (s ) ≤ 7966 ]
329

D. INTERVAL PROBABILITY
Prob[7872 ≤ Z (s ) ≤ 7966 ]

Sequential Gaussian simulation

Ordinary kriging estimation
330

BACK TO ELY: WHERE ARE TRUE VALUES
LANDING AT THE NODAL DISTRIBUTIONS?
Each node shows
the cumulative
probability below
the true value.
Blanks denote
that the true value
is outside the
nodal distribution,
which is certainly
not a good
situation.
331

ACCURACY OF CONFIDENCE INTERVALS
Using the nodal
distributions, for a
given probability, the
scatterplots show how
frequently the true
values are indeed
below the quantile
associated with the
given probability.
Note that these plots
are global assessments
comprising all nodes
for all realizations.
332

19. NAVIGATION CHARTS

333

KRIGING OR SIMULATION?

Although there are no clear excluding rules, common criteria
to choose one approach over the other are:
• Perform kriging if the paramount criterion is individual
minimization of prediction error, if the data spacing is low
relative to the semivariogram range, or if the main interest is
an exploratory mapping of a poorly known attribute through
a deliberately smooth, thus simplified, version of reality.
• Go for simulation if it is not acceptable to assume normality
of the errors for reliability assessment, if correct modeling of
spatial continuity is more important than local accuracy, or if
there is interest in a stochastic assessment of some global
attribute, such as the examples in page 326.
Always remember that, unless you are in a terrible hurry,
nothing prevents trying both kriging and simulation, as done
with the Ely data, and compare the results.
334

RANKING OF FOUR METHODS
IN THE SPECIAL CASE OF ELY WEST
Method
1 Filter simulation
2 Seq. Gaussian simul.
3 Simulated annealing
4 Ordinary kriging

Reproduction
of
distribution
1
2
2
4

Reproduction
of
semivariog.
2
2
1
4

Texture
1
2
3
4

1

Issues
with
errors
1
1
4
3

Mean
square
error
2
3
3
1

Confidence
interval
2
4
1
3

Process.
time
3
2
4
1

Total
12
16
18
20

2

Exhaustive sample

3

4

335

WHAT TYPE OF KRIGING?

Selection of the type of point kriging is ruled by the
knowledge of and type of mean (trend).
• Use simple kriging if there is no trend and the population
mean is known, a rare situation.
• Run ordinary kriging if the mean is unknown and the
trend is approximately constant within the search
neighborhood (page 185). This is the predominant case.
Ordinary kriging is actually a special name for universal
kriging for a polynomial of degree 0.
• Otherwise, employ universal kriging proper of degree 1 or
2, whose main feature is extrapolation, which is still poor.
Use the same criteria to decide the type of block kriging
and cokriging.
As the complexity of the trend model goes up from simple
to universal kriging, so does the estimation variance.
Use of semivariogram or covariance is immaterial
because, for all practical purposes, they are equivalent.
336

WHAT KIND OF SIMULATION?
• Apply filter simulation if you already have a good idea
about the spatial variation of your attribute, good
enough to be able to provide a training image. The
quality of the results is crucially dependent on the
quality of the training image.
• In terms of usage, the popularity of the methods is
divided among the other two types covered here. Go
for simulated annealing if you want to stay away from
any form of normality assumption and the grid size is
less than 100,000 nodes.
• Otherwise, try sequential Gaussian simulation.

337

ACKNOWLEDGMENTS

338

Teaching is the best way to learn. Many thanks to all those participants of my courses who helped
shape the present and earlier versions of these notes with their criticism.
I am grateful to Alexander Boucher (Stanford University) for productive discussions during my
evaluation of SGeMS and for the care he took two years later to scrutinize this primer. Michael Friedel
(U.S. Geological Survey), and Marlene Noble (U.S. Geological Survey) contributed to the polishing of
this report with their thorough reviews. Marlene deserves extra credit for an earlier round of remarks
while taking the latest offering of the course.
U.S. Geological Survey editor Peter Stauffer did a careful job scrutinizing final details. Eric Morrissey
(U.S. Geological Survey) inserted all the tags necessary to facilitate the use of the report by visually
impaired readers.

339

SELECTED BIBLIOGRAPHY

340

Caers, J., 2011, Modeling uncertainty in the Earth Sciences. Chichester, UK, John Wiley & Sons
Ltd, 229 pp.
Chilès, J.-P. and Delfiner, P., 1999, Geostatistics: modeling spatial uncertainty. New York, WileyInterscience, 695 pp.; 2nd ed. 2012, 734 pp.
Christakos, G., Olea, R.A., Serre, M.L., Yu, H.-L., and Wang, L.-L., 2005, Interdisciplinary public
health reasoning and epidemiologic modelling. Berlin, Springer, 319 pp.
Deutsch, C.V. and Journel, A.G., 1992, GSLIB: geostatistical software library and user’s guide.
New York, Oxford University Press, 340 pp. and 2 diskettes; 2nd ed., 1998, 369 pp. and CDROM.
Goovaerts, P., 1997, Geostatistics for natural resources evaluation. New York, Oxford University
Press, 483 pp.
Isaaks, E.H. and Srivastava, R.M., 1989, An introduction to applied geostatistics. New York, Oxford
University Press, 561 pp.
Mariethoz, G. and Caers, J., 2015, Multiple-point geostatistics. Chichester, UK, John Wiley & Sons
Ltd, 264 pp.
Moore, D.S. and Notz, W.I., 2006, Statistics: concepts and controversies (6th ed.). New York,
Freeman, 561 pp.
Olea, R.A., ed., 1991, Geostatistical glossary and multilingual dictionary. New York, Oxford
University Press, 177 pp.
Olea, R.A., 1999, Geostatistics for engineers and earth scientists. Norwell, Mass., Kluwer Academic
Publishers, 313 pp.
Remy, N., Boucher, A., and Wu, J., 2009, Applied geostatistics with SGeMS: a user’s guide: New
York, Cambridge University Press, 284 pp. and CD-ROM.

341

INDEX

342

anisotropy 126, 187, 247, 249
annealing schedule 285–286
assumption 29, 243
block kriging 217
cokriging 231
ergodic 88
multivariate normality 154
ordinary kriging 159–160
simple kriging 133
simulated annealing 283, 288
stationarity 89
universal kriging 195
autocovariance 112
backtransformation 103, 154, 271,
273
Bayes’s theorem 46
bias 90, 154, 246
binomial distribution 47–48
block kriging 212–224
box-and-whisker plot 38
causality 58, 62
Chebyshev’s theorem 32, 263
cluster 91–97, 119, 244, 301,
304–306
coefficient
kurtosis 42
quartile skew 41
skewness 40, 49
coin flipping 45, 48, 51
cokriging 226–239
comparison

of block and ordinary kriging
223–224
of cokriging and kriging 226,
238–239
of covariance and
semivariogram 112–113
of geostatistics and time-series
analysis 6–7
of kriging and simulation
322–323, 327–332, 334–335
of kriging methods 336
of ordinary and simple kriging
156, 181–184
of simulation methods 335, 337
of statistics and geostatistics 5,
87
conditional probability 45–46
conditional realization 274
conditional simulation 265
coregionalization 226, 232
model 234–236
correlation 246, 263, 281
coefficient 57, 60, 62, 245
spatial 118, 284
covariance 56, 110–111, 133,
138–139, 153, 159, 197,
215–216, 220–222, 299, 336
block-to-block 216, 219–221
point-to-block 215, 219–222
cross partition 304
cross semivariogram 235–236
crossvalidation 241–253

cumulative distribution 22, 65, 93,
97, 99, 101, 103, 105, 267
dataset
Ely West 187
UNCF 20, 247
decile 34
declustering 91–97, 119, 152, 244
dependent variable 58
diagonal of matrix 143, 233
distribution 99, 283, 321
binomial 47–48
continuous 49, 51, 105
discrete 48, 51, 107
Gaussian 22, 42, 47, 49–50,
64, 87, 154, 270, 324, 334
lognormal 49
normal 22, 42, 47, 49–50, 64,
87, 154, 268, 270, 324, 334
standard normal 64, 102, 246
drift 114, 195–196, 201
ergodicity 88
error 59, 119, 152, 242, 244–249,
252–253, 279–281, 295-297,
315–317, 319, 324, 334
mean square 60, 128, 152,
158, 219, 229
estimation variance 117, 134, 270,
336
block kriging 219, 224
cokriging 233
simple kriging 134, 138–142,
146, 151, 153, 183–184

343

ordinary kriging 161–170, 174,
184
universal kriging 198, 200,
207–208
estimator 24, 90, 128, 241, 270, 324
block kriging 214, 223
cokriging 229
covariance 56
ordinary kriging 157, 183
semivariogram 114
simple kriging 130, 142, 150,
153–154, 156, 183
universal kriging 194, 207–208
E-type map 321–322
expected value 51–52, 56, 105,
133, 135, 159, 217
experimental semivariogram
114–116, 119, 201–202
explanatory variable 58–59
exponential semivariogram 117, 236
extrapolation 7
extreme value 33, 37–38, 59, 82
filter 301, 303, 306
filter simulation 299–317, 337
fractile 34
frequency table 19–20, 22
Gandin, L.S. 9
Gaussian distribution 22, 42, 47, 49,
50, 64, 87, 154, 270, 324, 334
Gaussian semivariogram 117, 187,
249
genesis 126

grid 70, 76–80, 189, 268–269, 284,
306
GSLIB 68, 79, 243, 268, 289
simulated annealing 289
histogram 21, 24, 40, 47, 82–84,
91, 272, 284
historical remark 8–9
independence 45, 48, 87, 89,
152–153, 156, 288
independent variable 58–59
indicator 105–107
interpolation 7, 63, 152
interquartile range 36–37, 41
intrinsic hypothesis 160, 166, 195
inverse matrix 143
isotropy 119, 236
joint probability 45–46, 89, 268
k-means 304
Kolmogorov, A.N. 9
Kolmogorov-Smirnov test 100–101,
253
Krige, D. 9
kriging 128–129, 237–238,
243–244, 255, 322–324, 336
block 212–224
multigaussian 154, 267, 270
ordinary 156–191, 193, 226,
247–251, 256–263, 336
simple 130–154, 156,
181–184, 193, 226, 336
universal 193–210, 226, 336
with trend 193

kurtosis 42
Lagrange method 171–173, 201,
232
lognormal distribution 49
lognormal transformation 104
lurking variable 62
main diagonal of matrix 143
Mathern, B. 9
Matheron, G. 8
matrix 143–144, 176–177,
199–200, 227–232, 235–236
inverse 143
main diagonal 143, 233
transposed 143
maximum 33, 38, 63
mean 26, 29, 31–32, 38, 42, 49,
51–52, 64, 101, 112–114, 133,
153–154, 156–157, 159–160,
193, 195, 230, 245, 270,
321–324,336
square error 60, 128
measure
centrality 24–25
dispersion 24, 30
shape 24, 39
median 27, 29, 34–35, 37, 97, 245
minimum 33, 38, 63, 139, 170, 185
mode 28, 259
moment 52, 89, 235, 299–301, 321
multigaussian kriging 154, 267, 270
node 76, 269–272, 281, 283–285,
288, 320–321, 323, 332

344

noise 61, 257
normal distribution 22, 42, 47, 49,
50, 64, 87, 154, 268, 270, 324,
334
normal equation
block kriging 218
cokriging 232, 234
ordinary kriging 173
simple kriging 131, 142
universal kriging 197
normal score 102–103, 154, 269
nugget effect 118, 236, 244, 246
object 70–81, 188–189
omnidirectional semivariogram 119
ordinary kriging 156–191, 193, 226,
247–251, 256–263, 336
orthogonality 152
outlier 37–38, 246
P-P plot 65–66
pattern recognition 301
percentile 34, 38, 245, 325
permissible model 117–118, 139,
170, 234–235
plot
P-P 65–66
Q-Q 63, 66, 272
population 15–16, 24, 31, 294, 336
positive definite 139, 234
probability 44–46, 50, 325
conditional 45–46
cumulative 22, 64, 99, 102, 105
density function 47–50

distribution 47–50, 87, 284,
319–321, 324–326
joint 45–46
probability-probability plot 65–66
project 81, 188
property
ordinary kriging 184
simulated annealing 288
simple kriging 152–153
Q-Q plot 63, 66, 272
quantile 34, 245, 304
quantile-quantile plot 63, 66, 272
quartile 34–36, 38
random function 86–87, 89, 105,
133–135, 159–160, 195, 213–214,
217, 226–227, 267, 269, 320
random variable 16, 59, 64, 86–87,
89, 226
range 113, 118, 126, 236, 249–250,
255, 334
realization 86–88, 133, 159–160,
195, 217, 231, 264–265,
268–269, 271–272, 274,
283–285, 306–307, 320–323, 332
regressed variable 58
regression 58–61, 128, 245–246
regressor 58–59
reliability 319–332
residual 135, 195–197, 201
response variable 58
robustness 29, 31, 33, 36, 41, 61,
91

sample 15, 17, 24, 26–28, 31–34,
56, 58–63, 72, 87, 91, 99–102,
119, 124, 133, 153, 159–160,
217, 231, 242, 267, 269–272,
283–284, 287–288, 325
Ely West 187
size 17, 63, 242, 247, 267, 272,
300
UNCF 20, 247
saving
grid display 191
grid values 79
histogram display 84
object 81, 188
parameter file 190
semivariogram 125
scaling 63, 66, 153, 244
scatterplot 55, 332
screen effect 149, 152, 185
search 185–186, 247–248
semivariogram 109–126, 170, 190,
195, 201–203, 220, 235–236,
244, 246–247, 249, 255, 269,
272, 283–285, 288, 299, 336
experimental 114–116, 119,
201–202
exponential model 117, 236
Gaussian model 117, 187, 249
omnidirectional 119
pure nugget effect 118, 236,
244, 246
spherical model 117, 236

345

sequential Gaussian simulation
267–281, 322–323, 330–332, 337
SGeMS 11, 13, 68–84
availability 68
data 70–75
filter simulation 309
grid 70, 76–80
histogram 82–84
image save 84, 191
loading 74–75, 188
main menu 69
object 70–82, 188–189
ordinary kriging 188–191
project 81, 188
save 79, 81, 84, 125, 188,
190–191
semivariogram 120–125
sequential Gaussian simulation
273
universal kriging 209–210
shift 63, 66
sill 113, 244, 250
simple kriging 130–154, 156,
181–184, 193, 226, 336
simplicity 133
simulated annealing 283–297,
331–332, 335, 337
simulation 78, 255, 264–265, 337
conditional 265
filter 299–317, 337
sequential Gaussian 267–281,
322–323, 330–332, 337

simulated annealing 283–297,
331–332, 335, 337
unconditional 265
skewness 40–41, 49
smoothing 255, 257–259, 264, 334
spatial continuity 110, 187
spatial correlation 5, 100, 110, 118,
128, 284
spherical semivariogram 117, 236
standard deviation 32, 49, 57, 64,
101, 245, 263, 272, 281, 321,
297, 317, 323
standardization 64–65
standard normal distribution 64,
102, 246
stationarity 89, 113–114, 133, 154,
159, 166–167, 195, 212, 217, 231
statistic 24, 66, 84, 100, 241,
245–246, 300, 321, 323
statistical dependence 45, 58, 243
statistical independence 45, 48, 87
structural analysis 109, 226,
234–236
support 87, 212–216
theorem
Bayes’s 46
Chebyshev’s theorem 32, 263
time series analysis 6–7
training image 302, 305–308, 337
transformation 64, 99–107, 119,
269, 271
Indicator 105–107

lognormal 104
normal 102–103, 269
transposed matrix 143
trend 114, 119, 195–196, 201, 209
unbiasedness 90, 152, 154, 156,
261, 279, 294, 315
uncertainty 4, 6, 86, 193, 264, 319,
321, 323, 325
unconditional simulation 265
universal kriging 193–210, 226, 336
variable
dependent 58
explanatory 58–59
independent 58–59
lurking 62
random 16, 59, 64, 86–87, 89,
226
regressed 58
response 58
variance 31–32, 36, 52, 64, 113,
117, 119, 124, 136, 164, 270
variate 16, 267–268, 320
visitation schedule 269, 306
weight 130–132, 139–140, 142, 146,
148–149, 153, 156–158, 170, 174,
180, 208, 218, 229, 303
Wiener, N. 9
z-score 246, 253

346

