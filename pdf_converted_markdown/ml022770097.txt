iyC&

L&bJb J-cd/)

JL

Short Course in Ge ology: Volume 8

Fundamentals of Geostatistics
in Five Lessons
Andre G. Journel

American Geophysical Union
91040:3018,8 910327
WASTE
PFDR
PDIR
WM- 11
I

Short Course in Geology: Volume 8

Fundamentals of Geostatistics
in Five Lessons
Andre G. Journel

\Q Short Course Presentedat the
28th InternationalGeological Congress
Washington, D.C.
American Geophysical Union, Washington, D.C.

Maria Luisa Crawford and Elaine Padovani
Short Course Series Editors

tibrary of Congress Cataloging-in.Pubhcation Data
journel, A. G.
Fundamentals of geostatistics in five lessons.
Bibliography: p.
1. Geology-Statistical methods. I. Title.
QE33.2.S82J68 1989
551'.072
ISBN 0-87590-708-3

89-14911

Copyright 1989 by the American Geophysical Union, 2000 Florida

Avenue, NW, Washington,

DC 20009, U.S.A.
Figures. tables, and short excerpts may be reprinted in scientific
source is properly cited.

__

books and journals if the

Authorization to photocopy items for internal or personal use,
or the internal or personal use
of specific clients, is granted by the American Geophysical Union
for libraries and other users
registered with the Copyright Clearance Center (CCC) Transactional
Reporting Service.
provided that the base fee of 51.00 per copy plus $0.10 per page
is paid directly to CCC, 21
Congress Street, Salem, MA 10970. 0065-448/89/$01. + .10.
This consent does not extend to other kinds of copying, such
as copying for creating new
collective works or for resale. The reproduction of multiple copies
and the use of full articles or
the use of extracts, including figures and tables, for commercial
purposes requires permission
from AGU.

Printed in the United States of America.

CONTENTS

Preface

v

Introduction

I

Lesson I:

Statistics Review and Notations

2

Lesson II:

Linear Regression Theory, or Simple
Kriging

10

Lesson III: Linear Regression under Constraints,
and Ordinary Kriging

15

Lesson IV:

Non-parametric assessment of local
uncertainty

21

Lesson V:

Stochastic imaging for imaging
Spatial Uncertainty

30

Subject Index

39

Iii

PREFACE

From its inception as a separate discipline, geostatis
tics sought recognition from practitioners,
not from math
ematicians or physicists. and rightfully
so. Indeed, the
theory was essentially established by the
1950's by Kol
mogorov and Wiener and exposed by Matern
(1960), Whit
tle (1963), and Matheron (1965), among
others. But there
is a long, hard way between a concept expressed
by matrix
notations in a Hilbert space and its implementation
and
routine application. It is my opinion that
the main con
tribution of geostatistics has been and still
is implementa.
tion, an essential follow-up step much
too often forsaken
by theoreticians.
Implementation requires a prior effort
of simplifica
tion. A concept or algorithm will take
root only if un
derstood by the user, who can customize
it to the ever
changing needs of his various projects.
Practice over time
is a merciless judge that will strip all
concepts of their
fancy dressing, whether wording or computer
coding, and
let the sole essence stand for itself. Geostatisticians
would

accelerate the diffusion process if they could go about
ex
tracting the essence of their favorite tools a very hum
bling task - and deliver it in simple terms.
Stressing the essence of one's finding may not
make it
into a publication list but would help the understanding
and correct application of the corresponding
algorithm.
Behind most sophisticated concepts, there
is a simple
idea sometimes so simple that we feel like dressing
it up.
Practitioners face real data with their extraordinary
com
plexity that defies any pre-conceived model,
and they are
the best positioned to customize the algorithm
to make
it work. Thus, it is of great importance
that they (the
practitioners) understand what we (the
academics) are
proposing.

Andre G. Journel
Stanford University

v

I

Fundamentals of Geostatistics in Five Lessons
Andre G. Journel
Stanford Center for Reservoir Forecasting
Department of Applied Earth Sciences
Stanford University, Stanford, California 94035

Introduction

weighting criteria.
Associating kriging to distance
weighting algorithms, or in a dual fashion to surface
These lessons, except for the fourth, were "speed"-written
fitting algorithms, makes it more "ordinary" now
as support for a "Geostatistics for Reservoir
that it
Characteri
is
severed from that hazy random function source. Geo
zation" course given in Dallas, December of
1987. There
statistics may end up looking less prestigious (or mysteri
is definitely a need for new books in Geostatistics
that
ous?), but being better understood will be better applied.
would acknowledge the contribution of new
application
After much hard selling, the time for a fresh and more
fields and sort the wheat from the tares, the
theory that
temperate
look at geostatistics has come. Geostatistics
did yield from that which remained but elegant.
I know
is
foremost
Data Analysis and Spatial Continuity Model
of at least two such books in the mill. In the
meantime,
ing.
Such
analysis
and modeling cannot be done without
course supports were needed and I attempted
this quick
a
clear
understanding
of the origin of the data, includ
draw. I ask for the reader indulgence and
patience until
ing geological interpretation. The main reason
for model
the availability of official books.
ing spatial continuity is to assess spatial uncertainty.
Textbooks are polished logical constructions
As
which do
for using probabilistic models, it is naive to think that
not lend themselves to the spot-painting and
diversions
any statistical tool provides objectivity, it should
that could sometimes enlighten a class.
provide
Lessons allow
though consistency once a prior model has been
digressions, returns to fundamentals, parallels,
chosen.
that could
Geostatistics is a set of numerical tools to be
reveal a hidden side of the theory being developed
added to
and,
the large tool chest of the geologist; it allows transporting
in the best case, the essence of an algorithm
that which
quantitatively a geological model all the way
makes it work. Building from rigorous random
to process
function
design and engineering. That geological model should
theory, how could one tell that the essence
not
of ordinary
stem from a blackbox package, particularly if that
pack
kriging is:
age is cryptic. Good geology based on well understood
data is still the only recipe for good reservoir/site/deposit
1. the usage of a structural distance, specific
to the
characterization.
va iable being considered, which need not be
a var
These five lessons address the fundamentals of geo
iogram
statistical theory relevant to spatial interpolation,
image
2. the possibility of accounting for data redundancy,
reconstitution and uncertainty modeling. The practice
of
geostatistics, although of paramount importance, is not
as figured by the data covariance matrix?
covered here for lack of space. The book from
Srivas
If kriging sometimes works it is not because of
tava and Isaaks, which should be in the shelves by
its prob
mid
abilistic pedigree - in fact the algorithm could
3989, will fulfill that need beautifully. The remarkably
be estab
lished without a single reference to random
user-friendly and yet complete software "Geostat Tool
variables
but because it extends well-proven and intuitive
box", made public-domain by its author Roland Froide
distance
vaux, provides the tools for anyone to get started.
Copyright 1989 American Geophysical Union
Lesson I proposes a brief review of statistics and noI

2

FUNDAMENTALS OF GEOSTATISTICS

tations; needed for developing the further lessons. The
reader is supposed to have a prior familiarity with statis
tics, integral and differential calculus at an introductory
college level.

Too often, a Gaussian error distribution is casually taken
as model for uncertainty although evidence for the in
adequacy of such symmetric distribution model do exist.
Alternative models based on the actual distribution of
Lesson II presents the classical linear regression theory
neighboring data are proposed building on an indicator
with the particular geostatistical twist, in the sense that
data kriging paradigm. Construction of models of un
the data used (the so-called "independent" variables of
certainty precedes the derivation of an estimate for the
classical regression) are themselves dependent one upon
unknown, which allows retaining non-least squares, i.e.
each other and thus needed to be made independent in
a first step. The simple kriging system (SK) is shown to non-kriging-type estimates possibly better suited to the
project at hand.
be but a variant of the normal system of equations. A
All moving average-type estimates, including all krig
dual interpretation of the simple kriging algorithm shows
ing estimates, provide a smooth image of the underlying
it amounts to fit covariance-type interpolation functions
reality: the variogram of these estimates would not repro
to the data values at their locations.
Simple kriging requires that the mean of the variable duce the data variogram. The concept of conditional sim
ulation allows generating alternative, equiprobable, im
over the field being estimated be constant and known. Or
ages which honor data values at their locations and re
dinary kriging does not require knowledge of that mean,
as long as it remains constant. Kriging with a trend model flect a series of spatial continuity functions. The novel
technique of Indicator conditional simulations, presented
allows considering a variable mean, function of the coor
in Lesson V allows generation of images that do not suf
dinates values. That function is everywhere unknown but
is of known functional form and could represent a local fer from the maximum entropy (maximum disorganiza
tion for a given variograrn model) limitacin-i of Gaussian
trend component being added to residual values. In Les
related
random function models. Also, indicator simula
son III, it is shown that ordinary kriging and kriging with
tions
allows
honoring, in addition to hard data, soft infor
a trend model are achieved by adding specific constraints
mation whether local (e.g. constraint intervals at specific
to the normal system of equations.
locations) or global (e.g. interpretative structural geol
Lesson IV is possibly the most important of this set
ogy). A set of simulated images provide an assessment of
of five lessons, for it addresses the critical problem of un
certainty modelling. No estimator is devoid of potential joint spatial uncertainty rather than a series of, say, local
estimation variances.
error, thus qualified decision-making requires an assess
References are proposed at the end of each lesson, and
ment (model) of the uncertainty underlying each estimate.
an index of subjects is given.
Lesson I: Statistics Review and Notations
In this lesson, we will review only those basic notions

of statistics that are useful to geostatistics and spatial

interpolation problems. Engineering-type definitions are
preferred over more rigorous but less intuitive axiomatic
definitions.
Random variable (RV)
A random variable can be seen as a variable, say, Z, in
capital letters, that can take a series of outcomes or real

izations (zi , i = 1,..., N) with a given set of probability

If the number of possible occurrences is infinite, say, a
porosity value within the interval [0, 100%], the RV Z
is said to be continuous, and its probability distribution
is characterized by the cumulative distribution function
(cdf) defined as:
F(z) = Prob{Z <5

} E [0, 1]

(3)

The cdf corresponds to the notion of cumulative histogram
in the discrete case:
F(zi)

with the set (j) corresponding to
• pi, ' all
realizations zi < z,

of occurrence (pi,
1, .... ,N)
When the number N of occurrences is finite, one speaks
of a "discrete" RV. The N probabilities of occurrences
must verify the conditions

The cdf fully characterizes the RV Z. Probability inter
vals can be derived:

p, > 0 ,foralli=l,.-.,N

Prob{Z EIa,&] } = F(b) - F(a)

N

Y-p. = 1
*=1

(1)
(2)

UI)

Similarly, the probability of exceedence of any threshold
value can be derived:

JOUIUNEL
Prob{Z > a) = 1 -F(a)

k4= (zk+l

Quantiles
The p-quantile of the distribution F(z) is the value z.
such that: F(z,) = p E [0, 1], i.e., the value z. which has
a probability p not to be exceeded by the RV Z. Defining
the inverse F-'(p) of the cdaf:
p - quantile z, = F-'(p) , with p E [0,1]

+ zk)/2

The expected value of any well-behaved function of
Z, say, P(Z), can also be defined, under conditions of
existence of the integral:
- in the discrete case:
N

(s)

8=p

(4)

Expressing p in percent, the p-percentile is defined:

3

- in the continuous case:

p - percentile z. = F-'(100 p) , with p E [0, 100]

E

-p(Z(z dF(z)=

v••(z) -f(z) dz

Quantiles of interest are:
- the .5 quantile (50th percentile) or median:
M = F- 1 (.5)

(5)

m = EIZ) being the mean of Z, the variance of the RV
Z is defined as the expected squared deviation of Z about
its mean:
VarfZ} = 0 2 = E {[z - m]2} Ž 0

- the lower and upper quartile:
Z.25 =

(.25) , Z. = F-(.75)

-

(9)

,p,(z, - rn_
) , in the discrete case

i=1

The interquartile range (IR) is define- as the interval
bounded by the upper and lower quartii,.. R = [z.
25 , z. 75)
Expected value
The expected value is the probability-weighted sum
of
all possible occurrences of the RV.
The expected value of Z, also called mean of Z,
is
defined as:
- in the discrete case, corresponding to relations

=

(z - m) 2 dF(z) =

(z - m) 2f(z)dz ,

in the continuous case.
The mean is a central location characteristic of the
pdf A(z), see Figure 1. The variance is a characteristic of
spread of that distribution around the mean.
f(z)

(1)

and (2):
N

E{Z) = m = Ep,•,

(6)

i=1

- in the continuous case with cdf F(z), under condi
tions of existence of the integrals:
M 0 m
E{Z}=m = I

z dF(z) =jf'z f (z) dz

z

F(z)

(7)

K

J=im •"4,[F(zki) - F(z*)], with 4
Z]

zEIk,
Zk+]

k=1I

where: f(z) = F'(z) is the probability density function
(pdf) defined as the derivative, when it exists, of the cdf
F(z). The integral f- z dF(z) is approximated by K
clae of respective frequencies [F(zk+,) - F(z&) , and
z4, is a value within the kth class, say, the center
of that
class:

(10)

M

0 M

Fig. 1. Probability density function (pdf), and
Cumulative density function (cdf)

4

FUNDAMENTALS OF GEOSTATISTICS

Another central location characteristic is the median
M. A corresponding characteristic of spread would be the
mean absolute deviation about the median (mAD):

g(z)

mAD = E{ Z- Mf}
The skewness sign is defined as:
Sign of(m - M)
M
A positive skewness usually indicates a long tail of the pdf
towards the high outcome values.

0(z)
GW

Order relations

I

The cdf F(z) and its derivative, the pdf f (z), being
probability-related functions, must verify the following
conditions:
F(z)

f(u)du E [0,1]

0.5

F(z) Ž F(z') , for all z > z' , i.e., the cdf is a non
decreasing function of z.

f (z) = Fo(z) >_0
o

zis

i.e., the integral of the pdf
equal to 1.

The expected value is a "linear operator," in the sense
that the expected value of a linear combination of RV's
is the linear combination of the expected values of these
RV's:

{~akZk}

Gaussian (normal) model

9(Z) = a 121 exp [_Z-

M2

The standard normal pcif corresponds to m = 0 ,2

Linear property of the expected value

E

Normal (Gaussian) pdf and cdf

It is a distribution model fully characterized by its two
parameters, mean m and variance a2. The pdf g(z) is, see
Figure 2:

Correspondingly:

+

Z

mM.

(11)
Fig. 2.

o f

2

~E{k

(12)

k
whatever the RV's Zk whether dependent or independent

go(z) = v

(13)
=1

exp

The corresponding ecdf's have no close-form analytical ex
pression, but the standard normal cdf Go(z) is well tabu
lated:

from each other, and whatever the constant ak's.

Go(z)

= f

*go(u)du

(14)

In particular, for any transform functions •p and W2:
E{p 1 (Z) + ý02(Z)} = E{v,(Z)} + E{W (Z)}
2

provided the corresponding expected values exist.
Application:
VarZ

=
=

E{[Z - m]2} = E{Z 2 - 2mZ + m2}
2

E{Z }-2mE{Z}+m

2

G(z) =

gI~ d = G. z

2

Symmetry. The Gaussian distribution is symmetric about
its mean, thus:
m =M, g(z+nm)=-g(mn-z),
G(m-z) =l-G(m+z), for all z

(15)

2

=E{Z }-m2

zi-, = 2m-z,, for all p E [0- .51,

z, being the p-quantile.

JOURNEL
Some Gaussian values

5

The lognormal distribution is a two-parameter distribu
tion fully characterized by:

G(m + a) = .84, G(m - a) = .16
G(m + 2a) = .977,

"* either its mean and variance (in, C 2 ) also called arith
metic parameters,

G(m - 20) = .023

"*or the mean and variance (a,3 2 ) of the log trans

Thus:

form X = In Y , also called logarithmic parameters.

Prob{Z E [m ± c]} = .84 - .16 = .68
Prob{Z E [m ± 2a]} = .977 - 0.23 = .954 = .95
The "success" of the Gaussian model stems from a series
of Central Limit theorems which state that:
The sum of a not too large number of independent,
equally distributed (although not necessarily Gaussian),
standardized RV's tend to be normally distributed, i.e.,
if the n RV's Z,'s have same cdf and zero means, the RV
Y = I Z= Z, tend towards a normal cdf, as n - 0c.
The most restrictive constraint to application of Cen
tral Limit theorems is the condition of independence: the
n RV's Z, must be independent.
In the highly controlled environment of a laboratory
experiment where measurement devices are carefully cho
sen and monitored, one may expect normal distributions
for measurement errors. In fact, the experiment(s) may
have been designed specifically to generate such normal
distribution. However, in the nature-controlled environ
ment of a spatial distribution, say, that of porosity within
a layer or permeability within a sandstone formation, there
is no reason to believe a priori that the conditions of a
Central Limit theorem would apply. Nor should these
conditions apply to the various sources of error involved
in a spatial interpolation exercise, that of estimating, say,
a porosity value from neighboring well data.

The lognormal model
The normal (Gaussian) model is very convenient be
cause it is fully characterized by only two parameters,
its mean and variance. However, it is symmetric and al
lows for the occurrence of negative outcomes. Many ex
perimental distributions tend to be skewed (asymmetric)
with mean different from median, and most earth sciences
variables are non-negative valued.
Various transforms of the normal model have been de
fined to accommodate the usual features of earth sciences
data distributions.
A positive RV, Y > 0, is said to be lognormally dis
tributed if its logarithm X = In Y is normally distributed:
Y > 0-,ý logN(m,or)

,if X =lnY--,.- N(a, #')

(16)

The lognormal cdf is more easily expressed as a func
tion of its logarithmic parameters (a, f2).
Prob{Y <(

= Fy(y) = Go (n

for a

'- a)

>0

(17)
where Go(.) is the standard normal cdf defined in (15).
The corresponding pdf is:

fM(Y) =

(y) =

0 (ln Y-

a)

where go(.) is the standard normal pdf defined in (14).
The relations between arithmetic parameters and log
arithmic parameters are:

o.2

. in2[eP2 -- 1]

0{ m

R2
=~in(1+ 2&2T)
a =inm-R
/2

efl'

(18)

The lognormal pdf fy(y) plots as a positively skewed
distribution with a long tail allowing for very large y
outcomes, although with increasingly low probability of
occurrence, see Figure 3.

fy(y)

0

m

Yp

M

Fig. 3.

Lognormal density function

YI.p

Y

6

FUNDAMENTALS OF GEOSTATISTICS

If v, is the p-quantile of a standard normal distribution,
i.e.,
vp = G'(p), pE [0,1],
the p-quantile of the lognormal distribution (a, ,6) is:
, -- e+

'

(19)

entailing the following relation which can be used to deter
mine the logarithmic mean a from estimates of opposite
quantiles y,, yl-,:

P • YI-P ---e 2- = M 2 =* 2 a = In y, + Iny _,
1

in practice estimated by the proportion of pairs of data
values jointly below the respective threshold values z, y.
The bivariate (X, Y) equivalent of a histogram is a
scattergram, where each data pair (z,, y,) is plotted as a
point, see Figure 4.
The degree of dependence between the two variables
X and Y can be characterized by the spread of the previ
ous scattergram around any regression line, with perfect
linear dependence corresponding to all experimental pairs
(xi,,u), i = 1,.-., N plotting on that line.

(20)

where: M = y.5 : median of the lognormal distribution.
As a corollary of the Central Limit theorem, the product
of a great number of independent, identically distributed
RV's tend to be lognormally distributed. Indeed,
Y =ll
thus: Y

Yg
-.

nY

'I]

InY
-- as
Normal,
n

Y,

o.
0

Lognorrnal, as n -4o.

However, there is no a priori reason to believe that
the various sources of spatial variability of permeabil
ity/transmissivity are all independent, of roughly equal
variance, and are multiplicative. The traditional model
ing of permeability distributions by a lognormal model
is
more a question of convenience than a hard fact supported
either by data or theory. There exist many other possible
models for positively-skewed distributions of non-negative
variables.
Bivariate distribution
Up to now we have considered only one random vari
able at a time, whether Z,, or the sum Y =
= Z, of n
RV's.
In earth sciences, what is often most important is the
pattern of dependence relating one variable X to another
Y, relating the outcome of one RV X to the outcome
of
another Y. This pattern of dependence is particularly
critical if one wishes to estimate, say, the outcome
of X
(core porosity) from the known outcome of Y (well
log
datum).
Just like the distribution of outcomes of a single RV
X is characterized by a cdf Fx(x) = Prob{X < x},
see
relation (3), the joint distribution of outcomes from a
pair
of RV's X and Y is characterized by a joint cdf:
Fxy(z, y) = Prob{X <5z,

and Y < y}2 ,

(21)

X,
Fig. 4.

Pair (zx, ,) on a scattergrarn

Thus, the moment of inertia of the scattergrarn around
e.g. the 450 line would be a characteristic of lack of de
pendence, see Figure 4:
SNd

1

N

UYx

Y--]

(22)

This moment of inertia is called the "serni-variogram" of

the set of data pairs (x,, y,,),
t = 1,.-. ,N. The variogram, 2 Vrxy, is none other
than
the average squared difference between the two compo
nents of each pair. The greater the variogram value,
the
greater the scatter and the less related are the two vari
ables X and Y.
Just like one has defined the expected value of, say.

X, as the frequency-weighted average of the X-outcomes,

estimated here by:

N X,
Mnx= 71 E
sul

one could define the expected value of the product
XY
as the bivariate probability-weighted average
of the joint
outcomes XY = zy; more precisely and similarly
to rela

tion (7):

JOUPLEL

=1]

E{XY)

z+yd 2 Fxy(z, y) =

1N

zyfxy(zy)dzdy
2-

I N

1>
NZy

_ y.)2 I k7XY = T 1:

t=1

(23)

7

MXM)

t=1

in practice, estimated by:
1

I E
2-vxy= IN

N

_m2

+ [1~

N

?r~

'

E i=2

where: fxy(z, y) = d
density function (pdf).

is the bivariate probability

d 2 Fxy(x, y) = fxy(x, y)dxdy is the probability of oc
currence of the joint outcome
{X = x ± dz , y = y ± dy}. Similarly, the probability of
outcome of the pair (z,, yi) is 7 if N pairs were sampled.
The bivariate moment (23) is called the non-centered
covariance of the two RV's X, Y. The centered covariance,
or simply "covariance," is defined as:

Cov{X,Y}

= axy = E{[X-mxl.f [Y-my]}
= E{XY} - mx .my

(24)

in practice estimated by:

i=1

N1

1

with: mx

EN

,

E,

The variance, say, of X, as defined in relation (9) is
but the autocovariance of X with itself:

4 = Var {X} = Coy {X,X} = E{[X - mx] 2 } > 0
Note that although the variances a' and a2 are necessar
ily non-negative, the covariance axy may be negative if a
positive deviation [x - mxl is related, in average, with a
negative deviation [y - my].
The coefficient of correlation Pxy between the two
RV's X and Y is but a covariance standardized to be
unit-free:

=
=

axy

UXaY,

=

Cov{X, Y}
E[-l,+1
{5} • Var{Y})

(25)

It can be shown (Schwarz inequality) that the coefficient
of correlation is necessarily within [-1, +1].
Relation between covariance and variogram
Consider the bivariate data set (x,,y,),i = 1,.
and its moments:

N

E-j~[ix/-

mx -myJ + (m'x + m'

-

2mx my)

i.e.,

27xy= ax + cry + (mx- my)2 - 2ax

2!>0

(26)

Thus, whenever the variogram 2 -yxy increases, the covari
ance axy decreases: the greater the spread of the scatter
gram (xi,y,),i = 1,-.., N around the 45° line, see Figure
4, the larger "txy and the smaller the covariance axy and
the correlation coefficient pxy.
-txy is a measure of variability, while axy and Pxy are
measures of similarity.
Both measures r'xy and axy are dependent on a lin
ear transform of the variables X, Y, either a translation
X + b, or a unit rescaling aX. A more intrinsic, unit
free, measure of variability/similarity should therefore be
defined on the standardized variables:

X'= (X - mx)/ax and Y' = (Y - my)/ay
The corresponding statistics are:

mx, = my, = 0 , 4. = 4, = l

axr, = EE x
caxYmx

(27)

-rM
X Y - my) = Pxy E [-1' +1]

"x,=

1- Pxy E 0, 2]

The coefficient of correlation pxY is unchanged by any
linear transform applied on either RV X or Y.
Note that, when Pxy = 1 ==
yXy = 0, thus all
standardized pairs (.
A-.M,.
S
,-.,n are aligned around the 45* line.
Therefore:
-x(y - my) = ay + b
Zi= m + . Oy

(28)

A unit correlation, and more generally px2y = 1, char
acterizes two RV's which are linearly related. The coeffi
cient Pxy is a measure of linear (cor)relation between the
two RV's X, Y. However, zero correlation, i.e., Pxy = 0,
does not entail necessarily that the two variables are in-

8

FUNDAMENTALS OF GEOSTATISTICS

dependent; it indicates simply that their pattern of de
pendence cannot be modeled by a linear relation of type
(28).
Random function
In any of the previous discussions, the two RV's X, Y
can represent:
(i) either two different attributes measured at the same
location, say, porosity and permeability measured
from the same core plug;
(ii) the same attribute measured at two different loca
tions in space, say, porosity at locations x and x + h
distant of a vector h: X = Z(z), Y = Z(z + h); or
(iii) two different attributes measured at two different lo
cations, say, porosity at location x and permeability
at vector h apart: X = ¢(x), Y = K(x + h).
In all cases, the semi-variogram .yxy or -..e correl
pxy would measure the degree of variability/simi
between the two variables X, Y.
The second case (ii) is of particular interest for
tial interpolation problems, where a whole field of a
attribute, Z(z),x E Site A, has to be inferred (mal
a limited number of locations sampled for that
attribute.
Pooling all n(h) pairs of data found on attribu
over the same site/zone/layer A, these pairs beinj5 approximately distant of the same vector h (in length
direction), one could estimate the variogram charac'teristic of spatial variability over A:

z + h are in A. A(h) is the measure (area/volume) of that
intersection.
Just as the random variable Z(x) and its distribution
characterize the uncertainty about the attribute value at
location x, the random function Z(z), z E A, defined as a
set of dependent RV's, will characterize the joint spatial
uncertainty over A. The variograxn 27 (h) of that random
function (RF) characterizes the degree of spatial variabil
ity between any two RV's Z(x), Z(z + h) distant of vector
h. The variogran 2.y(h) is modeled after the spatial av
erage 2-YA(h) which can be estimated by the discrete sum
2jA(h) defined in (29).
The modeling of y(h) after the spatial average yA(h)
amount to decide to average statistics over a given area/
site/zone/layer A. That decision is necessarily subjective,
usually based on geological criteria, and cannot be proven
(or refuted) by statistical tests. Once the decision of mod
eling 7(h) after 7A(h) is taken, -y(h) just like 7A(h) is not
any more location (z) dependent within A:
2-t(h) = E{f[Z(x) - Z(z + h)]'} is independent of z E A.
(31)
The moment y(h) is said to be stationary. Stationarity
is a model decision, not some intrinsic property of the
actual distribution of z-values over A.
Once a model y(h) is obtained, the corresponding co
variance model can be deduced by the classical relation:
C(h) = Constant -.- (h)
Indeed:

Sand

2 7 (h) = E{[Z(x) - Z(x + h)12} =

[E{Z2(X)I
I
't=)Zxh

Y A(h) =

,n(h)

1-h)

[z(x.) - z(z. + hl)

By varying the vector h (in length and direction), that
characteristic is made into a vectorial function -y(h).
The experimental semi-variogram % (h) is not the estimate of some elusive expected value of squared incren nents
of a no less elusive set of random variables Z(x), Z(x + h).
It is the discrete estimate of a well-defined spatial int egral
defining an average over A:

m21

Var{Z(x

-m2)

+ [E{Z2x+h)}

2 [E fZ(x)Z(z + h)} - M2)

2

(29)

(32)

=

+ Var {Z(z+h)} -2 Coy {Z(x),Z(+h)

=

2 [C(o) - C(h)], thus: C(h) = C(o) - -y(h)
The constant of formula (32) can either be set equal to the
sill -y(oo) of the semi-variogram, model if it exists, or can be
set to any arbitrary large value. That arbitrary constant
should then be filtered out from all further algorithms, as
will be the case in ordinary kriging, see Lesson III, System

(4).
The very idea of geostatistics is remarkably simple. It

YA(h)

= A--- f

[z(z) - z(x + h)]) dx

(30)

with: AnA-h being the intersection of A and its tran slate
A-h by vector -h.
If z E A n A.h, then both x and

consists in the following sequence of steps:
(i) Define an area/site/population/.. A, deemed ho
mogeneous enough to warrant statistical averaging
within it.

JOURNEL
(ii) Scan all data available within A to calculate the
experimental h-characteristics of spatial variability,
i.e., calculate the experimental variograrn values (30).
(iii) Smooth and complete the partial experimental im
age thus obtained into a model -y(h), available for
all interdistance vectors h.
(iv) Use this model y(h), or the corresponding covari
ance model C(h) for traditional, well-proven regres
sion techniques, also called "kriging", see Lesson II
and III.

9

h
C(h)

The most important steps, and by far the most consequen
tial of any geostatistical study, are: step (i) - decision of
averaging or stationarity, and, step (iii) - the modeling.
Step (iv) is but well-known calculus.
The decision of stationarity is implicit in all statistics,
it is not particular to the geostatistical approach. That
decision allows defining a pool of data (area A) over which
experimental averages and proportions will be calculated
and assumed representative of the population as a whole,
not of any particular individual location (z E A).
In the practice of reservoir characterization, that deci
sion corresponds to the traditional split of the reservoirs
into zones/layers deemed homogeneous enough by the ge
ologist. Of course, that split should not be so fine as
to retain one zone per datum, for then no averaging (no
statistics) is possible.

I

",

0

h

82

Fig. 5. Anisotropic variogram and covariance
oa: direction of continuity
02: direction across continuity
&I: distance at which spatial correlation vanishes in
direction al:

C(Ihl,oa) = 0, "y(IHL, al) = sill value = C(0),
for all IhJ > &1
Let:

Typical variograms

a. Z(x.),

Y =
at=I

Variogram and covariance as defined in relations (23)
to (26) are functions of the vector h (length and direc
tions). When that function depends only on the length of
the vector h, the model is said to be isotropic. When it
depends also on the direction of the vector h, the model
is said to be anisotropic. The variograms in a direction
(oa) along continuity, say, of a sedimentation channel, will
present less variability than in a direction (a2) across con
tinuity; correspondingly, the covariance or correlogram
will indicate greater correlation in the direction of sed
imentation, see Figure 5.
The variance operator
Knowledge of the covariance function C(h) for a ran
dom function {Z(z),z E A}, or the covariance matrix
fC.oa,,6 = 1,.. ,] nfor a set ofn RV's
Z.,.a = 1, .. ,n,
not necessarily related to the same attribute, allows cal
culation of the variance of any linear combination of the
component RV's.

or

Y=E2a.Z.
&.1

then:
Var{Y)

=E =
2c.=i
=&_

= a,.a¢C(. - z 0 ) > 0
Er a~aoC,3 2! 0

(33)

A variance is necessarily non-negative, thus the previ
ous expression must be non-negative whatever the choice
of the n weights a., possibly negative. The variance
is
nought only if all weights a. are zero, assuming that none
of the component RV's are exactly constant. Thus,
the
covaziance function C(h) or the covariance matrix [C.,6]
must be such to ensure the positivity of the variance
op
erator (33) whatever the weights a. chosen: This
is the
condition of "positive definiteness" of either the function
C(h) or the matrix [C.,O]
Not all functions nor all matrix of numbers can
be
taken as a covariance/variogram model, respectively,
as a
covariance/variogram matrix. When modeling an exper
imental covariance/variogram, a valid function C(h)
or
7 (h) must be chosen.

10

FUNDAMENTALS OF GEOSTATISTICS

N-

Selected Bibliography
Gardner, W. A., 1986, Introduction to Random Processes,
MacMillan, 434 p.
Isaaks, E. and Srivastava, R., 1988, "Spatial continuity
measures for probabilistic and deterministic geostatis
tics,' Math. Geol., v. 20, no. 4.

Journel, A. and Huijbregts, C., 1978, Mining Geostatis
tic, Chap. II-III, Academic Press, 600 p.
Lindley, D. V., 1965, Introduction to Probabiity
and
Statistics: Part 1, Cambridge University Press, 259 p.
Thomas, J. B., 1986, Introduction to Probability,
Springer-Verlag, 247 p.

Lesson II: Linear Regression Theory or Simple Kriging
(SK)
The basic idea of linear regression is to estimate an
unknown value Zo by a linear combination of n known
values Zo, a = 1,..-, n. These n known values may cor
respond to the same attribute as Z0, in which case one
speaks of "kriging," or to attributes different from Z0, in
which case it is "cokriging." From an algorithmic point of
view, there is no difference between kriging and cokriging;
the difference appears only at the stage of inference of the
covariance/variogram models required for solving for the
linear regression weights.
Consider an unknown value z0 , interpreted as an out
come of a random variable (RV) Zo.
The n data values za,a = 1,..., n are themselves in
terpreted as n outcomes of n RV's Za, c = 1,..., n.
The (n + 1) RV's Z0, Z1,.. , ZA are characterized by
*" their means, for now assumed known:
Ef Z} = mo, a = 0,...,n

(1)

= EfZ.,z•}-,.mo=Co
= forall c,B=0,.--,n.

One can also define the standardized covariances or coef
ficient of correlation between any two variables Z., Zp:
ca

E[1

1

".,n]

(4)

from the data-to-unknown covariance column k (n x 1):
rkT= [C 0 ,

=1,...,n•]

(6)

Z;=A0 +

z; is said to be the "estimate." Corresponding]y. the lin
ear combination of the n RV's Z. is itself an RV called
the "estimator":
Z; = A, + •

(7)

AaZa

The actual error, zo - z;, is usually unknown and little

E{Zo - z;} = E{Zo} - E{Z}
= E{Zo} - \o -

(5

A.E{Z 0 } =

- A0 - f

A,

We would wish that expected error to be zero ensuring
0 is taken as:

"unbiasedness," thus the shift parameter A
A =o

(3)

We will distinguish the data covariance matrix K (n x n):

K = [Co , a,0 =

The unknown value z0 is estimated by a linear combi
nation of the n data plus a shift parameter A0 :

can be done about it. However, some moments of the RV
error, Zo - Z2, can be calculated thus acted upon.
The expected value of the RV error is:

The (n + 1) variances are none other than the autocovari
ances Values:
Var {Zo,}{=•{
'}
a-=co°
, &=O,...,n (2)

p0a0 =

Linear estimator:

O=1

* their covariances:
Cov{fZ,Z~gl

The critical step of covariance/variogram inference and
modeling allows determination of all the required covari
ance values.

Aarn*
\0=1

Thus:
2; = m +

~AO[ 0 -in.]

(8)

The unbiased estimator Z; appears as the result of a linear
estimation of the residual [Z0 - mo] from the residual RV
data [Z - r.]:

11

JOURINEL
[Z; - moo = [Zo - mo] =

.[Zo - m.)

(9)

In matrix notations, the SK system (12) is written:

boml

n.
There remains to determine the n weights A,, a = 1,..
For this, we will act upon (minimize) the error variance.
Viewing the RV error (Z 0 - 2;) as a linear combination
Y of (n + 1) RV's:
[,o - mo] - [Zo - rm]°

Y = Zo - Z=

(10)

fi Y•n

Var Y =

oao

(11)

aOfi•=

We would wish the error variance to be the smallest pos
sible. Thus, the n weights a,, = -A,,,a = 1,...,n are
chosen to minimize Var Y. This is done by setting to zero
the n partial derivatives of Var Y with regard to each of
the n parameters a. , a = 1,..-,n:
n

aoCao +

= E aoC,

2 Oat

,6=0

=

[•1

'

=

k

(14)

matrices defined in (4),(5)

Defining the residual data matrix

X• = [&1 -M,,...,Zn - M] ,

The error variance is expressed as a double sum of the
covariances C.,, see Lesson 1, relation (33):

IaVar Y

with:

a.[Z. - ma.]

l

ao

with:

KLA=k

a,6C,,
0=1

AoC.o =0 , for all a=1,-.-,n

C.0o-

the estimator (9) is written:

[z 0 - Mo]" =A, . B. = IT

B-

(15)

since: K = KT (the data covariance matrix is symmetric),
L-2 = (&-])T.

The minimized estimation variance is:
1

2

Coo - A

k-k = Qo - j.

•k

The system, (12) or (14), yields one and only solution
as soon as the data covariance matrix K is positive def
inite, in practice as soon as no two columns (or rows) of
that matrix are identical, i.e., there is no two data totally
redundant:

0}=i

Ca = Coa, for all 0 , if and only if: a = a'.
We have just established the well known "normal system"
of equations, also known as "linear regression" equations,
and belatedly renamed "simple kriging" (SK) system:
(12)

n

1

4,C0 = C0,

The corresponding minimized error variance, also
"simple kriging" variance, is then written:

E

VSK =var{zo-z}
{aa
=Z
n

Cnt

Independence case
Assume that the n RV data Z4 , a = 1,...,n, are
independent one from another; they are then called "in
dependent variables," the unknown RV Z, being the "de
pendent variable."
The data covariance matrix K thus reduces to a diag
onal matrix with for diagonal elements the data variances:

aemO$=0

C-oo-

Of course, K should be a covariance matrix, i.e.. the
n 2 values cannot be picked arbitrarily, see Lesson I (33).

ft

>-C.co +
+ F.
0=1

al

aw

K (Ca~pI =

= 21:A...+
m
0=1

=I

.-

1
J"0, for a
i~eC. = / Ca., for ao=
C.0

A0C 0 = C0

A0

i

aul

The SK system (12) is then written:

Finally:

4x =Var{Zo- Z;} =Cco0-

O_.C
0
0=1

•mA%

A.aCo = C 0o
* A\ = C.0/Coo , a=
Introducing the coefficient of correlation:

12

FUNDAMENTALS OF GEOSTATISTICS
oo/Cv
*

P.o = C

*=

A =

n

a
•,

Zoo0

Ono 3=
%1,.
-MI =o I=I ,,[." m
r.1
L

_zr[0

and the SK estimator (9) becomes:

[-o -

WW

=

LL

Ml.[I

mrl

Pio = aio/Oi•o E [-1, +1]

M.]
mo-adi

Z'

i.e., with the standard deviation notation: 0'.

rno, rni, a'2, 172, jlo are the experimental statistics of the L

pairs (Zi)
[Zo -_Mo=
t

E' oIPa

[Z. -ma
a,

(16)

In presence of independent data, the SK (regression) weights
A.,

a = 1,. . . , n, are none other than the correlation co

efficients p0, between each datum and the unknown (de
pendent) variable Zo.
The greater the correlation po0 of a particular datum with
Z0, the greater its weight.

z W
))

i = 1,..., L, available.
The exactitude property
Consider the case when one of the data, say, Z.,, is
considered the unknown:
., =_ Z0 . We would then wish the SK estimate to identify
that single datum.
In such a case: C.., = C"0 , for all a = 1, .-- ,n, and
the SK system (12) becomes:

Note that:
SACo +

=PLo # 1 , usually.

}=Cc®[1- E.p0oI

SK = CoO [I1- Fo

ACo = Co, a =

The unique solution is: Aw, = 1, Ae = 0, for all 0 # a'.

Moreover, if the n data are independent of the unknown
Z0, i.e., if Pgo = 0, the SK estimator reduces to the mean:
Zý = Mo.

Thus, the SK estimate is:

Example.

Spatial interpolation

n = 1(1 single datum ZI).

Expressions (12) or (16) reduces to:
[Zo -Mo
Oo

P10[Z,F

I

rn,]
a"i

,-,n

ZZ = Zo,, and cK= 0

(17)

Spatial interpolation corresponds to the case where the
n data Z, relate to the same attribute Z but at different
locations z. i zo.
For example, the porosity Z(xo) at unsampled location
zo is to be estimated from n porosity data Z(: 0 ) taken
at n surrounding locations xo:

i.e.

I

Z; = M'o + P10o•(Zl - MI)
which is the traditional expression of the regression line
related to a scattergram
(oW,-1 ," = 1,. -.. L), see Figure 1:

[Z(.) - mI

Z*(xo)-m=
-

(18)

with m being the common expected value of the (n + 1)
RV's Z(zo), Z(zx), a = I,

...

, n. In practice, m would

be the mean of all porosity data available within the same
layer or homogeneous formation.
Defining the covariance function C(h) as the covari
ance between any two porosity RV's Z(z), Z(z + h), dis
tant of vector h but still within the same formation:

z0

C(h) = Co- {Z(z),

Z(

+ h)}

(19)

The simple kriging system (12) is:
ZI

0

SA
0 C(Zo-Ze)=C(zo-Zo), a=1, .- ',n
Fig. 1. Scamrgm
m(ZO7 Z1 )

(20)

JOURNEL
mak The SK estimator (18) requires prior knowl
edge or estimation of the common mean m.
That prioi
step can be pooled with the error variance
minimizatior
step into a single algorithm, called "ordinary
kriging," set
Lesson III, system (4).
Simple cokriging
When the n data Zo relate to one or more
(up to n)
attributes different from the attribute corresponding
to
the unknown Z0, the previous SK process is called
"simple
cokrigin&."
For example, ZO may be the sought after core
poros
ity at any particular given location. The
Z.'s may be
n porosity-related well log data at that same
location.
The corresponding simple cokriging estimate
is none other
than the traditional linear regression of core porosity
from
the log data:

13

K

n = no + Enk
k= I

The corresponding okriging system to determine
the
n weights, A.0 , A&A, is no different from
the general SK
system (12) except for the notations differentiating
each
type of log data, see hereafter System (22).
The traditional regression using only the data avail
able at the location to be informed fails
to use existing
spatial correlations existing between further
away data
and the attribute Zo(z.,) to be estimated.
Simple cokriging system

40
for

0C

k=

1
ko = ,..,no

En

AO
1, C.,
00 , = C000

(22)

0

[ZO - mo] = •

,Ao
[Z - mto)

, according to (9)

0=1

Remark. If one of the data, say, Z,,, happens
to be
either the sought after core porosity, or !og
datum per
fectly correlated to the core porosity Z , -ze
previous SK
0
estimate would identify that datum:
Z; E Z,,, , see the exactitude property (17).
The previous linear regression considers for
estimation of
the core porosity Z 0 only those well log data
at the same
location. In fact, there also exists core data
at other lo
cations. If these data at other locations are
correlated
to Z 0 . they should be included to improve the
regression
defining a full cokriging pattern.
Consider for the estimation of an unknown
porosity
Zo(xo) at a location zo the information constituted
by:

"*no core porosity data Zo(z. 0 ) , ao = 1 , .,no

Z n =,0+kA,C.. ,a, + Z
,K~lZ'1
• = A ,C. ,"
Xk=
1
01, 1, 1 = C01,,0
for ok,=l,...,n,, and k'=1,...,K

with:
C-*4 = Coy {Zo(Z.°),
C-. 0 , = Coy {Zo(X0 .)

, k= 1, ... fnk , kl =

1,. .. , K coming from various locations z,, corre

sponding to either cored or simply logged wells.

The generalized linear regression, or "cokriging"
estima
tor, is written:
[Zo(Xo) - m0J"

=

+

A•.0

E

[Zo(°z0 ) - M0 ]

AL,[Zk(,..) - in1
A..

(21)
This expression is no different from the general
SK ex
pression (9). The total number of data being:

,zk(XO.)}

C 0oo= Coy {Zo(z 0 o), Zo(zo)}
C-.,1% = Coy {Zk'(Zo ,,),

Zo(z&)}

C.",C' = Coy {Z.,(x0 ,,) , Zk(X.)}
C.. , o = Coy {Zk',(Z.,),

Zo(Xo))

In matrix terms, the general system (14)
holds true with
simply the various matrices being partitioned,
e.g.,

[K

I

obtained from neighboring cored well(s)

" K sets of log data Zk(z°,)

Zo(r•)}

[oo1

...

7

(23)

Data orthogonalization
In most practical situations, the n data Z.,a = 1,..,
n
are dependent on each other. The idea is to define from
the initial set of dependent data Z., a = 1,.. .,n,
a new
set of independent data Y,,a =' 1, .,*,n, on which the
simpler regression (16) could be applied.
Given any initial data vector: Z! = [Z ,
Z,,] with
1
non-diagonal covariance matrix K, there exists
a linear
transform Y- = A. Z which will transform Z into
a
vector X with an identity diagonal covariance matrix,
i.e., the n transformed RV's L are uncorrelated.
The initial matrix , being a covariance matrix always
admits the following LU decomposition:

14

FUNDAMENTALS OF GEOSTATISTICS

K= LL.

(24)

with: L being a lower triangular matrix [\2J
and:
=IT
L being an upper triangular matrix
[Golub and van Loan 1983, p. 54).

Dual kriging
Going back to the matrix expression (15) of the SK
estimate:

-\],

[Zo -ml•]

=•Ao[Zo -m ,1o]=kT. K-I. te,

The inverse of,& always exists and is written:
K-=

since,& and K'-are symmetric it can be rewritten as:

=

Consider the linear transform A =L-1:
.=L-

=*
*.7Q

[Zo-m] =(i .~.L T=

K .
(26)

=T--

(25)

The covariance matrix of the Y.'s is written:
with: bT

=

L L--=

=

IZE)L- = L

fb,i,.

Thus: k =
. , and the coefficients b. are given by
the system, called "dual SK" system:

Thus, the transform Y = L-1 .e, "uncorrelates"
the
data. The new data Yo,, however, remains correlated
to
the unknown RV Zo, with for covariance matrix:
-y = E{ZoL} =

-

{z0

b_] =-.,. K-'

.Kk=&t , i.e.

(27)

L=

' .k
Consider the SK estimate based on the Y., data:
[z 0 - mo0

=

V-, [yO - my.]

,L

.'L

The corresponding SK system (14) is written, with

K(Y = L

L .=ky
Thus: Zo_- mo0f" =k_
i.e. [Z0 -

1=ky
L

. L-

(jT*-,L-,) .e

no]*" =_[Zo - mo]° , as defined in (15).

Thus, the SK algorithm as described by the system
(14)
is the sequence of two steps:
(i) an orthogonalization of the data
Y.a =4'-

Z. , with: K

_.•.

T

(ii) a straight weighting of the orthogonal data
Y. by
their correlation with the unknown Zo:

The dual expression (26) and the dual system
(27) are
both extremely similar to the original (primal)
expression
(15) and system (14). Instead of being read as a
linear
combination of the n data (z. - m.,), the SK estimate
is
read as a linear combination of n interpolation functions
which happen to be the n data-to-unknown
covariance
functions Coo.
In the case (18) of a single attribute Z(z) distributed
over various locations of a formation, relation (26)
is writ
ten:

Z'(zo)-m =

A.°[Z(x.) - ml =

bb,,C(ze°-.o) (28)

and the dual system (27) amounts to ascertain that
the
interpolated surface Z'(z0), zo E A, identifies the
data
values z(x,) at data locations, i.e. when zo =
z.,:
Z'(-.) - m =

E.1 boC(z# - z.) = r. = z. - m
forall a=1,.-.,n
(29)

[Zo - mo]" = kyT [-(,Y
- my,.
The first step accounts for the redundancy between the
data Zo. Once that redundancy is corrected, a
straight
forward correlation weighting, of type (16), is performed.

Spline interpolation amounts to replace in expression
(28) the covariance functions C(ze - x) by extremely
reg
ular and smooth functions f(z,, - z), then ensure data
identification through a system of type (29):

JOURPNEL
Z'(x) = b0 + Z bof(z

- x)

a=1

with:
Z'(x.) = bo +

= bf(x0 - z 0 ) =

datum z(zx)
for"a a=1,.,n

(31)
Then one may argue on the arbitrariness of choosing, in
the SK case, covariance functions for interpolation func
tions. As was shown before, the choice of covariance
functions allows to minimize the resulting error variance,
Var {Z 0 - zZ }.
Selected Bibliography
Aboufirassi, M. and Marifio, M., 1984, "Cokriging of
aquifer transmissivity and specific capacity," Math.
Geology, v. 16, no. 1, pp. 19-36.
Buxton, B., 1982, Coal Res•rves Assessment: A geosta
tistical case study (M.S. thesis), Bruxnnr Earth Sci
ences Library, Stanford University, 144 p.
David, M., 1976, "The practice of kriging," in Advanced
Geostatistics in the Mining Industry, ed. Guarascio,
et al., publ. Reidel, Holland, pp. 31-48.

15

Davis, M. and Culhane, P., 1984. Contouring very large
data sets using kriging. in Geostatistics for Natural
Resources Characterization, ed. Verly et al., publ.
Reidel, Holland, Part 2, 599-619.
Delhomme, J. P., 1978, Kriging in hydroscience. in
Advanced Water Resources 1(5), pp. 251-266.
Doyen, P., 1986, "Porosity mapping from seismic
velocity data: An approach based on spatial corre
lation functions," submitted to Geophysics.
Draper, N. and Smith, H., 1966, Applied regression
analysis, Wiley & Sons, 709 p.
Golub, G. and Van Loan, C., 1983, Matrix computa
tions, John Hopkins Univ. Press, 476 p.
Journel, A. and Huijbregts, C., 1978, Mining Geosta
, Chap. 5, Academic Press, 600 p.
t
Lapin, L., 1983, Probability and Statistics for Modern
Engineering, Brooks/Cole publ., 624 p.
Myers, D. E., 1984, Cokýiging - New developments, in
Geostatistics for Natural Resources Characterization.
ed. Verly, et al., publ. Reidel, Holland, Part 1,
pp. 295-305.
Weisberg, S., 1947, Applied Linear Regression. Revised
ed. 1985, Wiley & Sons, 324 p.

Lesson III: Linear Regression Under Constraint(s) and
Ordinary Kriging (OK)

In all the simple kriging (SK) developments of the
previous Lesson II, the mean(s) were supposedly known,
cf. relations (9) and (18). Consider, for example, the
SK estimator (18) of an unknown Z(zo) from n data
related to the same attribute but at different locations

and should be set to zero, whatever the unknown value of
rn. This can be achieved only if:

{AO
0
:=I A,. = 1
=

Thus, an unbiased linear estimator of Z(zo) is written as:
a

Z'(xo) - M

ao=l

a=1

The common (stationary) mean m is supposed known.
If the process Z(z) was sampled at all locations x within
the formation A considered, that mean m would be known
but also there would not be any estimation problem left.
Thus. in practice, either m is to be estimated prior to
the SK algorithm, or an estimation algorithm should be
designed which does not require prior estimate of that
mean.
Consider the linear estimator:

n1

Z'(xo) = E A.Z(Zo) , with:

(1)

[fZ(X,,) - M)

£

A,, = 1

(2)

ol

Note. Although for reason of convenience, we have used
the same notation A. for the SK weights of relation (1)
and the weights of relation (2) they are not equal.
The error variance is written, as in Lesson 11 (11):
S=

Var I

with: {1 a* = -- A,(a

aoa

=

-

a)(3)

no0==0
,.,n

and: C(Z. - Zs) = Coy {Z(Zo), Z(zp))
Z'(.o) = A0 + • A.Z(z 0 )
The error mean is:

E{2(Zo)-Z"(z0 )} = m-,A-• Am = -A 0+m

1-E

We wish to minimize that error variance, still ensur
ing unbiasedness of the estimator, i.e., ensuring the con
straint: E'. Ak.= 1, This amounts to an optimization
A) under linear constraint. The Lagrange formalism will
be used whereby a function, of the (n + 1) parameters

FUNDAMENTALS OF GEOSTATISTICS

16

A,, a = 1,..
defined as:

n and 2 a being the Lagrange parameter, is

{ A.C(O)+ ' =C(zo- ),a = 1,..,n
5"c., A13 = 1

S(A,,,a =1.,n;

a) = a' + 2p L•_n A. _ 11

i.e.

The extreme of that function S is obtained by setting to

=

C({• - . L
)

.- . -

C(O)

C(O)

(6)

Z.- o)

zero its (n + 1) partial derivatives:
-='

I 8S =

qOK

=An - 1 =0

The n first equations are rewritten:
C(x, - xo) - t
13=1

E AcC(xo - x3) + u = C(xo - xo) , a = 1,..., n.
8=1

Finally. the (n + 1) unknowns, the A.'s and y, are giver
a system of (n + 1) linear equations, known as the "(
strained normal" system of equations, belatedly renar
"ordinary kriging" (OK) system:

= C(o) [1 - E:"=I p 2 (Z,. - Zo)]
-', [1 - ELIP(.-o)]
0>
0

-p = C(O)/n

(4)

The corresponding minimized error variance, also caRled
"ordinary kriging" (OK) variance is written:
(5)

= C(o) - ELI AC((:o - X.0) - ' > 0
Just like the SK system of Lesson 11 (12), the OK
system (4) presents one and only one solution as soozn as
the data covariance matrix K = [C(z0 - x,3)] is posi tive
definite, in practice as soon as:
(i) the covariance function C(h) is licit, i.e., is a posi tive
definite function, see Lesson I condition (34)
(ii) there are no two data locations totally redundi 3nt,
i.e.,
= C(Z,,, - XO) , for allu

- Zo)]

{A,.=l/n,a=1,...,n

E{ =, AC(Xo - z,) + # = C(x,, - Zo) , a =

C(zc, - XO)

X(°

with: p(h) = C(h)/C(O) E [-1,+1] being the standard
ized covariance, or "correlogram," measuring the correla
tion between two values Z(z), Z(z + h) distant of vector
h.
If, moreover, the n data are uncorrelated with the un
known RV Z(xo):
p(x. - x0) = 0 , a = 1,-- ,n, the OK system (6) then
yields:

A8C(x, - zx1 ) - p = 0, i.e.,

2} =
a2OK = E{[Z(xo) - Z-(: 0 )J

C(O)[1 - ELI

(7)

Thus: Z() = E'. Z(x:), whatever the location x0
and:
= C(0) + Cn(o) > 0
In the case of total spatial independence, the OK es
timate reduces to the arithmetic mean of the n data re
tained.
In the SK case Lesson 11 (16), the SK estimate did
reduce to the known mean m0 . In the OK case, that
mean is unknown and is estimated by the mean of the n
data.
The exactitude property
If one datum, say, Z(ze), is considered to be the un
known Z(zo), then:
C(x, - zo) = C(zo - zo) , for all i=1,..,n, and the
OK system (4) is written:

E"., AaC(Z, - zo) + AoC(xz, - ZO) + p = C(Z, - =o)

=1.

if and only if: a = a'

Independence case
If the n RV data Z(x,) are independent (or unc...

'

related) one from another, the data covariance mat
[C(zo - x:)] reduces to a diagonal matrix with all
ements of the diagonal equal to the common variam
C(0) = Var {Z(:)}, for all x.
The OK system (4) becomes:

The unique solution is: Ae, = 1, A0 = 0$ # a',6 = 0.

Thus, the OK estimate identifies the datum value Z(o,) =
Z(zo), and:
0K = C(o) - A;.C(o) = 0

(8)

As SK, the OK algorithm features the exactitude prop
erty: the OK surface, Z'(zo) ,:o E Domain, honors the
data values at data locations.

JOURNEL
Similarity SK-OK

17

After some elementary algebra, it is found that:

Rewriting the expression Lesson 11 (18) of the SK es
timator as:

1

C(z 2 -X..0 )
1 C(I I -ZO)C(O)-C(.x-, 2)

2= 2
Z;E(2o)

Introducing the correlograrn p(h) = C(h)/C(0), it comes:

AZ()

)m+

-

(9)

It appears as a Linear combination of (n + 1) data, in
cluding the known mean m, with the sum of the (n + 1)
weights being equal to 1 by construction. In the OK case,
the mean is unknown, thus there are only n weights which
must sum up to 1, calling for a constrained optimization
and a Lagrange parameter.
Example.

The unbiasedness condition (2) fully determines the
unique weight: A, = 1, and: Z*(zo) = Z(xj).
The OK system (4) determines the Lagrange parame
ter value:
o)

= C(X

2

2

-

pXZ2 - --0)

1- P( 1 - z 2 )

As expected, the weight A given to the datum Z(z,) in
creases as
(i) the correlation p(xl - zo) of that datum with the
unknown Z(xo) increases
(ii) the correlation P(X2 Z(xo) decreases.

n = 1 (1 single datum)

AC(0) + . = C(z, -

= 1 P(z1 .. zO)-

A

Zo) of the other datum with

Similarly, it is found:

C(0)
2= Jp(X

- Zo) + p(Z 2 - -O) - p(I -i)

-Z1]
(12)

o) - C(0),

Thus:

OK/C(O) = 1 - AP(• 1 - Xo) - (1 - A)p(r 2 - ZO) -- lC(0)

o,< = E{[Z(zo) - Z(,,)])} = 2 [C(O) - C(z 1 - So)] > 0

creases, i.e., the estimation precision increases as

Thus, and'as expected, the estimation variance ao2K de

More generally, the basic estimation variance of one
value Z(z) by another Z(x + h), a vector h apart, isby
definition the "variogram," see Lesson 1 (31):

(i) the two data are better correlated with the unknown
Z(zo), i.e., the two correlations p(x1 --o)
and

2-1(h) = E{[Z(x) - Z(x + h )]2) = 2[C(o) - C(h)] (10)

(ii) the two data are less redundant: if p(xl - x2) de
creases, Mwill increase and aK decreases.

Example.

n = 2 (2 data), see Figure 1

Z(x0)
Z(x2) 0
(14.)

Z(X )
00(k

,XX2 - So) increase

As already pointed out in the previous Lesson II, the
regression (kriging) algorithm accounts not only for the
respective correlation p(x. - zo) between each datum and
the unknown, but also for the degree of redundancy be
tween the data through the data covariance matrix K.
If the two data of the last example are uncorrelated
(no redundancy): p(zX - Z2) = 0, and expression (11)
gives:

Fig. 1.

Regression from 2 data

The unbiasedness condition defines the OK estimator as:
Z'(Zo) = AZ(sa) + (I - A)Z(X2 )

The OK system (4) is written:

{AC(0)

+ (1 - A)C(X1 - Z2) + A = C(z 1 - Zo)
AC(X1 - X2) + (1 - A)C(0) + P = C( 2 - Xo)

A=

+ I [p(z, - Xo) - P(X2 - So)1

The weight A increases directly as the excess of correla
tion [P(zi - Zo) - P(z2 - XO)] of datum Z(x1 ) over datum
Z(Z2). If, moreover, the two data are equi-correlated with
the unknown: A = (1 - A) = 1/2, and:

3

fo&KIC(O) = -

- 2p(z, - zo)

If, moreover, the two data are uncorrelated with the un
known

18

FUNDAMENTALS OF GEOSTATISTICS
p(rI-o)=
oK =

p(X 2-zo)=0

iC(0) = C(O) + C
2

,
,

and:

(13)

as it should.

Kriging in presence of a trend
In presence of an unknown, but constant, mea
the condition E. A. = 1 allows to filter out that con stant
m from the OK system and in the same time acl ieves
unbiasedness.
One can generalize that concept and seek for eondi
tions on the n weights A. that would filter out a more
complex trend function m(x) = E{Z(x)} that woulId be
location x-dependent.
Consider a trend function, i.e., a smoothly varjable
function of the coordinate(s) z, which is of known shape
but with unknown parameters:

m(z,y) = ao + a,(x + y)

At data locations Z.,ck = 1,... ,n, only the Z(zo) val
ues are known, not the trend values m(zo), because the
notion of trend is usually a model of the experimenter
not necessarily intrinsic to the spatial distribution of the
attribute Z.
The problem consists of estimating at any unsampled
location zo 9 Zx, the unknown value Z(zo) ensuring the
unbiasedness condition:

E{Z(zo) - Z(zo)} = 0 , i.e.
E{Z*(zo)} = E{Z(zo)} = m(xo) =

L

alfl(zo)
t=0

Consider the linear estimator:
Z'(xo) = E A.Z(z.)

(16)

a-1

L

m(z) = Zalf,(z)

(15)

(14)

t=0

where the f/(x)'s are known functions of the -oordina te(s)
and the at's are unknown parameter values making the
trend value rn(x) unknown.
By convention, the zeroth term is the constant unc
tion, i.e.,
fo(z) = 1 , for all z
The case of a constant mean, m(x) = m, would c off e-L
spond to L = 0, and a0 = m either known (SK case )or
unknown (OK case).
Examples. In RI (e.g., 1 dimensional time space), the
fl(t) can be cosine functions of given frequencies wt:
fl(t) = cos 27rwtt

In R2 (e.g., horizontal space), the ft(zI,X 2 ) are
usually taken as low order polynomials, up to order 2 f!or a
quadratic trend:
m(z, Iy) = ao + al. + a2Y1 + a 3 X2 + a 4 y 2 + aszy

corresponding to the choice L = 5, and:
fo(Z,Y) = 1, A (z,Y) = X, M2(X,Y) = Y

Note. Again, for reason of convenience, we have adopted
for the weights the same notation A. used for the SK
estimator (1) and OK estimator (2). Although sharing
the same notation, these weights need not be equal.
The expected value of the RV error is to be set to zero:

E{Z(zo) - Z'(xo)} = m(xo) Eat [f(xo) -

Aof(z,)
o

=0

Unbiasedness, whatever the unknown parameters at's, is
thus achieved by imposing the following (L + 1) conditions
on the n weights A.:

A)f,(z.) =fe(zo) , for all

=0,..,L

(17)

-1

* The OK case corresponds to L = 0, fo(x)
= 1, thus
to the single condition: E. A. = 1
* The case of a diagonal linear drift (15) would call
for two unbiasedness conditions:
EA.=1, and:
a

Y) = X2 , f(,
M
y) = t, fs(Zy ) = ZY
A linear trend in R2 would be defined as:

A,-m(:o)

a

f3(,

m(z, y) = ao + a,- + a2y

If that linear trend is known to be limited to the
450
direction, the number of parameters can be limited to
two, i.e., L = 1:

We wish now to minimize the error variance under the
previous (L+ 1) linear constraints (17) on the weights A0.
The Lagrange formalism, used to establish the OK system
under one single constraint, is repeated similarly except
for (L + 1) Lagrange parameters / 0, pj,t = 1,.. ., L.
The resulting system of (n + L + 1) equations with
(n + L + 1) unknowns, the n weights A. and the (L + 1)

19

JOURNEL
Lagrange parameter ju, is but a high order constrained
"normal"system, belatedly renamed "universal
kriging"
(UK) system:

This last condition simply says that, e.g., estimation
of a linear drift in, say, the x direction cannot be done if
all n data are aligned in the y-direction.

{

The intrinsic random functions of order L

E•=

r 0 ) + X~10

A•C~o -

E0=1 A•df(x,)

= C(z 0 -TO)

f,(94)

= f,(xo) , t = 0,

Consider the random function Z(z), with covariance
function C(h) = Coy {f(z),Z(x + h)) and the linear
estimator

.. , L

n

The correspondihng minimized error variance, or "kriging"
variance, is written:
a.?,

-

2

E{[Z~xo) - Z(xo)J }

= C(o) - E

A 0C(Xo -

Xo) -

2(zo) =

The estimation error Y can be seen as a linear combina
tion of (n + 1) RV's:

Z d:Z(zT)
Y = Z(Z0 ) - Z'(XO) = 0=0

EL 0 ,(xo)

(19)
Remarks. The adjective "universal" stems from the (L +
1) unbiasedness conditions (17), also called "universality
conditions" for they ensure unbiasedness whatever the un
known trend parameters at's. This adjective "universal"
is unfortunate, for there is nothing universal nor intrinsic
to either the conditions (17) or the constrained system
(18): they depend on the arbitrary choice of the trend
component functions fi(x)'s.
A better, although more cumbersome name, for the
system (18) and the corresponding algorithm would be
"kriging under trend constraints."
The system (18) reduces to the OK system (4) in the
case of a constant-but known mean, L = 0 and m(x) = m.
The covariance C(h) used in system (18) is that of the
residual data R(z 0 ) = Z(zo) - m(x0 ). Since the trend
values m(xz0 ) are usually unknown, the residual data are
also unknown and their covariance C(h) cannot be in
ferred directly. The formalism of "intrinsic random func
tions of order L" has been designed for inference of C(h)
directly from the original data Z(zo), see discussion in
the next section of this lesson.
In most practical situations, within a site or forma
tion there exists subzones or privileged directions within
or along which the trend effect m(z + h) - m(x) can be
ignored, at least for short interdistances h. In such sub
zones and/or directions, the R-covariance C(h) can be
inferred directly from the Z-data.
The system (18) presents one and only one solution if:
(i) the data covariance matrix K = [C(z 0 - zo)] is
positive definite, which requires that the covariance
model C(h) be itself positive definite.
(ii) the (L + 1) functions fe(z) are linearly independent
on the set of n data.

A.0Z(X()

(20)

with:
do

1,

for a = 0
- ,for~a=1,...,n

The error variance, i.e., the variance of the linear combi
nation is given by Lesson I (33):
Vary = E E d.d0 C(z0

-. zT)

00

(21)

a=0,6=0

A variance being necessarily non-negative, the covari
ance function C(h) must be such that the expression (21)
be positive whatever the choices of the (n + 1) weights
do, positive or negative. This is the well known "positive
definite" condition for a function C(h) to be a valid co
variance function. Zero variance is obtained only when
all (n + 1) weights d, are zero.
Calculation of the variance (21) requires existence and
knowledge of the covariance function C(h), which required
prior knowledge or estimation of the constant mean:
E{Z(x)} = m, for all x.
There are cases, as in (14), where the expected value
E{Z(x)} = m(z) is neither constant nor known. It would
then be useful to express the error variance Var Y as a
function of some "generalized" covariance function K(h)
that would not require prior knowledge of m(z).
Consider first the case of a constant but known mean:
E{Z(x)} = m, for all z. Under the OK unbiasedness
condition (2):

A
0 =1, entailing

d, = 0
(22)
s00
the estimation error Y can be written as a linear combi
nation of (n + 1) increments:
0=1

Y = Z(Xo) - Z'(zo) = f do[Z(xo - Z(zTo)]
0-0

(23)

20

FUNDAMENTALS OF GEOSTATISTICS

and its variance is expressed as:
Var Y =

0-0 0=0

Introducing the variogram Lesson 1 (22) defined as
the
variance of the increment Z(z + h) - Z(x):
2-r(h)

o=0 B:
Var Y = E
)d.d£3KL(z.T.

E dad# Cov{Z(z. - Z(xo), Z(zo - Z(x )}
0

= Var{Z(z + h) - Z(z)}
= Var{[Z(z + h) - Z(xo)] - [Z(x) - Z(xo)]}

2-y(x + h - Zo) + 2 y(z - zo)
-2Cov{Z(zo) - Z(z + h), Z(zo) - Z(x)}
Thus:

- z)

>0

(26)

under the (L + 1) conditions (17):

•,

(

)O,

t = O,...,-L

a=0

A generalized increment of order L, Y =
Z(z),
would filter any polynomial of order L and more gener
ally any function f(z) =
0 alf,(z) being added to the
random function Z(z). Indeed:

Cov{Z(xo)

Z(r + h), Z(xo) - Z(x)}
= t(x + h -- xo)
+ -t(x zo) - 7t(h)

(24)

The error variance (23) is then rewritten:
VarY
0=o
Z3= d3o [-y(z - xo) + -f(r0 _x(0)
Xo) -

=
[Z=

d,-

z

-

z0)1

+ [Zd=0 .(x. - x0)].
aE'=0 d.d,3-y(x - x£)

j''=

g

do]0

Accounting for relation (22), it comes finally:
VarY=

-Z=O:0
=0odd,3y(z.-x) >0
(25)
under the condition E=0 do = 0
Note the similarity of the two error variance expres4
;ions
(21) and (25). The negative of the semi-variogram, is said to be a "generalized covariance" of order zero. Th),
order zero comes from the order of a polynomial t:
m(z) = Po(z) = m that can be filtered by the errorrend
. expression (20). Indeed, if a constant m (polynomial ti
of order 0) is added to the random function model Zrend
i.e., to the (n + 1) values Z(zo), Z(zo) in expres
sion
(20), it would be cancelled out thanks to the condi
tion
E'=0 d. = 0.
Any linear combination Y =
o daZ(Za), such i
E 0d,,, = 0, is said to be a "generalized increment that
order 0 and its variance is given by expression (25). "of
particular, the elementary increment Z(x + h) - Z(z Is
r)is
an increment of order 0 and acccording to (25):
Var{Z(z + h) - Z(z)} =
+ -y(x + h - x) + y(x - x - h) - 2y(0) = 2-y(h)
since -f(0) = 0
Genealiztion. Any linear combination Y = £--.0
d.2 g(zY),
such that the weights d. verify the (L + 1) unbiasedn
ess
conditions (17), is said to be a "generalized incremen
at"
of order L and its variance is given as a double sum
of a
"generalized covariance" of order L:

dc [.

o=0

= £od.Z(Z.) + EL-ae

)+ E
at()
1=0
-deft(xa) =En

whatever the coefficients a,.
An intrinsic random function of order L (IRF-L) is
a class of random functions defined up to an arbitrary
polynomial of order L, and more generally up to an ar
bitrary trend of type f(z) =
0Loatfe(x). The variance
of such random functions is then characterized from the
"intrinsic" covariance KL(h).
The variance (26) can be minimized under the (L + 1)
constraints yielding a system of equations identical to the
constrained system (18), except for the residual
covari
ance C(h) being replaced by the generalized
covariance
KL(h).

Up to this point, notwithstanding its formal elegance,
the formalism of intrinsic random function of order L
(IRF-L) has not brought any new insight into the very
classical and simpler algorithm of constrained minimiza
tion of an error variance.
Infrrn . Proponents of the IRF-L approach argue
"that the generalized covariance KL(h) is easier to infer
than the residual covariance
C(h).
Inference of the residual covariance C(h)
requires the
choice of subzones and/or directions where the trend
Et alft(z) can be ignored; C(h) is then inferred directly
from the original Z-data.
Inference of the generalized covariance KL(h) requires
also a preliminary definition of generalized increments
of
order, i.e., linear combinations Y = E.o d. Z(x) whose
variances would be insensitive to any trend of type
ELo atft(r) and expressed as function of KL(h) through
relation (26).
Assume that N such generalized increments
l=,j =- ,...,N could be defined.
The generalized covariance KL(h) is arbitrarily as
sumed to be of a rather restrictive and isotropic form,
such as a parametric polynomial form, e.g., for L = 1 in

JOURNEL

K1 (h) = b0(l - jhi0÷)- b5lhl + b2fhj 3

(27)

with: /o, bl, b2 > 0.
The covariance parameters bo, bi, b2 are identified by a
regression-type procedure that seeks to identify the theo
retical variances Var {Y, }, calculated from the model (27)
and relation (26), to the experimental values y:
N

Ej [yN-Var{1}
J=1

is minimum

(28)

An analog of such procedure in the case of L = 0, and
b2 = 0, would be a least-squares fit of a linear variograxn
model to the experimental average squared increments
y,• with y!, = z(
)
z()].
Well estab
lished practice has shown that such blind least-squares fit
of a parametric variogram model is not recommendable.
Besides the severe restriction to narrow families of
parametric polynomial-type covariance models, the regres
sion-type procedure (28) used to identify the parameters
b's has little resolution, is highly outlier-data sensitive and
allows little experimentor interaction. All practitioners of
traditional variogram modeling know the paramount im
portance of "variogram cleaning."
Moreover, it is this author's understanding that au
tomatic determination of the parameters b's often yields
generalized covariance models close to a pure nugget ef
fect, i.e., a model where the first term bo(1 - hl°÷) is
predominant. In which case, the corresponding kriging
yields but a traditional least-squares fit of the specified
trend, assuming little or no spatial correlation; indeed, a
small yield for such an elaborated formalism.
In conclusion, this author does not recommend us
age of the IRF-L formalism and the related canned pack
ages. If programs like Bluepack are used wisely, i.e., set-

ting aside all the automatic parameter determination as
pects, they do not differ from any other constrained krig
ing problem, i.e., from a linear system solver.
Selected Bibliography
Bras, R. and Iturbe, I., 1984, Random functions in hydro
logy, Addison-Wesley, 559 p.
Delfiner, P., 1976, "Linear estimation of non-stationary
spatial phenomena," in Advanced Geostatistics in the
Mining Industry, ed. Guarascio, et al., publ.
Reidel,
Holland, pp. 49-68.
De Marsily, G., 1986, Quantitative Hydrogeology, Aca
demic Press, 440 p.
Englund, E. and Sparks, A., 1988, Geo-Eas User's Guide,
(Geostatistical Environmental Assessment Software),
ed. US EPA, EMSL, Las Vegas, NV 89193, 155 p.
Froidevaux, R., 1988, Geostat-Toolbox Primer. (Geo
statistical 3 dimensional software), ed. FSS Interna
tional, 10 Ch. de Drize, Troinex, Switzerland, 88 p.
Goldberger, A., 1962, "Best linear unbiased prediction in
the generalized linear regression model," in JAjA,
v. 57,
pp. 369-375.
Journel, A. and Huijbregts, C., 1978, Mining Geostatis
tic, Chap. 5, Academic Press, 600 p.
Kitanidis, P., 1986, Parameter uncertainty in estimation
of spatial functions: Bayesian analysis, in Water
Resources Research, 22(4), 499-507.
Luenberger, D., 1969, Optimization by vector space meth
ods, Wiley & Sons, 326 p.
Matheron, G., 1973, "The intrinsic random functions and
their applications," in Adv. Appl. Prob. 5, pp. 438
468.
Whittle, P., 1963, Prediction and Regulation, U. of Min
nesota Press, revised ed. (1983), 187 p.

Lesson IV: Non-Parametric Assessment of Local
Uncertainty
Uncertainty about any paritcular unknown value is
modeled by a probability distribution os that unknown
value conditional to available related information. The
essence of that model lies in the assumed relations be
tween information and unknown and between the various
elements of information available. These conditional dis
tributions are not necessarily related to any particular
prior multivariate distribution model, such as Gaussian.
Their determination is done prior and independently of
the estimate(s) retained, and accounts for the data config
uration, data values and data quality. It allows for usage
of soft information either of local or global (structural)
nature.

21

Priorto the choice of a goal oriented optimal estimate(s),
availability of conditional distributions for the unknown
allows mapping of probabilities of exceedence, risks of mis
classification and the assessment of need for additional
sampling.
Introduction
Consider the general problem of spatial interpolation
and uncertainty assessment. The "local" information con
sists in a data set distributed in space, with not all data
of the same quality or relevant to the same attribute.
The "global" information consists in usually soft quali
tative, appreciation of the patterns of spatial dependence
between local data arising from structural, geological in-

22

"•_-

FUNDAMENTALS OF GEOSTATISTICS

terpretation and/or experience from similar and better
several measurements are performed yielding several esti
developed fields.
mates, zj, j = 1,... , n. Provided that the measurement
Deterministic interpolation techniques, including tri
device and its accuracy have remained constant over the
angulation and inverse distance-weighting, do not con
sequence j = 1, - -., n, (which implies in particular that
sider the possibility of a distribution of potential values
there is no potential for trend in the n outcomes zj), the
for the unknown, they do not provide any measure of the
distribution of these n outcomes z, can be used to model
reliability of the estimates. The main advantage of prob
the uncertainty about the model z.
abilistic interpolation (prediction) techniques, e.g. ordi
For example it may be said that "the probability that
nary kriging as described in Lesson III, is that an error
the unknown z is lesser or equal to a threshold z0" is
variance is attached to each estimate. Unfortunately, un
modeled by the corresponding proportion of outcomes
less a parametric distribution of spatial errors is assumed.
zj _<z0 . By so saying the unknown value z has been
an error variance falls short of providing confidence inter
elevated to the status of a random variable (RV) Z, the
vals and the error probability distribution required for risk
cumulative distribution function (cdf) of which is modeled
assessment. Most often symmetric, Gaussian-related, dis
by:
tribution models are assumed for the errors; these models
Prob{Z < zol(n)} = proportion of zj < Zo, j =
n
are fully characterized by only two parameters, the er
(1)
ror mean and the error variance. Such congenial models
Introducing the indicator function (of the threshold
are perfectly reasonable for the distribution of, say, mea
value zo):
surement errors in the highly controlled environment of
i(ZO;Z)= O0,ifzo<z'
a laboratory. However, they are questionable when used
(2)
1, if Zo > Zj
for spatial interpolation errors, the kind considered in this
The previous model for uncertainty is written:
text.
It is argued that uncertainty about an unknown is in
F(zol(n)) = Prob{Z < zof(n)} =
i(,; zo) E [0, 1)
trinsic to the level of information available and to a prior
model for the relations between that information and the
(3)
Again the notation F(zol(n)) serves to recall that this
unknown. Thus assessment of uncertainty should not be
probability model is a function of both the threshold value
done around a particular estimate, if only because many
z0 and the information set (n) constituted by the n out
optimality criteria can be defined resulting in different,
come
values zj, j = 1,..., n.
yet all "optimal", estimates. Non-parametric geostatis
The
model (3) corresponds to an equal weighted av
tics put as priority, not the derivation of a "optimal" esti
erage
(by
.) of the indicator data i(zo; z3 ). Under the
mator, but the modeling of the uncertainty. That uncer
previous
hypothesis
that the measurement accuracy has
tainty model takes the form of a probability distribution
remained
stable
over
the sequence j = 1, ... , n, there is
of the unknown rather than that of an estimation error.
indeed
no
reason
to
over/under
weight any particular out
The modeling of the probability that the unknown at
come. Otherwise one may consider an alternative model
any unsampled location is greater than a given threshold
corresponding to an unequal weighting scheme:
value capitalizes on the proportion of neighboring data
valued above that same threshold. That modeling ac
F(zol(n)) = Prob{Z < zol(n)} = Zaj i(zo; zj) E [0, 1]
counts for the proximity of each datum to the unsampled
J=.
location, the particular threshold considered and the qual
(4)
ity of the data being considered.
with a, _>0, for all j, and r
a'
= 1.
In this Lesson, the concepts of non-parametric Geo
Remaks. Expressions (3) and (4) are different mod
statistics are presented with a minimum of probabilistic
els of the uncertainty about the unknown value z. They
formalism. For example, stationarity is introduced as the
are not different estimates of some elusive "true" prob
decision to proceed with averaging and statistical infer
ability distribution. In particular, one cannot say that
ence over a pre-determined population or area in space;
model (4) is better than model (3), before having defined
when it comes to implementation, the hypothesis of sta
what a "good" model should feature. Also such defi
tionarity boils down to that, no more, no less.
nition would be needed to determine the set of weights
faj, j = 1,..., n}.
Probabilistic assessment of uncertainty
Consider an attribute value z to be measured. Any
particular measurement would provide an estimated value
z, likely to be in error. Hence and whenever possible,

The conditions, aj > 0 and rj aj = 1, ensure that the
function F(zoj(n)) is an acceptable cumulative distribu
tion function (cdf), i.e. a non-decreasing function valued

between [0,1].

JOUR.NEL

23

Probability intervals. Availability of cdf's as models of
for retaining a single value for estimate of the unknown
uncertainty allows the derivation of probability intervals:
value z.
Prob{Z E]a, b]j(n)I = F(bI(n)) - F(al(n))
(5)
Because there is no unique "best in all cases", estimate
Again such a probability interval is but a model fig
for the unknown z, assessments of uncertainty such as
uring the uncertainty around the unknown z, Questions
probability intervals of type 5 should not depend on the
such as "How reliable is this probability interval?" amounts particular estimate
chosen but solely on the information
to asking "How reliable is the model F(zo0(n))?", and can
available (n). In other words, the uncertainty linked to an
not be answered unless a model for the distribution of cdf
unknown value z is a function of the information available.
models is built. Statisticians do have such second level
not the estimate retained.
models, but most often they do not bother qualifying the
Indeed, consider n analyses zk of a particularly toxic
uncertainty attached to models of uncertainty.
(lethal) substance; for cleaning decisions, the maximum
Note that these probability intervals can be estab
value estimate z'P) may be retained. Such an estimate
lished prior to the choice of any particular estimate for
however reasonable will be outside most probability in
the unknown value z.
tervals of type 5 based on the same information set (n).
Estimates for z. Beyond the assessment of uncertainty,
A corollary of the previous remark is that traditional
a unique estimated value for z may be required, say, for
95% probability intervals, leaving 2.5% probability below
decision-making or engineering design purposes. If there
and above respectively, need not be centered or even con
is no reason to over/under weight any of the n outcomes,
tain the estimated value retained.
a "reasonable" estimate is the equal-weighted arithmetic
We have thus established the need for:
average:
-I

z,

(6)

n=1

An equally "reasonable" estimate is the median value
of the n outcomes, i.e. the value i(2; - ýt would leave
approximately one half of the outcome values below it
and one half above it:
P) = F-(0.5I(n)), such that: F(i(2)J(n)) - .5
(7)
If the n outcomes are not equally accurate, they should
be weighted differently. Using, for example, the weights
a, retained for the cdf model (4), the two following "rea
sonable" estimates for z are derived:
z'1) =

ajzj

(8)

j=1

and z"(') = F-'(0.51(n)).

(9)

However, other estimates can be derived independently
of the cdf model. Examples are:
Z"(3) = ma{z:, y = 1,.. .,n), for conservative reasons

maz(zi) + min(zj)
2
:.0) equal-weighted arithmetic average
z-(4)

z-=
=

-(

(10)
(1I)
(12)

"
ziarithmetic average eliminating
n-2
the lowest and highestobserved outcomes

(13)

emarks. All previous estimates can be considered
as "reasonable", although they can be quite different one
from another. There is thus a need to go beyond the adjective "reasonable" and define precisely worded criteria

"* defining a model for the uncertainty about the un
known value, given the available information (n).

"* defining criteria for retaining a unique value for es
timate of the unknown.

These two tasks need not be related, nor should they
call for any Gaussian assumption.
But first we need to broaden our discussion on prob
abilistic assessment of uncertainty to the case of non
repetitive measurements at different locations of a given
space.
The case of spatial distributions.
In the previous section, the case of repeated measure
ments, zi, j = 1, .-. ,n, of a unique attribute value z has
been considered. The cdf model F(zoI(n)), cf, relations
(3) or (4), provided an assessment of the measurement
uncertainty.
We will consider now the uncertainty linked to spatial
interpolation when an attribute value z(z) at location x
,s to be estimated from measurements z(zj), j = 1,..., n
made at different locations z, 0 X.
For the sake of simplicity we will consider first that
he data z(zi) are error free. This limitation is removed
n the later section "Soft Kriging".
The previous space of measurement variability is now
replaced by the physical space of locations z, but other
, vise the approach used is the same.
Define "spatial independence " as the state whereby
t he attribute value z(z), at any location z, is not
influ
• nced in any way by the attribute values z(x,)
at other
hocations z, 4 z, no matter how close they
are. If the

24

FUNDAMENTALS OF GEOSTATISTICS

(n+l) values z(x,) and z(z) relate to the same attribute
Z, but otherwise could be considered as independent one
from another, a possible model for the uncertainty about
the unknown value z(x) given the n data z(zj) is the dis
tribution of these data. More precisely it is said that the
probability of the unknown z(z) be lesser or equal to a
given threshold z is the corresponding proportion of such
data values z(zx). Again by so saying the unknown value
z(z) has been elevated to the status of a RV. Z(z), the
cumulative distribution of which is modeled by:
Prob{Z(z) :5 z1(n)} =

proportion of data
z(x 2)• z, j = 1,...,n

(14)

or introducing the indicator variables, one for each datum
location Z, and each threshold value z:

i(z;

){)

0, ifif zz <
1,
> Z(Xj)
z(xj)

(5

the previous relation is written similar to relation (3).
1 n
F(z; zj(n)) = Prob{Z(z) < zl(n)} = -nf
i(z; x,) E [0,1
n.,=1

Remark. By pooling all n data z(x,) into the sarr
(16), we are assuming that, although independent
one another, they still have something in common
fact that they relate to the same attribute Z. This i
source of the stationarity hypothesis, which states
the (n+1) random variables, Z(z), Z(xj), j = 1,.
have the same distribution which can then be estin
by an average of type (16) considering data taken a
ferent locations.
Dependence. In the general case there will be some
pattern of dependence between the attribute v alues
z(z), z(x'), z(z") at different locations x, x', x". Thus
the datum value which is the closest or more generall:y the
most related to the unknown value should be weil ;hted
more, leading to an unequal weighted average:

F(z;zl(n) = Prob{Z(z) < zl(n)}
= E'., a(z) i(zz;x,) E [0,1]

(17)

with aj(x) ? 0 for all j, and ,=I a2 (x) = 1.
Since the probability value F(z; zI(n)) needs to bt
culated for each different threshold value z, the wei
can be made also dependent on z, defining the ne;
model:

F(z; zI(n)) = -aj(z;z) i(;;zj)

F(z;.I(n)) E [0,1]
F(z;xj(n))

_ F(z';xj(n)), for all z > z'

(19)

Probability Intervals. The uncertainty about the value
z(z) can be assessed through probability intervals derived
directly from any of the cdf models (16), (17), and (18):
Prob{Z(z) E]a, b]I(n) = F(b;zl(n)) - F(a;zl(n))

(20)

Of particular importance for many applications is the
probability of exceedance of a threshold b:
Prob{Z(z) > bl(n)} = I - F(b; xI(n))

(21)

Again note that such probability intervals and probabili
ties of exceedance can be established prior to the choice

of any particular estimate of the unknown value z(z).
Isopleth curves for the probability of exceedance (21)
can be contoured (for z varying in space and b fixed).

For many decision making processes, these iso-probability
maps suffice; thus there would be no need for the addi
tional hypotheses required to retrieve unique-valued esti
mates of z(x).
Exactitude Requirement.
Whatever cdf model
F(z;zl(n)) is retained to assess the uncertainty about the
unknown z(x), it should be consistent with the fact that
at data locations there is no uncertainty, i.e. the model
should be as such

F(z;zjl(n)) = Prob{Z(zx)
S-i('z;X')=

< zj(n)}

0, if z< datum value z(x,)
1 1, otherwise

(22)
for all data locations z,, j = 1,-.. ,n.
Remarks. Relation (22) assumes that the data z(xj)
are exact. If they are not, then the cdf model F(z; zj(n))
need only be consistent with what is actually known at
location zj, possibly only a prior distribution for the value
z(xj), see section on Soft Kriging.
Relations (18) and (22) show that the cdf model
F(z;zj(n)) may be seen as an estimate of the unknown
indicator function i(z; z) using weighted averages of the
known indicator values i(z; z,):
F(z; z1(n))

[i(z;z)"

=

a,(z;.T)
i
i(z; z,)

(23)

j,=1i

(18)

J7=1

•.

(18) remains that of a cdf, i.e. provided that the following
order relations are verified:

Remarks. The weights a,(z:x) need not be any more
non-negative nor sum up to one, as long as expresssion

Under a rigorous probability setting one can show that
the uncertainty model (18) or (23) is, indeed, a linear
estimate of the conditional distribution of Z(z) given the
n data Z(z,) = z(xj), [Isaaks 1984, p.26] or [Journel 1986
- appendix].

JOURNEL
Determination of the cdf uncertainty model
The very reason for considering an unequal weighting
of the data, as in relations (4), (17), (18), or (23), is the
recognition of the existence of patterns of spatial depen
dence between the data z(z,) and the unknown z(z), thus
entailing some dependence between the indicator data
i(z; zx) and the unknown indicator function i(z; x). Hence
these patterns must be identified and first, the notion of
spatial dependence must be defined and measured.
E.
A datum value z(z,) can be seen
as a pair of information, one related to the datum loca
tion z, the second related to the attribute value. Com
mon measures of proximity between two values z(z) and
z(z + h) are linked to the eucidian distance jhj, or mod
ulus of the interdistance vector h; examples are:

The corresponding measure of proximity or depen
dence is the covariance:

Cz(h) = Consitant- -z(h)
Cj(z; h) = Constant - 71(z; h)

'

with

Such measures account for only the location informa
tion z, of a datum z(xj), not for the attribute-information
z. Indeed, the Euclidian distance Ijh is attribute Z in
dependent; it is the same whether very continuous layer
thickness or very erratic transmissivity values are consid
ered. Also the Euclidian distance is the same for two indi
cator pairs fi(z; zj), i(z; r, + h)] and [i(z'; x), i(z'; x, + h)],
although z 6 z'.
Variogram Distance Measure. The classical, although
not unique distance measure used in geostatistics is the
variogram function 27 (h) modelled from the experimental
average squared discrepancy between the n(h) data pairs
separated by approximately the same vector h, see Lesson
1 (29):
2-yz(h) modeled from ' Z'I)[z(,) -z(z, + h)]'
2y,(z; h) modeled from .

'

i(z;

) - i(z; z, + h)] 2
(24)

The average squared discrepancy usually increases with
the interdistance jh[. However, as opposed to the Euclid
ian distance Ihl, the variogram distance 27(h) is attribute
specific with in particular: 't(z; h) 6 yt(z'; h) if z 9 z'.
The variograms are dependent on both the modulus jhI
and the direction of the interdistance vector h. They are
said to be anisotropic, if they actually depend on the di
rection of vector h otherwise they are said to be isotropic.
As an example, porosity data present greater spatial de
pendence (smaller variogram values) in directions of de
position (horizontal and along ancient channels).

(25)

The arbitrary constant is filtered out from all subse
quent utilizations of these proximity measures.
The Indicator Kriging Algorithm. Once a proximity
measure is available for the attribute being estimated,
a straightforward generalized linear regression algorithm
also called "ordinary kriging" can be applied to deter
mine the weighting system, Lesson III (2). For example
interpreting relation (23) as an estimation of the unknown
indicator value i(z; x) :

F(z; x (n)) = [i(z;.T)]"

* Constant - IhI
e or the reciprocal of Jhj to some power:
usually w = I or 2.

25

(26)

The n weights a (z; a) are given by a constrained system of
normal equations, also called the ordinary kriging system,
Lesson III (4).

7•,=
a2 ,Cz;z) . C,(z;j•, - z,) +j~z;•) = CJ(z;L -

E:= agCz;z) = 1

)

for allj=1,.-,n

(27)
Reh-k•. System (27) appears as a system of (n+l)
linear equations with (n+l) unknowns, the n weights
aG(z;T) and a Lagrange parameter p(z;z) associated to
the condition that the weights sum up to 1. A sufficient
condition for the system (27) to provide one and only one
solution is that the proximity model Ci(z; x) be positive
definite, i.e. be a covariance function, and that no two
data be at the exact same location, i.e. ,j 0 xj, for all
j # j'. There is one such system for each threshold value
z and for each unsampled location x. In practice the
interval of variability of the attribute Z is discretized by
a small number of K threshold values, Zk, k = 1,- --,
K.
Thus at each location x, K systems of type (27) need
to be solved to provide an assessment of the uncertainty
through K discrete cdf values F(zk;zI(n)). Cdf values
for intermediary threshold values Z E ]Zk,zk+lj can be
obtained by interpolation, e.g. linear:
F(z;L.I(n)) = F(zk;xI(n))

+ &4,i--I&
" [F(zh+z;.I(n)) - F(za,; l(n))]
(28)
Other non-linear interpolation procedures might also
be considered.
Exactitude. The ordinary kriging systems (27) provide
weights such that the exactitude requirement (22) is met.
Order-elation. The K systems (27) do not ensure
that the resulting K cdf values verify the order relations

26

FUNDAMENTALS OF GEOSTATISTICS

(19). In practice a piecewise linear model is fitted to the
K values F(zk; xl(n)) so that to guarantee the order rela
tions, and it is this model which is used to characterize
the uncertainty about the unknown. For more details on
such corrections see [Sullivan 1984 p 36-42].
The Probability Kriging Algorithm. The indicator es
timate (23) uses only the indicator part of exceedence
rather than the whole information available z(x:). That
estimate is enhanced if all the information available is
used, defining the new weighted average:
[:(Z; XT]=

j=--1

a1 (z;z).- (z; z3) +

j=--1

b3(z;.T z)

zi

This new estimate mixes indicator data valued either 0
or 1 and data z(z 2 ) valued in the possibly widely different
measurement unit of attribute Z. This scaling problem is
solved by considering instead of the data z(x,) their rank
order transforms r(z,).
If the n data z(xj) are ranked in increasing order and
r(z,) E [1,n) is the rank of the datum z(z,), the trans
formed datum lr(xj) is valued between [0,1]. The im
proved indicator estimate is written and is taken as model
for the uncertainty about z(z):
F(z;xI(n)) = [i(z; x)]"
=

•f aj((z; z) i(z; Xz)

+

En

(29)

Relation (29) can be read as a cokriging estimate of
the indicator value i(z; z) by two sets of data, the n indi
cator data i(z; xj) and the n rank order data r(xj). The
corresponding 2n weights aj(z;x) and bj(z;x) are given
by a cokriging system of the type described in Lesson II
(21-22), see also [Isaaks, 1984, p. 16-27], [Journel, 1984-a].
It is of paramount importance to understand that ex
pressions (26) or (29) are not used as estimates of the lo
cal indicator value i(z; x), although they are built as such.
These expressions are used as model values for the con
ditional probability F(z;zI(n)) = Prob{Z(z) <_ zl(n)),
[Journel, 1986, Appendix].
The Soft Kriging Generalization
In all the preceding developments, the data available
were all considered "hard data", that is without uncer
tainty, generating indicator data that are valued either 0
or 1.
However in real practice, data are never exactly hard
if only because of measurement errors. It would be ap
propriate to enter actual data either under the form of
constraint intervals:

z(zj) E]"(zj), b(xj)]

(30)

or under the form of prior probability distributions:
Prob{Z(z3 ) _<z} = F(z; z:) E [0, 1]

(31)

For a given datum location the indicator information
can be viewed as a column of K indicator data, one in
dicator datum for each of the K threshold values Zk. In
the case (30) of a constraint interval, the indicator data
column is informed only outside that interval with:
i(zk;z,) = Ofor all zk < a(xj)
i(zk; Xz) =
unknown for all zk Ela(z:), b(xz)]
i(Zk; j) = 1, for all zL. > b(zx)

(32)

A "hard" datum is but a particular case of a constraint
interval (31) with zero amplitude: a(zj) = b(x,) = z(zx).
In the case (32) of local information given as a probability
distribution, the indicator datum column is filled-in with
values between [0,1] instead of equal to either 0 or 1.
i(zk; xj) = F(zk; Xj) E [0, 1]

(33)

The indicator algorithm (26) or (29) thus allows pooling
and joint weighting of data of several types,

"*hard data.
"*inequality constraints of type (30) with possibly
1. a(z:) = -co corresponding to z(z 2 ) < b(Xz)
2. b(x,) = +oo corresponding to z(z 2 ) > a(x1 )

"*soft information presented under the format of a
prior distribution of type (31).
At a given location z. where no core sample is avail
able, well log data or simply geological interpretation may
indicate that a particular rock type prevails. At that lo
cation z., no hard data is available but one may consider
the soft information constituted by, say, the distribution
(31) of all core porosity data available within the indicated
rock type.
The result of indicator kriging is a probability column
F(zk; x1(n)) valued between [0,1] and used as a model for
the uncertainty about the unknown value z(x).
E.
If at a location z, there exists prior soft in
formation of type (33), the exactitude property of kriging
entails that this prior information is restituted unchanged:
F(zk;zl(an)) =-F(z,; z), for all zh,

(34)

whatever is the information at the n other locations. In
other words the process of indicator/probability kriging

JOURNEL
does not update prior information, it completes it by in
terpolation.
In most practical cases at every location there exists
minimum prior information such as a large constraint in
terval [0, 100%] for porosity or saturation. Consequently
at any location, the initial indicator column is only par
tially filled with:
i(zk; z) =

0 for all zk < 0

i(z,; x) = 1 for all zk > 100%
The process of probability/indicator kriging does not
update these prior values but completes the indicator col
umn with probability values for those threshold values
Zk E [0, 100%].

27

Loss Functions and L-Optimal Estimates.
When estimating an unknown value by a single value
z'(x) = z', a non-zero error z' - z(z) is likely to occur.
Assume that the impact or loss attached to each level of
error can be assessed by a function L(z" - z(z)). The
function L(e) is known, e.g. L(e) = e2 , but the argu
ment e = z" - z(x) is unknown. However an uncertainty
model F(z;zj(n)) which represents the distribution of the
unknown z(z) is available. Thus the idea is to use that
distribution model to determine the expected loss value:

E{L(z" - Z)I(n)} = J

L(z" - z) . dF(z;xl(n)) (35)

in practice approximated by the discrete sum:

Optimal estimate(s) for the unknown
The probabilistic assessment of the uncertainty about
an unknown value is completed with determination of a
probability distribution of type (23). As discussed before,
knowledge of that distribu•i•n suices to the derivation
of type (18).
It is recalled that the determination of a probability
distribution for an unknown does not, and should not, re
quire any preliminary choice of a unique estimate for z(z).
Similarly, a probability-oriented oriented decision making
process should not require more than the assessment of
the uncertainty about z(z).
However it is traditional to require the derivation of
such unique estimated values z*(x). Since the unknown
value could be any value within the interval [0,100%] for
concentrations, there is clearly a need to make precise the
criterion for selecting a unique estimated value. Plausible
criteria are numerous, each leading to different estimates.
Different uses of the estimates may call for different cri
teria, and there is no such thing as an "optimal for all
purposes" estimate or estimation algorithm. As an exam
ple, most estimates featuring good local accuracy proper
ties, including the kriging estimate, are smoothed in their
spatial variability; thus they may prove inadequate for
mapping purposes if the map is to be used to evaluate
spatial variability.
An incisive layman-oriented discussion of optimality
for decision making can be found in [Srivastava 1987).
This is not the place to survey the vast array of diverse
optimality criteria leading to as vast an array of estimates,
all optimal. We will limit our discussion to those esti
mates that can be derived straightforwardly from the un
certainty model F(z; zI(n)). These estimates are but a
subset of the former vast array of optimal estimates and
may not include the one needed for a specific application.

K

;ý _L(z'-Zk').[F(zk+I; zj(n))-F(Zk; zI(n))] =
k=!

V(z*; )

(36)
with, e.g. z,?,
if the attribute Z interval of vari
ability has been discretized by K threshold values
zj,k = 1,... ,K, and usually: F(zK+I;zl(n)) = 1.
The expected loss (35) calculated from the model
F(z;zj(n)), appears as a function o(z*;z) of the partic
ular estimated value z' retained. The optimal estimate
for the loss function L is the value z4(z) minimizing the
expected loss V(z'; z):
zj(x) = value z' minimizing ((z'; z)

(37)

Derivation of the optimum zj(z), for any particular
loss function L, can be done through iterative calculations
of the discrete sums approximating the Stieltjes integral
(35); such calculations do not present any difficulty.
For some particular loss functions, the solution is straight
forward,

9 if L(e)

2

= e , i.e. for the least squares criterion,
the best estimate z' is the expected value of the
probability distribution F(z;zl(n)), also called E
type estimate:

4(z) = f

z dF(z; xI(n))

(38)

see relation (43) for an approximation of this inte
gral.
0 if L(e) = jej, i.e. for the mean absolute deviation

criterion the best estimate is the median of the dis

tribution F(z;.Tl(n)) defined as:

q.s(z) = F-'(.5;
zI(n)) such that F(q.s(z);zi(n)) = .5
(39)
if

28

FUNDAMENTALS OF GEOSTATISTICS
for e > 0 (overestimation)
for e <0 (underestimation)
(40)
i.e. for an asymmetric linear loss function, the best
estimate is the p-quantile of the distribution
F(z; zf(n)), see [Journel 1984 -b]:
L(e) =

wle

w21eC
W

qp(x) = F-(p; zI(n)) with p =

w

E [0,1]

w 1 •W 2

(41)

sif
0(0,fore =0
L constant, otherwise

(42)

the best estimate is the most plausible outcon
fe of
the distribution F(z;zI(n)), i.e. the mode of
fthe
corresponding density function

f(z; =1(n)) = ,F(z;lj(n))/8z
Remrks. Spatial distributions of toxic chemical concentrations are usually highly skewed and would gen erate
strongly asymmetric pdf models f(z;zl(r- for the
uncertainty about an unknown. In such casez_ the opttimal
estimates (38) and (39) are quite different, and the eimpact of using one criterion rather than the other ca . be
dramatic.

It bears repeating that nature is rarely Gaussian. and
that uncontrolled spatial distributions of earth/env
ironmental sciences attributes are almost always non-Gau ssian,
as opposed to distributions of repetitive measuremen .ts in
a highly controlled, carefully planned, laboratory ex.periment.
Most decision making processes, if properly anal yzed
w uld call for an asymmetric loss function of type
(40),
whether linear or not. For example, for mud-loading
purposes, the impact of pressure underestimation (with b lowout risk) is usually much higher than that of an over estimation (using too much mud). Thus for such decisi on a
standard least-squares type estimate such as (38) w,ould
be inappropriate; fortunately decision makers are aisare
of such shortcomings and apply various sorts of safety factors which amount to considering asymmetric loss i Lnctions. The rigorous concept of loss function and opti mal
L-estimate may allow greater consistency in the decision
making process, once a particular and possibly subjec tive
loss function has been chosen.
The E-Type Estimate. The E-type estimate (40)
, although a least squares type estimate, is usually different
from the direct ordinary kriging estimate, z:,,g(x) u sing
only the original data z(z:) and the corresponding p:roximity measure model Cz(h) as defined in relation (25).
As opposed to the approach yielding the E-type estim ate,
the ordinary kriging approach does not qualify the or-

dinary kriging estimate for uncertainty (probability in
tervals), unless a Gaussian distribution for interpolation
errors is assumed.
The Stieltjes integral (38) defining the E-type estimate
is, in practice, approximated by a discrete sum:
K

(z() Z E Zk. [F(z&+i; z(n)) - F(zh;zf(n))]
k=i

(43)

with: zk' =
z:+' dF(z;Il(n)) E ]zk, zk+,]
being the PLh class mean

and zk, k = 1,...,KK + 1 being (K+1) class bounds dis
cretizing the Z attribute interval of variability.

The indicator/probability kriging process provides for
the probability values F(zk; z:(n)) but not for the class
means zk'. Estimation of these class means, except for the
last one, usually does not pose any problem. For exam
ple a class mean can be estimated by the equal weighted
average of all data falling into the corresponding class.
Whenever the class mean zk' relates to a narrow class

amplitude zk+l - zk, there is no need to consider sophis
ticated unequal-weighted averages.
However, for heavily skewed distributions the last class
mean may have a disproportionate influence on the esti
mate (43). This last class mean is highly dependent on
the largest datum observed and thus on the decision to
keep it, cull it, or set it back to a lower value. A conser
vative decision consists of using the class median for the
estimate. Another solution calls for a parametric model of
the distribution within that class: for example a 2 param

eter lognormal model can be fitted to two robust statistics
of the highest data z(zj) E IZK,zK+1I, such as ZK being
the F(zK; zl(n)) - quantile, and the median of these high

data being the [1 + F(zK; zI(n))]/2- quantile, then the
last class mean of that lognormal model is taken for the
estimate of zK'.
The poor outlier resistance of the E-type estimate (43)
is shared by all mean type estimates, including the direct
ordinary kriging estimate. At least in the case of an E

type estimate, that outlier-sensitivity is confined to the
last class. Quantile type estimates such as (39) and (41)
have much better outlier resistance and should be con
sidered whenever their underlying optimality criteria are
acceptable.
Risk-qualified maps
Availability of cdf models F(z; z1(n)), one such func
tion for each location z within a given area A, allows
contouring of isopleth curves of.
e the optimal estimate(s) z'(:) retained. These esti

mates can be derived independently of the uncer-

JOURNEL
tainty model F(z; zI(n)), see for examples relations
(6) to (13).
It is recalled that most estimated values z'(z) de
rived from a local accuracy criterion, including all
kriged estimates and the E-type estimates (38), tend
to present an oversmooth image of the spatial dis
tribution of the actual attribute values. A solution
to this problem is proposed in the next Lesson.
e probabilities that the actual unknown value z(z) ex
ceeds any given threshold, such as:
Prob{Z(z) > zo0(n)J = 1 - F(zo; zI(n))

(44)

Considering for threshold value, e.g., a high per
meability value zo, a zone where all such proba
bilities of exceedence are high may indicate a vol
ume of high pay or a flow path. Correspondingly,
if a low threshold value z0 is considered, mapping
isopleth values of probabilities of non-exceedence
1 - Prob{Z(z) :_ zo0(n)} indicate probable zones
of low pay or flow barriers.
• risk of misclassification:
a(z)

= Prob{Z(x) < zojz0()
= F(zo;x.I(n))

> zO,(n)}

(45)

for those locations z such that z'(z) > zo, or:
8(x)

= Prob{Z(x) > zOz'(x) < zo,(n)
= 1 -F(zo; zI(n))

(46)

for those locations z such that z*(x) 5 zo. Again,
the estimate z*(z) used for the classification could
be any.
p-quantiles, qp(r),x E A, p fixed in [0,1], of the
distributions F(z;zj(n)), i.e. contour maps of the
values qp(z) for which the probability of exceedance
is I-p. Ifp is chosen low, e.g. p = .1, the probability
of exceedence of qp(z) at location x is high: 1
p = .9, thus those areas where the values 9.1(z) are
high are high z-values areas with a large confidence.
Conversely, if p is chosen high, e.g. p = .9, the
probability of non-exceedence of q,(x) at location x
is high: p = .9, thus those areas where q..(x) are low
are low z-values areas with a large confidence. Note
that determination of p-quantiles and their mapping
do not require the choice of a particular estimator
Z'(z).
It is suggested that a probabilistic assessment of ad
ditional sampling need not require a prior map of esti
mated values z'(z), i.e. a prior, often arbitrary, deci-

29

sion about an optimality criterion. Additional
data are

needed in zones that are not only poorly sampled but also
critical for the design at hand. For example, if a string
of locations (z) with high probability of exceedence of a
high permeability threshold, say Prob{Z(z) > zo0(n)} >
.8, is interrupted by some low such probability values
Prob{Z(z') > zol(n)} < .5, the latter locations z' are
prime candidates for additional sampling if ascertaining a
continuous flow path along the string (z) is critical to the
design at hand. Similarly if a string of locations (z) with
high probability of non exceedence of a low permeability
threshold, Prob{Z(z) :_ zol(n)} = .8, is broken by a few
locations (z') with lower such probability, these latter lo
cations (x') are candidates for additional sampling if the
continuity of that flow barrier (low permeability string)
is critical. Similar assessments can be done from quantile
maps {q,(X),z E A).
Warning: Quantile maps q,(z) or exceedence maps
Prob{Z(x) > zoI(n)}, for very large p and threshold val
ues z0 , may depend almost totally on the model extrapo
lating the last calculated value F(zK; Zl(n)) towards the
maximum value 1. If q,,(z) is larger than zK, or p is
larger than F(zK; zx(n)), they are not any more actual
data related. This model dependence for extreme values
occurence is not particular to the indicator kriging for
malism, it is shared (although possibly not stated) by all
other algorithms, e.g. when used a standard normal 95%
or 99% confidence interval. Non-parametric geostatistics
is no replacement for actual data.
Conclusions
The available information whether hard data, con
straint intervals, or soft information under the form of
prior probability distributions, is coded as a series of in
dicator columns. There is one such indicator column for
each location, whether a datum location or an unsam
pled location. The indicator/probability kriging process
consists in filling-in the missing entries of the indicator
columns with probability values. These probability val
ues are obtained by weighting neighboring and related
indicator data.
This process yields at each location a probability dis
tribution for the unknown attribute. The process is non
parametric in the sense that it does not assume any par
ticular prior model for these distributions. These proba
bility distributions characterize the uncertainty about the
unknowns, independently of the particular estimates re
tained; they are only information.dependent and are fully
so in the sense that they are not only data-configuration

30

FUNDAMENTALS OF GEOSTATISTICS

dependent, but also data-values and data-quality depen
dent.
If a loss function measuring the impact of any given
magnitude of the interpolation error is available an opti
mal estimate minimizing that loss can be derived. Various
optimal estimates may thus be considered depending on
their particular utilizations. However a single measure
of uncertainty applies to all such estimates, for a given
information set.
The availability of distributions of the unknowns al
lows contouring isopleth maps of

"*conditional quantile values
"*probability of exceedence of any given threshold
"*risks c and # of misclassification
Such maps can be used for various decision-making pro
cesses, including the assessment for additional data.
Implementation of the indicator/probability kriging
technique requires but linear geostatistical tools, i.e. a
variogram and an ordinary kriging (normal equations)
software. However, as in many statistical applications,
the necessary art of approximations requires some expe
rience.
Typical of non-parametric techniques, the elaboration
through indicator/probability kriging of the distributions
for the unknowns does require an appreciable amount of
data. This requirement is in fact a positive aspect, for
it indicates whenever data are clearly insufficient for the
goal at hand, the alternatives being to acquire more data
or rely on a model which is thus not data-based. The gen
eral rule is to exhaust the information available through

non-parametric tools, and only then revert to parametric
modeling.

Selected Bibliography:
Berger, J.O., 1980, Statistical decision theory and Bayesian
analysis. ed. Springer-Verlag, 617 p.
Hohn, M.E., 1988, Geostatistics and Petroleum Geology,
Van Nostrand, 264 p.
Isaaks, E.H., 1984, Risk qualified mappings for hazardous
wastes a case study in non-parametric geostatistics,
MSc Thesis, 8 5 p, Branner Earth Sciences Library,
Stanford.
Journel, A.G., 1984-a, The place of non-parametric Geo
statistics, in "Geostatistics for Natural Resources
Characterization", ed. Verly et al., public. Reidel,
Dordrecht, Part 1, p. 307-335.
Journel, A.G., 1984-b, mAD and Conditional quantiles
estimators, in ibid, Part 1, p. 261-270.
Journel, A.G., 1984-c, Indicator approach to toxic chemi
cal sites, EPA, EMSL-Las Vegas, Report of Project No.
CR-811235-02-0
Journel A.G., 1986, Constrained interpolation and soft
kriging, in Proceed.
of the 1 9th APCOM Symp.
publ. Soc. Min. Eng., Littleton, Co. p15-30.
Luenberger, D.L., 1969, Optimization by vector space
methods, ed. Wiley and Sons, NY, 326p.
Lemmer I.C., 1986, Mononodal indicator variography- Part
1-2 Math Geology, Vol. 18, no7, p.589-623.
Srivastava, R.M., 1987, Minimum variance or maximum
profitability in CIMM, Vol 80, no.901, pp. 6 3 -6 8 .
Sullivan, J.A., 1984, Non-parametric estimation of spatial
distributions, PHD Thesis, Branner Earth Sciences
Library, Stanford University, 368 p.

Lesson V: Stochastic Simulations for Imaging Spatial
Uncertainty
In the previous lessons we strove to model the un certainty about one unknown at a time, for example, the unsampled porosity value Z(z) at location z. In Lesson IV
that uncertainty is modeled by a posterior (conditioiaal)
distribution:
F(z; zx(n)) = Prob{Z(z) _5zl(n)}
(1)
However, that posterior distribution does not provide information about joint spatial uncertainty, e.g. the prc•)ba
bility that a string of locations z#,j = 1,..-, N be valued
above a certain threshold value zo characterizing, sa:y, a
fracture or a preferential flow path. Such joint spa.tial
uncertainty would require the joint N-variate poste rior
distribution:

K (zo; z,, j = 1,..., NI(n)) =

(2)

----Prob{Z(z,) > zo, j = 1,- -- ,NI(n))

where (n) represents the set of prior (conditioning) data.
Only when the N RV's Z(z,) can be assumed indepen
dent one from another, the joint probability (2) appears
as the product of the N probabilities of type (1):
K(zo; zj,j =- 1,.-,Nj(n)) = Ii",1 [1 - F(zo; xjl(n))]
When N is large, the previous product would yield
an artificially small probability of joint exceedence of the
threshold z0 . Spatial dependence of the RV's Z(r,) could
make this joint probability much higher, i.e., the possibiliy
of a flow path along the string z, more likely.

JOUIMEL
Similarly, a map of local estimates Z'(x,), whether
ordinary kriging estimates (Lesson III, (2)) or E-type es
timates (Lesson IV, (40)). may fail to reveal the possibility
of such a flow path along the string xj. Local estimation
algorithms are designed to minimize a certain local error
impact, whether an error variance in the OK case or a
conditional variance in the case of the E-type. They are
not designed to reproduce patterns of spatial continuity,
less those of extreme values such as Z(xj) > zo. Kriged
maps are notoriously smoothed in their spatial variation,
compare Figures 3 and 5 which fail to reflect the propor
tion of extreme values (both high and low) as seen on the
data.
For reservoir characterization where detection of pat
terns of occurrence of extreme values (permeability-poro
sity) are more critical than local accurate estimation or
global estimation of mean values, we need maps that would
honor patterns of spatial continuity and which provide an
assessment of joint spatial uncertainty.
Simulation vs. Estimation
One may see the process of reservoir characterization
as that of providing the "correct" input to a transfer func
tion representing some aspect of the reservoir engineering,
see Figure 1. For example, the transfer function may be
a flow simulator requiring for input an unsmoothed spa
tial distribution of permeability/porosity/saturations, the
output being breakthrough times, production rates and
sweeping efficiency.
Short of a unique perfect input map of permeabili
ties which would require exhaustive sampling, the idea is
to propose equiprobable alternative maps as input. All
such maps should share in common whatever information
is available, hard and soft, numerical and interpretative,
local and structural. Their differences then provide an
image if not a measure of spatial uncertainty about the
input. These alternative inputs can then be processed
through the flow simulator to yield the corresponding
measure of uncertainty on the response function, and an
swers to such questions as, "What is the probability of an
early breakthrough?", see Figure 1.

IZA
L ahemativc images
Fig. 1.

i

TMnsfer

0
Response disibuLion

Processing uncertain spatial information

31

This approach requires running the transfer function
on each alternative input spatial distribution and could
be tedious. A shortcut consists in selecting among the
alternative input images some deemed diametrically op
posed, say, a most favorable image, a median case and a
worst case, then run a limited sensitivity analysis on these
images. Other shortcuts do exist, most of them specific
to the particular transfer function under consideration.
In an estimation approach, the unknown spatial dis
tribution {z(x),x E A) is replaced by a unique set of
estimated values {z'(x),z E A). In a (stochastic) simu
lation approach, joint spatial uncertainty is characterized
by L alternative sets of simulated values
{z(z),z E A),e = 1,...,L.
In the first case, one gets a unique response with no
measure of its uncertainty:
r" = P(zW(),z E A)

(3)

Knowing the estimation variance or even the posterior
cdf about each of the unknown z(x) does not provide any
measure of uncertainty about R or R*, unless the transfer
function V is linear.
In the second case, one gets a distribution of responses:
rt= X(zz),x E A), 1,...,L
(4)
That distribution of responses provide a measure of un
certainty and an assessment of such critical probability of
exceedence as Prob {R > ro) is estimated by the propor
tion of observed responses r, > ro.
Remarks. No matter the unbiasedness or the local
accuracy of each estimate zr(z), the average response F
need not be close to the response (3) based on the unique
estimated map:
1i L
t=1

i r = V(z(z),X E A)

(5)

Local accuracy does not entail that the map is any
good as a whole. In all rigor, estimates based on a local
accuracy criterion such as kriging should only be tabu
lated; to map them is to entice the user to read on the
map patterns of spatial continuity that may be artifacts
of the data configuration and the kriging smoothing ef
fect. Where data are abundant the kriged map would
appear more variable, where data are sparse the kriged
map is essentially flat. The previous remark would apply
equally well to most traditional interpolation algorithms,
including spline or polynomial fitting and inverse distance
weighting.

32

FUNDAMENTALS OF GEOSTATISTICS

Each set of simulated values {ze(x), z E A) shoul
reproduce those spatial statistics of the unknown true vldalues
{z(x), X E A} which are deemed the most conseque!ntial
for the transfer function p. For example, flow in a fractured reservoir is essentially dependent on the pa ttern
of connectivity within A of the high permeability values
(z(z) > zo), not so much on whether at any particulhLr 1ocation x the permeability is high or low, nor even oii the
proportion within A of these high values. In such a case,
the zl(z)'s should reproduce patterns of connectivi ty of
extreme values even if this means trading off such I)roperty as local unbiasedness or reproduction of the o 'erall
histogram of the z(r)'s over A.
A set of simulated values {zt(x), z E A} is good inasmuch as it is drawn from a model that identifies th(
evant spatial characteristics of the unknown true v alues
{z(),z E A)}.
The random function Z(x), function of the co,ordinate(s) z within a domain A. can be seen as a sAet of
spatially dependent random variables {Z(z),z E A}, one
at each location z. Stochastic simulation amount s to
draw joint realizations from all random variables. The
essence of any stochastic simulation is the particular spatial dependence model that is imposed on the random
"function and thus imparted to each of the simulated sets

{zt(z),x r S},I = 1,... ,L.
Spatial connectivity of extreme values between any
two locations z and z + h is a highly multivariate p roperty that would require establishing the probability that,
along any path linking z and z + h, all values z(u) ex,ceed
jointly a certain threshold. Notwithstanding the innwmerable number of possible paths linking two locations in a
3-dimensional space, such multivariate statistics is never
accessible from usually available data. Short of a full mnultivariate connectivity measure one can define a two-pc ints
connectivity function as the joint probability that at two
locations z and z + h the attribute does not exceed the
threshold values z, z:
F(z, z'; h) = Prob{Z(z) < z, Z(z + h) : z'}

(6)

This two-points connectivity function was defined in] Lesson 1 (21) as the bivariate distribution function charac terizing spatial dependence between any two RV's Z(z) and
Z(x + h), separated by the same vector h. The bivar iate
cdf (6) can be seen as an non-centred indicator covaria. ace,
function of the separation vector h:
F(z, z': h) = E{I(z; z). I(z + h; z')}

(7)

with the indicator transform random function definecdas
in Lesson IV (15):

)
AZ()) = I(ZX)

O, if z < Z(z)
1, if z _ Z(Z)

(8)

Note that the complement indicator transform:
J(z; z) = 1 - I(z; z) can be used to define the joint prob
ability of exceedence:
D(z, z'; h)

= Prob{Z(z) > z, Z(z + h) > z'
E{J(z; z) . J(z + h; z')}

(9)

A full characterization of the spatial dependence be
tween two RV's Z(z), Z(z + h) is given by the bivariate
cdf (6), thus correspondingly by the set of all indicator
covariances for all choices of the pair of parameters z, z'.
Such indicator covariances can be inferred from experi
mental covariances run from the corresponding indicator
data. For example, the two-points connectivity function
F(z, z; h) can be inferred by the proportion of data pairs,
in number n(h), approximately distant of vector h and
jointly not exceeding z:
,(h)
FA(z, z; h) =

-

Wa=

i(z;z). i(z; z + h)

(10)

Alternatively, the function F(z, z; h) can be synthe
sized from "soft" structural information as provided, e.g.
by digitized geologically interpreted and drawn maps based
on outcrop sections and/or data from similar depositional
environments. Also the model F(z, z; h) can be borrowed
from a similar but more mature field where information
is more abundant.
Consider for example the indicator covariance (9) de
fined for a high threshold value z = z': if the two-points
connectivity function D(z, z; h) is valued high for all
h < h0 , one can expect a significant probability that two
locations z and z + h0 be linked by one or more strings
(paths) of low values (< z). However, it bears repeating
that D(z, z; h) is not the multivariate connectivity func
tion that one would ideally wish to have.
The two-points measure of connectivity,
whether F(z, z; h) or D(z, z; h), need not be isotropic (the
same in all directions of h), nor the same for all thresh
old values z. For example, very high permeability values
(open fractures) may be well connected in a given direc
tion and not in others, resulting in a strongly anisotropic
covariance D(z, z; h); whereas low permeability values (e.g.
non-pay shales) may be more isotropic in their horizontal
variability.
The covariance of the original random function Z(r),
as defined in Lesson 1(32), is but a moment of the bivariate
cdf F(z, z'; h):
C(h) = E{Z(z)Z(x + h)) - [E{Z(x)}]2

(11)

JOURNEL
Just like the variance of an univariate cdf F(z) does
not characterize fully that cdf, the covariance C(h) does
not characterize fully the bivariate cdf F(z, z'; h) i.e. does
not reflect the various connectivity functions F(z, z; h) or
D(z, z; h). Consequently, a stochastic simulation
{zt(z), z E A) that reproduces only the covariance C(h),
or equivalently only the semi-variogram -t(h), does not
necessarily identify any of the important connectivity func
tions F(z, z; h) or D(z, z; h). There are cases where iden
tification of one or more critical connectivity functions,
i.e. indicator covariances F(z, z; h) or D(z, z; h), is more
relevant than reproduction of their indifferentiated aver
age C(h).
The Gaussian model.

indicator covariances, i.e., connectivity functions of type

(7) or (9).
Maximum entropy.
Not only do Gaussian model-related simulations lack
the flexibility to identify actual indicator covariances, they

impart to the realizations {zt(z), z E A} a severe maxi
mum entropy character. It can be shown [Jones 1979,
p. 137], that among all possible random function models
with an imposed covariance CQh) or correlogram p(h) the
Gaussian model maximizes entropy, i.e., spatial disorder.
Indeed, in expression (12) for extreme threshold values
y, whether extremely low or high, V2 is large and the
integral is close to zero leaving a constant value for the

bivariate probability.

The Gaussian RF model is remarkable and particu
larly congenial in the sense that it is fully determined by
the single covariance C(h) of type (11). Conversely, the
Gaussian model does not leave any flexibility for the re
production of more than one indicator covariance model,
[Journel and Alabert, 1988).
Consider a Gaussian RF Y(z), with zero mean and
correlogram py(h) = Cy(h)/Cy(o). Ar.- adicator covari
ance of type (7) or (10) is fully determined from py(h),
[Xiao 1985], e.g.,
Prob{Y(x) > y, , Y(z + h) > y,} =

= ( -)+T
1

33

I (10 P)2 +h]exp

1 + sin0]

(12)
dO

with yl, = G-(p) being the p-quantile of the standard
normal cdf, see Lesson 1 (14). For example, at the median
threshold: Y.s = 0:
1

1

Prob{Y(z) > 0, Y(z + h) > 0} = 1 + -Larcsinpy(h),
4 2ir
(13)
[Abrarnovitz and Stegun 1964, p. 936.]
In practice, the original variable Z(z) may not be
normally distributed, thus the Gaussian model would ap
ply to its normal score transform Y(z) = T(Z(z)), e.g.,
Y(z) = In Z(z). However, such univariate-type transform
does not change in essence the bivariate cdf. Indeed,
Prob{Z(z) > zp, Z(z + h) > z,I =_

(14)

Prob{Y(z) > y,, Y(z + h) > ye), for all p,p' E [0, 1)
whatever the monotone transform T.
(z,, z,,) and (yV, ,) are the p, p'-quantiles of the uni
variate cdf's of respectively Z(z) and Y(z).
Thus, a normal score transform does not introduce
any additional degrees of freedom for modeling of specific

Prob{Y(z) > y, , Y(z + h) > y,)} :-

(15)

Prob{Y(z) > y,) . Prob{Y(z + h) > V,) = (1 - p)2
when p tends towards 0 or 1.
Thus, the Gaussian model does not allow for spatial
connectivity of extreme occurrences: the two indicators
I(z; x) and I(z; z + h) become independent as the thresh
old value z becomes extreme (departs more from the me
dian).
If high permeability values might connect each other
in space generating preferential flow paths, a Gaussian
related model (e.g., a lognormal model) would be inad
equate, because it would fail to reveal the possibility of
occurrence of such preferential flow paths. Maximum en
tropy, i.e., minimum ordering of extreme values, does not
represent a conservative (i.e., less favorable) model for
flow simulations and reservoir management.
The extreme convenience of the Gaussian model is no
sufBcient justification for adopting a model that may dan
gerously understate the potential for unfavorable spatial
distributions. It behooves the geostatistician to come up
with non-Gaussian alternatives allowing patterns of con
nectivity of high/low values, if only for worst cases anal
ysis.
Simulation features.
Stochastic simulation provides alternative, equiproba
ble sets of simulated values {zt(z),z E A} , I = 1,---,L
mimicking part or all of the bivariate spatial variability
of the unknown true values {ze(z),z E A).
The vast majority of simulation algorithms are Gaussian
related. They yield simulated sets values {zt(z), x E A}
which features:
(i) reproduction of an univariate cdf, usually some his
togram deemed representative of the domain A

34

FUNDAMENTALS OF GEOSTATISTICS

(ii) honoring the data values at data locations, i.e.
== zXx

(16)

for all realization t, at any datum location zx
(iii) reproduction of the covariance C(h) as defined in
relation (11)
(iv) a multivariate Gaussian distribution for the nor
mal score transforms, [Journel and Huijbregts 1978),
[Mantoglou and Wilson 19811, [Luster 1985], and
[Alabert 1987a).
Because of the limitations of the Gaussian model, for
some applications we may trade the last two features (iii)
and (iv) for:
(iii-a) reproduction of any number K indicator covariances
F(zk, zk; h) of type (6) or (10)
(iv-a) honoring not only the hard data values z(z ) but
0
also any number of soft local data, such as con
straint intervals z(z,) E]a 0,,b 0 j and/or prior prob
ability distributions for the datum value z(xo), see
Figures 2b and 2c.
Sequential indicator simulation
Consider K threshold values zk, k - I,.., K discretiz

R2

TO

--0

-0

"0

z(x --- -0
-I

z(X
2)E

X3

X4
-0O

TO

-0

0.5

0.7

Z(X3 ) C

-?

z(x)
.7

0.9
ýb2

.1

.9
-I
'I

.9

-I

.9

-I

'I

I1
I1

f
a-Hard
datum
Fig. 2.

b-Constaint
interval

Indicator coding.
Any particular hard (or assumed so) datum z(zl) can
be coded into an indicator data column
{i(zk; z 1 ), k = 1,.-.,

K), with the indicator data defined

as in (8), see Figure 2a.
At the location X2 of a soft datum known only to be
in an interval, z(X 2) E Ia 2, b2], the indicator data column
is incomplete: the values i(zk;z2) for zk E Ia 2, b j are
2
unknown, see Figure 2b. Such constraint intervals may
arise from, say, soft porosity data deduced from well logs
or seismic data, the bounds a., b. being deduced from
calibration using neighboring hard data, [Thadani, et al.
1987].
At some other locations z 3 the soft datum z(z ) may
3
be known to lie within an interval Ja , b3l with some prior
3

probability distribution. Thus, at that location:
i(zk; X3 )

ing the range of variability of the attribute Z(z). K can
be made as large as needed, however, in practice a small

Xl

number from 3 to 9 would be sufficient, defining from 4
to 10 classes for Z(z). Indeed, in many applications the
exact value of z(z) with 2 or 3 digits of precision is not
as important as the fact that it is low, median or high
and whether those lows, medians or highs are spatially
connected or not.

= 0, for all z, < a 3
- Prob{Z(z 3 ) < zk I, for all zk EI
a3 , b3]
- 1 , for all zk_ b3

(17)
Figure 2c and [Journel 19861.
At any unsampled location, where no soft information
pre-exists, the indicator data column is void, see Figure
2d.
The idea of indicator/probability kriging (IK/PK) is
to complete all indicator columns with a probability val
ued between [0, 1], see Lesson IV (26, 29).
The idea of sequential indicator simulation is:
(i) to complete a first indicator column for all K thresh
old values zk, say, at location z by IK or PK.
(ii) to draw from that indicator column (posterior cdf)
a value, say, z,(z)
(iii) to move that simulated value ze(z) into the condi
tioning set, ...

t

t

c-Prior
cdf

d-No
information

Indicator coding of prior information

and repeat the process at another location z', until all
required locations are informed by a simulated value.
At each step, the process is conditional to the initial
information and all previously simulated values. As the
sequential process moves forward, only the nearest condi
tioning data are retained for the IK/PK algorithm deter-

JOURNEL

35

mining the posterior cdf from which the simulated value
is drawn.

Thus, the SK indicator estimator P*(zk; x) is such that
it honors all the indicator covariance model values,
Ck(x - x,). The condition (iii-a) is therefore verified.

Exactitude property.

Implementation problems.

Since the IK/PK algorithm is class-exact, see Lesson
IV (35), the simulated values ze(z) will also be class-exact,
in the sense that, e.g., at a location of a prior interval
z(X 2 ) E]a 2, b2], all simulated values zt(z 2), t = 1,...L,
will honor that interval up to the class zk, zk4+j resolution.
Thus, conditions (ii) and (iv-a) are verified up to the class
resolution of the initial indicator coding.

Implementation of a sequential indicator simulation
exercise requires the following five steps:

Indicator covariance reproduction.
Consider the simple indicator kriging estimate used to
determine a posterior cdf model for the RV Z(z):
r(Zk, z) - Pk =

.\o [i(Zk;
l
Z.) -

Pk)

(18)

with: p, = F(Z,) = E{I(zk; x)} being the mean indicator
value to be reproduced.
The corresponding simple kriging (SK) system is writ
ten, see Lesson 11 (12):
A,=Ck(:T

-

X.) = Ck(

-X

(19)

for all c = 1,--.,n
if the n nearest data locations zx are retained for IK.
Ck(h) is the centered indicator covariance model for
the threshold zk, i.e., according to relation (6):

Ck()= F(zk, zk; h) - pk

(20)

By construction of the SK estimate (18), the indicator
estimate is unbiased:

E{r*(zk; x)} = ph = F(zk),
thus condition (i) is honored: the model histogram will
be reproduced by the simulated values zt(z).
Consider now the covariance of the indicator estimator
I'(zk, z) with anyone of the n indicator data:
E{1r(Zk;z) - Pk][I(zk; Xo) - Pk(Z)]} =

x.) - Ph[lJ(Zk;X.) - Pk]} =
$s

ZACk(X0 - Zo) = Ck(z - Z.)
0=1

according to (20) for any a = 1,. .~n.

1. Indicator coding of all local prior information, in
cluding possibly soft data. This coding requires de
termining the minimum resolution acceptable and
the K threshold values zk.
2. Indicator covariance/variogram inference. The K
indicator covariances Ck(h) are either modeled from
the corresponding experimental covariances or syn
thesized from geological interpretation and draw
ings. It is often preferable to use geological inter
pretation, i.e., a form of "soft" structural informa
tion, rather than accept blindly implicit Gaussian
type indicator covariance models with their maxi
mum entropy character.
3. Define a random path through all N nodes to be
simulated.
4. At each node zj of the simulation grid, perform the
(up to) K indicator/probability krigings to deter
mine the posterior cdf of the unknown Z(z3 ) at that
location:
i*(zj,; x) = Prob{Z(z,) < zkjInformation}

5. Draw from that posterior cdf the simulated value
ze(xj) and add that value to the list of conditioning
data, to impart the necessary correlation structure
to the next simulated values.
6. Return to step 3 and loop over all N nodes
z,,j = 1,...,N.
Generation of another set of simulated values requires
starting the process again from step 3 or 4. Starting again
from step 4 amounts to keep the same random path, in
which case considerable CPU time can be saved: indeed
all indicator data configurations remain the same and thus
the kriging weights at each node can be saved for reuse at
that same node in other simulations.
The main advantage of the sequential indicator simu
lation algorithm over other more traditional, and Gaussian
related algorithms, is the possibility to control K spatial
covariances instead of a single one. Of course, this ad
vantage is real only if these K indicator covariances are

36

FUNDAMENTALS OF GEOSTATISTICS

m74- 111-5

N

U47

Uw

EPAs

55.74

40.5-55
13
[--] 19.5 -40.5

number of data
mean
variance
coefficient of variation
minimum
maximum
prob. [z < 40.51
prob. [40.5 < z _<55.1
prob. {55. < z < 74.1
Lprob. 1z > 74J

1600
55.53
249.
0.284
19.6
111.5
0.17
0.36
0.34
0.13

Fig. 3. Statistics and greyscale map of the exhaustive
data set

74 11IJ

Fig. 4. Greyscale map of 1600 kriged values from 64 data

Fig. 5. Greyscale maps of the three sets of simulated
values

JOURNEL

37

INDICATOR VARIOGRAMS "FOR
z4 .40.5
NS7 W

AG

NWE

IN
9

Mie I

.120
I

INDICATOR VARIOORAM FOR z - 16.0
NSrW

N#WE

,10a)

I

9

I

a

INDICATOR VARJOGRAMS FOR z. - 74.0
1457*W

MWE

h

h

PERMEASILITY VARJOGRAMS
N5?'W
9

h

,l•('N

N33" E
ic

a

h

Fig. 6. Indicator variograms of the simulation of figure 5a (dots .)
indeed representative of the spatial distribution of Z(x)
over the domain A.
The Berea case study.
The greyscale map of Figure 3 images the spatial dis
tribution of 40 x 40 air permeameter measurements taken
on a vertical slab of Berea sandstone of dimensions two
feet x two feet. This data set is described in [Giordano, et
al. 1985] and made available by Arco. The corresponding
statistics of the 1600 data values are attached to Figure
3. Note the diagonal banding, particularly strong for the
low permeability values.
Typically, the image of Figure 3 is not available and
must be reconstituted with much less data. Figure 4 gives
a greyscale representation of the kriged map (ordinary
kriging) using 64 data taken at random locations. Coin-

Dash line is the model

paring Figures 3 and 4, note the strong smoothing effect
induced by the kriging algorithm which is in essence a
weighted moving average.
Three sets of simulated values {ze(zj),j = 1,..., 1600),
1 = 1 .. , 3, using the sequential indicator simulation al
gorithm are given on Figure 5. Only four classes (K = 3)
were considered, thus yielding a rather gross resolution.
The three threshold values considered are: z, = 40.5
, Z2 = 55.0 , z2 = 74.0 millidarcies, corresponding to
the .14, .5 (median), and .84 quantiles, respectively, of
the 64 data distribution.
Figure 6 gives, as an example, the indicator variograms
calculated from the simulated set of Figure 6a together
with the corresponding models. The variogramn models
are seen to have been reasonably well-reproduced by the
simulated data set.

38

FUNDAMENTALS OF GEOSTATISTICS

The banding of the original Figure 3 is well reproduced
by all three simulated images of Figure 5. In particular,
the main streaks of low values are reproduced.
The variance of the 1600 simulated values of Figure Sa
is oa = 229, a value close to the variance 0 2 = 249 of the
original 1600 permeability values, whereas the variance of
the 1600 kriged values of Figure 5 is only a' = 175.
Selected Bibliography
Abramovitz, M. and Stegun, I., I164, Handbook of
mathematical functions, Dover PubL, 9th print,
1046 p.
Alabert, F., 1987a, "The practice of fast conditional sim
ulations through the LU decomposition of the covariance
matrix," in M,
v. 19, n. 5, pp. 369-387.
Alabert, F., 1987M, Stochasting imaging of spatial
distributions using hard and soft information, M.S.
thesis, Stanford University, 197 p.
Anderson, T., 1958, An introduction to multivariate
statistical analysis, Wiley and Sons, 374
Fogg, G., 1986, Groundwater and sand body interconnec
tedness in a thick multiple-aquifer system, in Water
Resources Research, 22(5), 679-694.
Giordano, R., S. Salter, and K. Mohanty, 1985, "The
effects of permeability variations on flow in porous
media," SPE Paper 14365, 60th SPE Annual Confer
ence, Las Vegas.
Hewett, T., 1986, "Fractal distributions of reservoir
heterogeneity and their influence on fluid transport,"

SPE Paper 15386, 61st SPE Annual Conference, New
Orleans.
Jones,
D.,
1979,
Elementary information theory,
Clarendon Press, Oxford, 182 p.
Journel, A. and C. Huijbregts, 1978, Mining Geostatistics,
Academic Press, Chap. 7, 600 p.
Journel, A., 1986, "Constrained interpolation and
qualitative information," in M
,
v.
18,
n. 3, pp. 269-286.
Journel, A.G. and Alabert, F., 1988, Focusing on spatial
connectivity of extreme-values attributes: Stochastic
Indicator models of reservoir heterogeneities, SPE
paper 18324
Luster, G., 1985, Raw materials for Portland cement:
Applications of conditional simulations of coregionaliza
tion, Ph.D. thesis, Stanford University, 531 p.
Mantoglou, A., and J. Wilson, 1981, Simulation of
random fields with the turning band method, Report
No. 264, Department of Civil Engineering, M.I.T.,
199 p.
Thadani, S., Alabert, F. and Journel, A.G., 1987, An
integrated geostatistical/pattern recognition technique
for the characterization of reservoir spatial varia
bility, in Proc. of SEG Annual Meeting, New
Orleans,
1987
Xiao, H., 1985, A description of the behavior of
indicator variograms for a bivariate normal distribution,
M.S. thesis, Stanford University, 51 p.

SUBJECT INDEX

A

E

Additional sampling: 29
Anisotropy: 9-25
Averaging decision: see Stationarity
Autocorrelation function: see Covariance function

Entropy: 33
Estimate, see also Kriging: 23-27
Estimation error: 10
Estimation variance: 10-15-19-22-31
E-type estimate: 27-28
Exactitude property: 12-16-24-26-35
Expected value: 3-18
Extreme value: 31

B
Bias, see also Unbiasedness conditions: 10-15-18
Bivariate distribution: 6-7-32
C

G

cdf (cumulative distribution function): 2-22-24
Central limit theorem: 5
Class means (determination): 28
Cokriging: 13-26
Conditional (posterior) distribution: 21-25-30-35
Conditional expectation: see E-type estimate
Confidence interval: see Probability interval
Connectivity (spatial): 32-33
Constraint interval, see also Soft kriging: 26-34
Correlation (coefficient of -): 7-11
Correlogram: 16-17
Covariance: 7
Covariance function: 12-19-25-32
Covariance matrix: 10-13-17
Cross-covariance function/matrix: 13

Gaussian model: 4-33-34
Generalized increment/covariance: 20

H
Hard data: 26-34

1
Independence: 8-1 1-16-23
Indicator coding (of information): 26-34
Indicator covariance/variogram: 25-32-35-37
Indicator kriging: 25-34
Indicator variable: 24-32
Interpolation: 12-14-23
Interquartile range (IR): 3
Intrinsic random function: 19

D
Data orthogonalization: 13
Data redundancy: 14-17
Data uncertainty: 26
Decision making (under risk): 29
Dependence (measure of.-): 6-24-32
Drift: see Trend
Dual kriging: 14

3
Joint probability distribution: 30
Joint spatial uncertainty: 30

39

40

INDEX

K
Kriging, see! also Simple -,
Ordinary -,
Universal ., Indicator Probability

10-15

Kriging variance: 11-16-19

Probability intervals: 23-24
Probability kriging: 26-34
Probability maps: 24
Probability of exceedence: 2-29-31
Q

L
Least-squares estimate: 27
Linear estimator: 10
Linear operator: 4
Linear regression: see Regression, Kriging
Linear transform: 7
Logarithmic parameters: 5
Lognormal model: 5-33
Loss function: 27
LU-decomposition (of kriging matrix): 13

M
Mean abosolute deviation (mAD): 4
Median: 3-23-27
Multivariate distribution: 30

N
Non-parametric geostatistics: 22
Normal model: see Gaussian model
Normal system: 11-16-25

0
Optimal estimate: 27
Order relations: 4-24-25
Ordinary kriging: 15-25-37
Orthogonalization (of kriging matrix): 13

Quantile: 3-6-28
Quantile estimator/map: 28
Quartile (lower/upper): 3
R
Random function (RF): 8-32
Random variable (RV): 2-10
Rank order transform: 26
Regression (linear without constraint): 2
Regression (linear with constraint): 10
Residual covariance: 19
Response function: 31
Risk assessment: 29
S
Scattergram: 6-12
Semi-variogram: see Variograrn
Simple kriging: 11-17-35
Simulation: 30-31-34
Skewness sign: 4
Smoothing effect: 31-37
Soft information/kriging: 26-34
Spline interpolation: 14
Stationarity: 8-22-24
Stochastic simulation: see Simulation
T
Transfer function: 31
Trend (Kriging with a _):

18

U
Unbiasedness conditions: 15-17-18

P
Parametric vs. non-parametric geostatistics: 22-30
Percentile: 3
Polynomial trend: 18-20
Positive definite condition: 9-11-16-19
Posterior distribution: see Conditional distribution
Prior distribution: 26

Uncertainty (measures of

Uncertainty imaging: 30
Universal kriging: 19
V
Variance: 3-9
Variogram: 6-8-17-25

_): 22-25-30

1 REVIEW

Non-Gaussian data expansion in the Earth Sciences
A.G. Journel and F. Alabert
Stanford Center for Reservoir Forecasting,Applied Earth Sciences Department, Stanford University

ABSTRACT

A formalism is proposed to generate alternative equiprobable images of an under
lying population spatial distribution. The resulting images honour data values at
their locations and reflect important characteristics of the data such as patterns of
spatial connectivity of extreme-values. The formalism capitalizes on a coding of all
information available into bits (0-1), which are then processed all together
accounting for their patterns of correlation in space. Such common coding allows
accounting for qualitative information, possibly of an interpretative nature, to
complement the usually sparse hard data available in Earth Sciences applications.
The approach proposed, although of a probabilistic nature, does not call for any
Gaussian-type modelling or hypothesis.
TERRA No'a 1989, 1, 123-134.

INTRODUCTION
Popular understanding tends to limit statistics to a data
reduction (descriptive) r6le: large tables are summarized
into a few numbers or graphs such as mean, median,
histograms and pie charts. In fact in many applications
statistics are taken towards a data expansion (inference)
objective. In Earth Sciences applications, for example, the
ratio of sample size to population size is usually extremely
low, from 10' to 10-a or below. The problem is not to re
duce the information but to expand it towards a picture of
the considerably larger population. At the heart of that
data expansion process is a probablistic model that allows
for a number of controlled interpolations between the data
available. The critical question is 'how good is the model
and have enough alternative models been tried?'
Consider the grey-scale map of Fig. 1. It presents the
spatial distribution of 40x40 air permeameter measure
ments taken on a vertical slab of Berea sandstone of dimen
sions 2x2 foot (Giordano et al., 1985). These 1600 horizon
tal permeability measurements taken on a regular grid will
be considered as providing an exhaustive sampling of the
slab population at the scale considered. Note the clear
diagonal banding and the strong connectivity of low
values.
Typically, the image of Fig. 1, or at least its important fea
tures, must be reconstituted from a much smaller sample.
Sixteen randomly located sample values were drawn from
the 1600 reference database. Their locations are given by
crosses on the location map in Fig. 12.
Figure 2 gives a first reconstitution of the reference map
of Fig. 1. A generalized linear regression algorithm, also
known as ordinary kriging (Goldberger, 1962; Journel and
Huijbregts, 1978) is used to interpolate from the 16 data.
The kriging algorithm is 'exact' in the sense that it honours
the data values at their locations. The algorithm accounts

for data spatial correlation through a covariance model.
This covariance model is usually inferred from the experi
mental correlation observed between the data available.
However, since statistical inference is not the subject of this
paper, all covariance models used in this paper were de
rived from all 1600 values of the reference database.
Figure 2 reveals the well-known spatial smoothing
characteristic of all regression and moving average
techniques. The initial 16 data are honoured through local
discontinuities whose amplitude depends on the high fre
quencv content of the.movariance model used.
Figure 3(a) gives a second reconstitution drawn from a
stochastic Gaussian-related model (Journel, 1974). A Gaus
sian-related model is fully characterized by a marginal dis
tribution or histogram, not necessarily normal, and bv a
single covariance function. Then realizations are drawn

Berea sandstone data set
40

mOarcy
76.111.5

O 68.76
E] 58
E] 55.58
51 •55
47 - 51

42.5 47

S19.5-42.5

Fig. 1. The Berea data set. Colour-coded spatial distribution ot
1600 permeability data.

123

REVIEW
Kriging map, 16 hard data

Gaussian simulation
mOarcy
-'

40

76-111.5

I

68-76

n

58-68

EJ 55.58
51-55
47-51
42.5.47
*19.5

42.5

I
40

0-58 - 68
Indicator simulation

124

76-111.5
68-76

Fig. 2. A regression-type estimated map. (Ordinary kriging using
16 da a.)

from it through a generalized Monte Carlo procedure.
Figure 3(a) corresponds to one such drawing. Each realiza
tion is 'conditional' to the data (16) availabUtz in the sense
that it honours the data values at their L - ins. In addi
tion, the simulated values of each realizati.,-i are related in
a way which ensures reproduction of the input covariance
model.
If Fig. 3(a) reproduces reasonably well the diagonal
banding of the exhaustive map of Fig. 1, it does so with ex
cess local noise. This noise tends to blur the strong connec
tivitv of low values. At a larger scale such connectivity of
extreme-valued permeability may be the most important
feature. controlling fluid flow, say in an oil reservoir or, a]
ternativel, a nuclear repositor-.
Figure 3(b) provides a third reconstitution based on a
-non-Gaussian stochastic model. This model accounts for
the spatial connectivity of extreme-valued permeabilities
through a sequence of indicator covariance models. As
defined hereafter, an indicator covariance is a measure of
the joint probability of two values in space exceeding a
given permeability threshold. An indicator covariance is
thus a model of two-point spatial connectivity. As was the
case for the two previous reconstitutions of Figs 2 and 3(a),
the 16 data values used are reproduced at their locations.
However, the diagonal banding and particularly the con
nectivity of low values is reproduced with much less noise:
compare with the exhaustive map of Fig. 1.
Figure 4 is based on the same non-Gaussian model and
the same 16 data as used for Fig. 3(b). However, the aniso
tropy of the indicator covariance model (connectivity
model) f&r the high permeability values has been intention
ally inverted, generating banding of high values ortho
gonal to the true banding. The same 16 data, however, are
honoured at their locations.
Since all four images of Figs 2-4 honour identically the
same 16 data, they cannot be differentiated from statistics

mOarcy

[

55 -58

U

S1-55
47-51

C
*

425

47

19.5

42.5

Fig. 3. Two reconstitution using 16 data. (a) By drawing from a
Gaussian-related model (b) L:sing an indicator simulation

approach (non-Gaussian).
based on these data. Yet these four images appear quite dif
ferent, and any planning based on them might yield vastly
different decisions.
Imaging of spatial uncertainty
None of the three images of Figs 2 and 3 identifies the 'true'
distribution of Fig. 1. In any process of data expansion,
whether interpolation or simulation, there is necessarily
some error involved and the resulting spatial
uncertaintY
should be assessed.
A unique estimated map such as that of Fig. 2, no matter
the criterion for which it is optimal, does not carry any
measure of spatial uncertainty. A measure of spatial uncer
tainty is necessarily multivariate: it should not only ad
dress the uncertainty at any particular location u within a
site but also the joint uncertainty along any string of loca
tions u., I = 1 ..... L within that site. For example, if a string
of high values is observed on any one ot the reconstituted

.111'

Orthogonal anisotroples

REVIZ
Consider the distribution in space of an attribute :, say

mDarcy

E- 76-111.5
-1

68-76

8E5

68

E 55-58
U 51 • 55
47.5f
42.5. 47

E

19.5 -42.5

40

Fig. 4. A reconstitution wvith inverted anisotropy for the high
values (non-Gau-.sian model and 16 data).

images of Figs 2 and 3 and the existence of such a string is
highly consequential for the project at hand, the question
arises as to how reliable that string is.
Stochastic simulation allows drawing several equiprob
able realizations all honouring the same initial data at their
locations. Figure 5 gives three such equiprobable simu
lations (reconstitutions), among many more possible, of
the reference image of Fig. 1. All such simulations use the
same non-Gaussian model and the same 16 conditioning
data as used for the fourth simulation of Fig. 3(b). Spatial
features observed consistentlv on all simulated images
may be considered as reliable, i.e. representative of the
true reference map, whereas features present on some
simulations but not on others would be deemed unreliable.
These introductory examples emphasize the uncertainty
faced in data expansion, and how the results depends on
the prior model explicitly or implicitly adopted through the
choice of an interpolation or simulation algorithm. The
process of inducing a model, be it deterministic or prob
abilistic, from prior information is usually more important
than the details of how to extrapolate (deduction process)
from that model.
It behoves the cartographer to provide a readily usable
assessment of spatial uncertainty, and the statistician to
come up with stochastic models with greater flexibility
than Gaussian-related models.

that of horizontal permeability :(u), u being the location on
the vertical sandstone slab of Fig. 1. The spatial depen
dence between any two values :(u) and :(u+h) separated
by vector h is modelled by the bivariate probability distri
bution:
P(Z(u) -_5 z. Z(u-1-h) -< z'}.

(1)

A stationaritv decision over a given domain (population)
S allows inference of the probability distribution (1) or part
thereof. The stationaritv decision defines a space where
repetitiveness can be found, giving physical significance to
the probability (1) and allowing its inference from corres
ponding proportions of data pairs.

3 Indicator simulations

mDarcy

"D76-111.5
El168.76
E'

-68

51-55

E

47-51
42.5-47
19.5-42.5

MODELLING SPATIAL DEPENDENCE
In most Earth Sciences related applications spatial depen
dence is an irrefutable hard fact that cannot be ignored.
Identification and reproduction of patterns of dependence
in space and or time might be the very goal of the statistical
analysis. In mapping problems spatial dependence is a
blessing for it allows building from the correlation of
nearby data to estimate (predict) a particular unsampled
lu e .

S.va

Fig. 3. Three equiprobabie reconstitutions (using 10 data and thei
saine non-.,au-.ian model as for Fig 3b).

125

7WsM

REVIEW

Indicator statistics
Indicator coding of information and indicator statistics
are
particularly convenient to infer characteristics of spatial
de
pendence such as the bivariate probability distribution
(1)
(Switzer, 1977; Journel, 1983).
The indicator transform of the spatially distributed
attri
bute z(u) is defined as the binary variable:
i[Z(u); z] = i(u; Z)

1, if not)

(2)

At any location u, where a datum z(u,) is available, the
whole series of indicator data i(u ; z) for all threshold
values
1
z is also available.
The indicator values i(u; z) are interpreted as outcomes
of indicator random variables l(u;- z). The marginal
dis
tribution of the binary random variable I(u; z)
is fuily
characterized by a single parameter, its mean or
expected
value, which is seen to be the marginal probability
distribu
tion of Z(u) for the threshold value z:
E{I(u, z)} = I xP{Z(u) _- z}+0xP{Z(u) > z}
= P{Z(u) <_ z} = F(z).
(3)
Following from the decision of stationaritv the
distri
bution model F(:) is made independent of the location
u
and is inferred from a spatial average of the corresponding
indicator data available.
Similarly, the bivariate distribution (1) of any two
ran
dom variables Z(u) and Z(u+h) is seen to be the
expected
value of the product of two indicator random variables,
e.g. in the casez - :':
E{1(u; z). I(uh+:h)} = P{Z(u) _- z, Z(u+h) -_ z}.
= F(h; z).

Expression (4) appears to be the non-centred covariance
of the indicator process 1(u; :) There is one such indicator
covariance for each new threshold value z.
The indicator correlogram, or measure of linear corre
lation between two indicators 1(u; z) and l(u+hI z) is tne
centred standardized covariance:
p1(h, z) - F(h; z)-FF2():
F(z)[1-F(z)] E

-)
+1+(5)

with: Var{l(u; Z)} = F(z)[1-F(z)J.
The threshold parameter z can be written as a p-quantile
such as: F(z.) = p E [0, 1], in which case the previous
indicator statistics (4) and (5) are parametrized
with the
probability value p.
Z'

E ,"Iu;z;.)}

= F{l(u; ,))

Varil(u; p)} = p(1 -p)
pj(h; :;*)

= p;(h; 1).

(6)

The indicator formalism allows inferring Z-probal:.iiitv
distributions through indicator statistics: mean indicator
data for the marginal distribution F(z), indicator
covariance
for the bivariate distribution F(h; z), (Journel,
1983). For a
given set of data locations, inference of the indicator
covariances is usually easier than inference
of the tradi
tional Z-covariance, at least for not too extreme
thres1co'd
values of z. Also indicator covariance models
can be svn
thesized, e.g. from interpretative hand-drawn
maps, to re
flect patterns of spatial continuity of the process
Zx).
Besides its relation to the bivariate probability
F(h; :) the
indicator correlogram p.,(h ') can be read
as a unit-free
measure of two-points spatial connectivity:
the higher
p.(h; p) the greater the probability of having
two values z(u)

Table 1. Statistics of the 1600 reference permeability data.

Berea data set
0.2

number of data
mean
vtriance

0.15
U

0.1
0

a.'
Ca..
0.05

t
0

126

j 1600

55.526
coefficient of variation 249.
0.284
skewness
0.379
kurtosis
3.127
minimum
19.5
maximum
111.5
first quartile
4-5. 0
median
55.0
third q-uarti~le_
65.0

RE3VIEW
Normal probability plot

ri

I,,

" 0.75

W

0.5

E

"• 0.25
0

r,

data cumulative frequency
Fig. 6. Normal probability plot ot the reierence lot1h data (note the
cua-i-normalitv of that data seti

and z(u+h) jointly not exceeding (or equivalently exceed
ing) the same threshold value z., Conversely, when
p (h; p) = 0 from
relation
(5) it appears
that:
F(h; z•) = Fz(z,.) = p 2, i.e. the two events Z(u--h) _- z and
Z(u) <_ z are independent.
The quantification of spatial connectivity using indicator
covariances has been anticipated by a few authors, see in
particular (Haralick et al., 1973), but not actually put to
work for stochastic imaging.
Table I gives the statistics of the 1600 permeability data
shown in Fig. 1. The corresponding cumulative histogram,

i.e. the indicator mean E{I(u; ;')} = F(::.) is plotted on a
normal graph in Fig. 6.
The quasi-straight line indicates a good fit by a Gaussian
(normal) model. However this marginal Gaussian charac
ter does not imply any multivariate Gaussian character as
shown by the following figures.
Figures 7-9 give the indicator correlogram maps, pj(h;,10
corresponding to the p = 0.1, 0.5, and 0.9 quantile
threshold values of ;. The experimental correlograms cal
culated from the 1600 indicator data are given on the left of
the maps while the models calculated from a bi-Gaussian
hypothesis are given on the right. The maximum corre
lation value p1(0; p) = 1.0 is plotted at the centre of each
map. Each two-dimensional correlogram value p,(h., It:.; p),
with h = (h_, It,), is plotted at a point h,,-units left of and
hk-units above the centre point. Such correlation maps al
low a global appreciation of directional patterns of spatial
dependence.
The generation of the bi-Gaussian models on the right
hand side of Figs 7-9 is discussed in the next section.
All three experimental correlogram maps exhibit strong
anisotropy: the connectivity measure p1(h; p) depends not
only on the length ;hi of the vector h but also on its direc
tion. Maximum continuity is obtained in the N5TW
direction of banding: refer to Fig. 1. Most striking are the
differences in correlation behaviour as the threshold value
increases.
* At the low threshold value ,=
35.5 md, indicator cor
relation (p > 0.5) persists beyond the map limits in the
direction of banding but vanishes ven- quickly in the di
rection across band'ihg.
* At the median threshold value :,• = 55.0 nid, the indi
cator correlation anisotropy is less marked.

INDICATOR CORRELATION MAPS - 0.10 PERCENTILE
1600 data

Gaussian model

20

20

El 0.85 - 1.00
0.70 - 0.85

0.55 - 0o70
0

0

El

0.40-0.55
0.30-0 40

n] 0.20- 0.30
U 0.10- 0.20
[] 0M00 - 0 10
.20m
-20

0

Fig. 7. Berea indicator correhI,,rar in:.
1,

.

ad

m

20

-20

0

= 35.53 m.i) (a' Experinmental vaIlues (1rmM

a -20
20

16l00 data) (b) Cau-,-,ian rnodtdl

127

I

REVIEW
INDICATOR CORRELATION MAPS - 0.50 PERCENTILE
1600 data

Gaussian model
20

20

0.85 - 10.0
0.70-0.8s

E-0.55.0.70
0.40-055
0

0.30-0.40

H 0.20 -0.30
E

0.10- 0.20

S0.00-0 10
-20
-20

0

Fig. 8. Berea indicator correlograni p..(h; Zv,

20

-20

20.20

0

55,0 "id).(al E\perimental values (from 1600 data) (b) G.u-,

mode

INDICATOR CORRELATION MAPS - 0.90 PERCENTILE
1600 data
Gaussian model
20
0.85- 1.00
0.70- 0.85
0.55-0.70

[] 0.40-0.55
[] 0.30-0.40

0E

0.20 - 0.30
0.10-0.20
0.00-0.10
-2t
-20

0

Fig. 9. Berea indicator coTrelogram p I

20

-20

7fni
i da)

20

ENperimental values (from 3-00 data). (b) Gaui,,-ii model

* At the high threshold :,,4 = 76 nid, leaving only 10% of
Z-data above it, experimental indicator correlation be
comes almost isotropic: there is no strong banding of
high permeability values as can be checked on Fig. 1. In
dicator correlation in the direction across banding is
much stronger at high threshold values than it is at low
threshold values.
The three experimental indicator correlation maps show
that the pattern of spatial connectivity (correlation and
anisotropy) is clearly different for different classes of per
meabilitv %aluti•,.
A• a measure of comparison, Fig. 10 gives the Z-cor
re!ogram map. p/(h), of the z-permeabilit' attribute itsel!.
again calculated from all loot ) data. Thi:- /-correlhgrani
128

w•i-20

0

map appears as intermediary between the three indicator
correlogram maps at the left of Figs 7-9. This is no surprise
since it is known that an attribute covariance is the average
of all its indicator covariances and cross-covariance-,
(Alabert, 1987).
Cw(h)

dz f C

o(h;
z, z)d

,

h

1

(7)

with C,(h; z, z') = Cov{I(u; z), I(u-fh; :)lW
However, the Z-correlogram map of Fig. 10 undcrstatet,
severely the spatial connectivity and anisotropy of low per
meability valucs as evidenced in Fig. I and at the left of Fil,.
7. Conversely, Fig. It) oý erntate, the banding (COnnLe ti\ It'

UE-2

1600 data

=ýýmREV IEW

CORRELATION MAPS
Gaussian model

20

20
0.55-1.00

"E"0.70 - 0.85

U 0.55-0.70
0

-

0.40-0.55

Li

0.30 - 0.40

E 0.20-0.30
S0.10-0.20
000-O.0
-20
-2
-. ..
...
Z 0
O " 20 -20
"
-0
• -20-2()
Fig. 10. Berea permeability correlogram (from original 1600 permeability data). (a) E\perimental values (from 1600 data). (b) Gaussian
model.
and anisotropy) of high permeability values as seer -n the
left of Fig. 9.
If detection of such connectivity of low values (flow bar
riers or aquitards) is critical, the average correlation image
given by the Z-correlogram of Fig. 10 is inadequate.
'-.-The Gaussian attraction

The Gaussian mode! holds a dominant. though question
able, place in statistical theory for it provides a full mul
tivariate probability distribution at very little inference
cost.
First a normal score transform is applied to the original
z-data. This transform allows the defining of a new set of
z-data with standard normal marginal distribution (Ander
son, 1958):
y(u) = Tjz(u)], and: :(u) = T- '[y(u)].

11)1
piftJ

p•(h; -2

The transform T is usually made monotone and invert
ible so that any proposition made on the ys can be trans
formed back into asimilar proposition made on the original
zs. In particular, the multivariate distribution of the pro
cess Z(u) is expressed in terms of the multivariate distribu
tion of the Gaussian process Y(u), itself fully determined
from the Y-correlogram pý(h):

P{Z(u) -_ z;,, j = 1L .. ,n) = P{Y(u,) -- y,, J = 1_.... fl, (9)
with zP, and y, being the p,-quantiles of their respective
marginal distributions.
The bivariate case, n = 2 in expression (9), is particularly
illustrative because analytical expressions are available
Tnderson, 1958; Xiao, 1985; Abramovit7 and Stegun,
J164). The Z-bivariate distribution, expressed in terms of
indicator correlogram as in relations (5) and (h), is written:

f~i

(-

)hO

(10)

0

with y:. being the standard normal p-quantile, and:
p,(h) = E{Y(u) Y(u+ h)} being the Y-correlogram.
The Gaussian model (10) possesses an extreme, and un
fortunately not always well understood, property of
maximum entropy Jones, 1079) i.e. of ma\imum disorder
for a given covariance pj(h). This maximum entropy
character is shown by the behaviour of the Gaussian indi
cator correlogram (10) for extreme values of the threshold
y,,: when p tends towards zero or one, the normal p-quan
tile tends towards ± x, and the expression under the inte
gral sign (10) tends towards zero leaving:
p,(h; p) -" 0.

(S)

ep

11(1p)
-)

(11)

The Gaussian model does nmoallow for any indicator
correlation at extreme threshold values, i.e. for any' spatial
connectivity of extreme values. Moreover, the pattern of
loss of indicator correlation is symmetric as the threshold
value z, moves away from the median value z,.;. Indeed the
expression (10) is symmetric about p = 0.5, since Y = 2._;..
Figures 7-9 provide side by side the experimental Berea
sand indicator correlogram maps and the corresponding
Gaussian models as calculated from expression (10). The
experimental correlogram maps show patterns of correla
tion that are neither symmetric about p = 0.5, nor decreas
ing as p comes closer to zero or one. In fact, the best indi
cator correlation is found for an extreme value p = 0.1 in
the direction of low-permeability banding. The divergence
between the experimental indicator correlation, and the
Gaussian models iý seen to be larg4er for the t% o extreme
thresholds (p = 0.1 and 0.qi.
129

RE1tIEW

-

Multiple steps connectivity
Just as the two points (u) anLd

(u~h) connectivit\, is

charac
terized b' the mean of the
product of two indicator
variables 1(u; :) and I(u--h; z)
as in relation (4), a three

point connectivity can be
characterized

ot
the
product
of three
1(u: _-),!(u~h; --], l(u+2h;
z):

by the mean

indicator

variables

The reference map (upper curve)
connectivity followed
exhibats
by the simulated
map the
using
a non
strongest
Gaussian model. The Gaussian-related
model resulted in
the lowest connectivity.
A map based on spatial independence
would entai:
4)(n) = [F(z,)]': = p", for all n.

EJ1(u, Z) - I(u-+-h; Z) - (u +-2h; z)}

On Fig. 11, for it = 2, it is read:

Similarly, n-point connecti%it,
in a particular direction can
be characterized by the mean
of the product of it indicator
variables:

4)(2) = 0.065 for both the reference
map and the non
Gaussian simulated map, Figs I
and 3(b) respectivel",
db(2) = 0.038 for the Gaussian-related
map of Fig. 3;a i,
4(2) = ;72 = 0.01 would correspond
to spatial indepen
dence.

EfJ--'u+ (j--I)h;

:

6(n)

The elementary lag vector h considered
hereafter has for
direction, .57\W the direction
of banding on Fig. 1; the
modulus h is the basic step
between two pixels in that
direction. The threshold value
considered is the low 0.1
qulicn!:e :

= 35.5 Ind used to specify connectivit%

ot low
values. Figure 11 gives three
experimental connectivity
functions 6(n), for up to n = 16
steps, in the N57:WV direc
tion. The upper curve corresponds
to the 1600 data of the
reference map shown in Fig.
1. The middle curve corre
sponds to the simulated map
of Fig. 3(b) built from a non
Gau-sian model. The lower
curve corresponds to the
simtulated map of Fig. 3(a) using
a Gaussian-related model.

When used for data expansion
a maximum entropy
model, such as the Gaussian model,
need not be the most
conservative. In the Earth Sciences
and civil engineering,
lob entropy patterns such as connected
strings of extreme
values often correspond to
hazardous features whose
potential occurrence should
not be understated. Areas
with potential clusters of low
soil strength art: hazardous
pile sites for construction; strings
of high permeability
values can be leakage conduits
hazardous for the design of
a nuclear repositorn, or detrimental
to oil sweep efficiency
in a reservoir. When mapping
in the field or drawing on a
board geologists seek low entropy
features. Conversely
statisticians prefer not to commit
themselves and ,,ould go
for maximum entropy models,
not always realizing the ad
verse consequence of such models
for certain applications.

0

0.1

0
0.075

PC

0.05

0
0.025

.0
. 1

.. . . -

04
0

Fig. 1 .
todir,

\ht 1 4,..
\.

4
65
8 of
7 Conne~fp
0 9 10
number
1a
0 111

cpcnn'cti\'it
Cth
fu iati ko6e1', I 'i' pro
:
bahihlt
tt7,1
ixes in a ro
A
at Maj ,rrt.,,ni
toi
I IbI.wIprr,,polu-L.ng
si

to Fig 3ra, i ((.auian simULatiton.L

130

2

indicator simulation

\

1?13
,1 , 14 15
al Iuede- than r

toFig. 3,b: (indcatorsinniut

18

,qual k
t,

=

, , ,

, . --Tr-

---

- ____

-

, -

_

____

-

EMENNi

7-Eý--p/-\REVVIEW
Tractable non-Gaussian models for data expansion
which can account tor low entropy characteristics of the
data spatial distribution should be proposed.

P{Z(u 2 ) - zI(ni+1)) = P{Z(u:) _ zIZ(u.) = z-. a, E 0(,),

SEQUENTIAL INDICATOR SIMULATION
Consider an\ tw\o random events, A: and A 2 , with joint
probability denoted bv P(A2 ,A1 ). For example, A. can be the
event 'Z(u) <*z' related to the permeability value z(u) at
location u or it could be the event 'earthquake of magnitude
greater than 7', or it could be 'rock type i is present at
location u'. The two events A, and A, need not relate to the
same variable, for example A2 may correspond to the ques
tion 'is z(u) -_ :' whereas A: may correspond to an informa
tion of interpretative nature 'sand is present at location u.'
The conditional probability of event A4,knowing that
event AI has occurred is given by Bayes postulate (Thomas,
1986):
P(A1),

P(A<I A1) = P(.

with P(AI) being the marginal probability of event .A, and
P(A,, A,) being the joint probability of events A1 and A,.
Joint simulation of the two events can be done by
simulating first the event A, from its marginal probability
distribution P(A;), then simulating the event A- from the
conditional probability distribution P(A2 1Ai). This se
quence requires inference of the conditional probability
More generallyv, any number of dependent events
A. = 1.... _, can be sequentially simulated using the
e\precsion (Devrove. 1986):

P(A,,A .
P(A\-21A,

=

1.....\-1) P(A,.-!A,.
i ....

'-3)

=

1 ....,N -2).

(12)

P(A,!A ) - P(AI).
-..

The technique requires inference of the successive (N-1)
P(A,,A.
conditional probability distributions P(A 2 A,)..
Relation (12) is absolutely general, in the sense that the N'
event A,can be of any nature. In particular it can be applied
to the simulation of joint realizations of N-dependent con
tinuous random variables Z(u,), j = 1,...,N modelling the
spatial variability of an attribute z(u) over a grid with N
nodes u, The sequence is:
1. Starting at any node, say ul, derive the conditional dis
tribution of Z(u1 ) given any available information. This
available information, denoted (n), usually consists in
known outcome values Z(u.) = z. at data locations u.. The
conditional or posterior distribution of Z(ul) is denoted:
PJZ(ul) < :(), = P{Z(ul) -_ z1Z(u,,)

of dimension (n-+
1).
3. Move at random to a second node, say u- Derive the
conditional distribution of Z(u-) given the information
(n + 1), i.e. the distribution:

= ,, CeE (17)1.

2. Draw a realization of Z(u,), say :., from that distribu
"tion.Transfer that realization into the data set which is now

Z(u,) = ").
Draw a realization of Z(u,), say z, from that distribution.
Transfer that realization -," into the data set which then
becomes of dimension (n+2).
Loop at random over all N nodes, until each node is in
formed with a simulated value
The set of N values {z,", j = 1 .... N} represents one
realization of the random field {Z(u), u E S} over the N
=
nodes u. Any number L of such simulations {:a,
en
the
repeating
by
1 ..... }, I = 1 .. L, can be obtained

tire sequential process L times.
Two major implementation problems must be ad
dressed:

"* the derivation of the (N-1) successive conditional prob
ability distributions of type P{Z(u,) -_ z:(,,+j-1)J

"* the increasing size of the conditioning information from
(ii) to (n-N+1).

Derivation of conditional distributions (Journel, 1983;
Sullivan, 1985; Suro Perez, 1988)
The event Z(u) -_ z can be characterized by a random bi
nary indicator variable, as defined in (2):
1, if Z(u) <:
0, if not
Any conditional probability for Z(u) can be written as a
conditional expectation of an indicator random variable,
similarly to relation (3):
P{Z(u) -_ zI(n)} = E{1(u; z)I(n)}.

(13)

Consider K threshold values zi, k = 1 ... , K providing a
discrete approximation of the range of variability of the at
tribute z. Each conditioning datum Z(u.) = z. can be coded
into an indicator data column with K members valued
either zero or one:
Z(u,) = z. -

{i(u.; 2k), k = 1,...,K).

(14)

The previous probability distribution can be seen as con
ditional to the n indicator data columns, i.e. to the nxK
indicator data:
zk,,IZ(u.); = z., ca E (n)}

P{Z(u)
-

EJ(u; z,) I1(u,.; Z•) = i(u.; :•),
k = I_... K;cE (n)i,

(15)
131

TI7'JL REVIEW
with 2 4 being one of the K threshold values Z
considered.
Arguing or assuming that l(u; z") is more correlated with
I(u.; z,) than with any of the other indicator data l(u,,; "-,)
with z P :_,, the probability (15) can be approximated by:
P{Z(u) _ z, 1(n)} = E{l(u; z, 1(u.; z,) = i(u,,;:A,), u E (E)O.
(16)
This approximation amounts to ignoring all cross-corre
lation between indicators defined at different threshold
values. The result is a considerable reduction in the
number of conditioning indicator data, it instead of Ki. In
each particular application, the indicator cross-correlations
can be calculated and compared to the indicator auto
correlations of type (5). In cases when these indicator cross
correlations cannot be ignored, a more demanding for
malism using indicator principal components can be called
for (Suro Perez, 198S).
A conditional expectation can be written as a function 6
of the conditioning information, i.e. for the conditional
expectation (16):

E{l(u; ,,) i(u_; , ) = i(u,,; :k ), a E (n)}
6)i(.uý; Z, ),a E (11)}
=
aC+'• a,(c)i(u,,:-,)
=

+7

Vz

t- E (I:

Ez (,:!, , =

+ia,.7

iu,; 2-,)
K

a-(a, c')i(uý; Z.) . i(ut .; ::, )-+-..
,l

(17)
where the symbol H indicates multiplication.
Indeed any function of one single indicator variable can
be written as a linear combination of that indicator
N'(i) = a0o+ai. Similarly any function of any number i of
indicator variables can be written as a linear combination
of
products of these indicators (Journel, 1986). Then, a first
order approximation to the conditional probability (16)
amounts to retaining only the first (n+]) terms of the
expansion (17):
P{Z(u) -- Z,,(10)) = a,)+

a,(a) - i(u,,;

k.)

(18)

The argument is that introducing more terms of the ex
pansion (17), such as the products i(u,; zik) - i(u..; AX),
would require the use of trivariate and higher order statis
tics for which inference is rarely possible. Limiting the
expansion (17) to functions of one indicator at a time as in
(18) reduces the inference needed to only bivariate dis
tribution functions expressed as indicator covariances, see
definition (4).
132

It remains to determine the set of (17-.-) coefficients ai,
ai(c) for the approximation (18). These coefficients arc ob
tained by linear regression of the unknown indicator
i(u; :J) on the n indicator data i(u,,; z, ). That regression calls,
only for the indicator correlogram p,(hJ,
) as detined in
relation (5j. In its simplest form, the regression estimate is
written (Journel, 1983):
[P{Z(u) -_ zj,!(,i)}-F(zJ ]"
,(,*-[iu :: - {,)

(19)
with the it weights {,,(k0), a ( (ii)} given by a system
of
normal equations.

'

,d(k
0 ) ' p,(u,,-uO; A, ) = pi(u-u,,; 2. ), for all , C (1:),
(20)

The choice of a linear regression procedure to determine
the weights a. of expression (18) is not arbitrary. It can be
shown (Journel, 1986) that a conditional expectation being
by definition a L2-norm projection on the 'nformation
space, a regression technique must be used to determine it.
Also the normal system (20) can be seen as an identification
of the correlation of the estimator (19) with any one of the
it indicator data I(u,,; zi) to the model indicator correlogram
value pl(u-u,; z, ). The K systems (20) for k,. = 1 ... ,K are
interpreted then as the conditions for the simulated values
- = 1..... Nto reproduce the K indicator correlogram
models p(h; z4 ), k0 = 1 .
,K (Alabert, 1987).

Dimension reduction
In practice, at each node u, K normal systems of type (20)
must be solved to generate a K-discrete version of the
conditional probability distribution P{Z(u) _- :, I(n)).
k,
= 1..
,K. The factor K is not a problem if the sv-.tems
(20) are not of too large dimension n. But precisely, in the
sequential simulation process over the N nodes u;.1 =
1..
,N, the number of conditioning data keeps growing
from ii for the first node being simulated to n+N- l for the
last node. A grid with N = 10' nodes is not extraordinary
in Earth Sciences applications and thus would require
the
solving of unreasonably large systems.
The solution consists of retaining only those data loca
tions u., closest to the node u. being simulated. The argu
ment is that the information carried bv the further away
conditioning data is 'screened' by the information content
of the nearest data. For example, if the five nearest data are
retained in each of four quadrants with apex at the node u
being simulated, a normal system of maximum dimension
20 would have to be solved for each node, no matter
how
advanced is the sequential process ! =
1,

REVIEW

;
Conditioning to soft information

In many applications, particularly in the Earth Sciences,
'hard' numerical data of type (14): Z(u.) = z. are sparse.
The reconstitution of the spatial distribution {:(u), u E S} is
thus likely to be poor. Fortunately, hard data are usually
complemented by a host of 'soft' data stemming from
different and less accurate measurement devices or from
interpretative origin (e.g. geological interpretation). The
reconstitution can be dramatically improved if that soft in
formation is accounted for.
The indicator formalism offers great flexibility for coding
information of various sources and qualities into a common
format; that of zero/one bits (indicator data):

"* A hard datum Z(u.) = z. generates a complete indicator
data column as in expression (14).

"* A constraint interval generates an incomplete indicator
data column:

r0, ifZk <a
Z(u.) E [a, b] = i(u ; z) =., 1, if :k -_b
0

One simulated field using that additional soft infor
mation is shown in Fig. 13, and should be compared to the
simulated fields of Figs 3(b) and 5 which use only the 16
hard data. The improvement achieved by the incorpor
ation of the soft information is appreciable: compare Figs 5
and 13 to the reference Fig. 1.
In addition to allowing a coding of soft local information,
the indicator formalism lends itself to utilizing soft infor
mation of a structural nature. Indeed, the indicator simu
lation approach utilizes K connectivity functions p,(h; Zk).
k = 1.. ,K, as opposed to a single one py(h) for the Gaus
sian approach. Some aspects, such as spatial anisotropy, of
these connectivity functions may be obtained directly from
geological (soft structural) information. For example, it
may be known beforehand that low permeability values
are connected along N57W whereas high permeability
values are connected in the orthogonal direction N33'E.
Soft and Hard data locations

(21)

.+,0
0

Lundefined otherwise.

+ 0

0

0

* Local prior distributions are coded as 'fuzzy' indicator
data valued between 0 and 1, as opposed to 'hard' indi
cator data valued either zero or 1:

0

:+

0

A

o
0

0

0
0

0
0

* 40
0

0

1], if :. E [a. b]

1,
Zk <-- ab .
0, ifif Zk

(22)

For example, at a location u. where a hard measurement
-(u,) could not be taken, information about the prevailing
rock type may be available. That rock type information may
indicate in which interval [a, b] the attribute value z(u,,)
resides. In addition the distribution (histogram) of z-values
within that rock type may be available and that information
can be coded as a prior local distribution of type (22) and
put to use.
The indicator data regardless of their origins, are then
processed all together for:

"* Inference of the required indicator correlograms pI(h; zk),
k=1 ...

o

0

0

0

0

0
0

a4a

@0.0

--

4.
0

*-A-- hard information

0
04.0

0

a

0

a

a

0

a

0

a

0

0

0

0

0

+.

0

0

0

soft information

0

0

a@0

0

[0,

0

0

I

Z(uj) E [a, b] = i(u0 ; :Z =

0

0

o
0

0

a

o+o

0

0L
0

+

East ->

Fig. 12. Location of available information. + 16 hard data, C 144
constraint intervals (soft data).

Simulation with soft data
mOarcy

C 76-111.5

K.

"* Conditioning the probability distribution (13) required

68-75
6

El

for generating the simulated maps.
In addition to the 16 hard data, 144 constraint intervals of
type (21) were given at the nodes of a regular 12x 12 grid,
(see the location map of Fig. 12). At each of these 144
locations soft information indicates that the permeability
value z(u,,) belongs to one of the three intervals [0, 35.5],
[35.5, 76], [76, 150]. This soft information mimicks the
imprecise permeability information one might be able to
extract from well logs or, in the best case, from dense
seismic lines.

s8-68

*55-58
51.55
47-51
U42.3•47
M

19.5. 42.5

Fig. 13. A reconstitution using both hard and soft information
(compare with Figs 3b and 5)

133

imim
S2REVIEW
"Theanisotrop% modelled for p;(h; z, 1)at the low threshold
value .mav then be inverted for the model p1(h; :,..) at
the high threshold value' : .. A resulting simulated field
with this inverted anisotropv has been given in Fig. 4, to be
compared with the realization of Fig. 3(b); in both cases the
same 16 hard data were used.
CONCLUSIONS
In the Earth Sciences, where data are typicall]\ scarce,
statistics are often taken towards a goal of data expansion.
Models of dependence in space are built from sample
statistics and allow expansion of the original information
into a detailed picture of the population of interest. The
reconstructed maps or numerical models are considered
good to the extent that they' reproduce certain characteris
tics of the spatial distribution of the underlying population.
Important characteristics to be honoured include:

"* the sample values and qualitative information at their
locations,

"* patterns of connectivity in space, part:-ularly those of
extreme-valued attributes.

Geological mapping is aimed at detecting and enhancing
low entropy (organized) patterns of spatial connectivity,
wýhereas traditional interpolation techniques tend to
smooth out extreme values, and statistical models lean
towards maximum entropy (maximum disorganization)
whenever a doubt arises. Yet, strings or patches of con
nected extreme values might be the most consequential
features of the map depending on the intended usage of
that map.
Spatial connectivitv models defined as probabilities for
any two attribute values, separated by a vector h, to jointly
exceed any given threshold value, can be inferred and
imposed on the reconstitution exercise. The resulting maps
will reflect these connectiv'itv models, in addition to hon
ouring local information of any type. These connectivity
functions can be in number K > 1, and can image patterns
of low values different from patterns of median or high
values, they can be inferred from hard data or synthesized
from soft interpretative information. Thus, they allow the
generation of maps with a greater and more valuable infor
mation content, maps that better reflect the actual popula
tion distribution.
Typically in the Earth Sciences, hard numerical data are
scarce and the reconstitution exercise must rely on soft
information of various sources. The indicator formalism
calls for a common coding of all information available at
any location into a series of elementary bits (0 or 1). These
bits of information are then processed together, indepen
dently of their origins. The results are a series of equiprob
able maps honouring all prior intormation, whether hard
or soft. The differences between these equiprobable maps
image the prevailing spatial uncertainty. That stochastic
134

imaging of spatial uncertainty can be put to different uses,
including the assessment of consequent risks and the need
for additional information.
REFERENCES
Abramovitz M. and Stegun 1. (1964) Handbook of ,atheniatcal :I:(
fio,,s Dover PubI., 104b pp.
Alabert F. (1987) Stochastic imaging of spatial distributions using

hard and soft information, MSc thesis, Branner Earth Sciences.
Library, Stanford Univ., pp. 169.
Anderson T. (1958) An introdiuctionto ,ultwariate

statistic

mu ,,

Wiley & Sons, 374 pp.
Devrove L. (1986) Non-Uniforni random variate generatios:, Spnnger

Verlag, 843 pp.
Giordano R., Salter S. and Mohantv K. (1985) The eftecs o( permc
!'iithtyijrtiaiitt,,ns)i 0n
Ioz
n porous niedia, SPE pai;,r 14365, tOlth SPE
Annual Conf., Las Vegas.
Goldberger A. (1962) Best linear unbiased prediction in the
generalized linear regression model, ]. An: stat. Aq,
57,
3o9-375.
Haralick R.M., Shanmugam K. and Dinstein 1. (1973) Textural
features for image classification, IEEE Trans, Nov 1973.
610-621.
Jones D. (1979) Elementaru infornation theorv. Clarendon Press,
Oxford, pp. 137.
Journel A. (1974) Geostatistics for conditional simulation of
orebodies, Ecomi. Geol.. 69, 673-80.
Joumel A.G. (1983) Non parametric estimation of spatial distn
butions, .Math Geol.. 15, 445-468.
Journel A. (1986) Constrained interpolation and soft kriging. In
Prec. 79th APCOM Syrup. Soc. of Min. Eng., Littleton, CO
pp. 15-30.
lournel A. and Hui*',egts Ch. (1978) Mumnggeoatistits , Chapter
;, Academic Press. 600 pp.
Sullivan J. (1985) Conditional recovery estimation through prob
ability kriging: Theory and practice. In: Geostatishcs for atiuraa! ,c
sources characterization led. bv G.Verlv, M. David. A. Jourel and
A. Marechel), Reidet, Dordrecht, pp. 365-384.
Suro Perez V. (1988) Indicator kriging based on pnncipal compo
nent analysis, MS- thesis, Branner Earth Sciences Library, Stan
ford Univ., 168 pp.
Switzer P. (1977) Estimation of spatial distributions from po,nt
sources with application to air pollution measurement. Bidl. ;Wt.
Stat. JIst., 47, 123-137.
Thomas J. (1986) Introduction to poobabiliei.. Springer-Verlag, 247 pp.
Xiao H. (1985) A description of the behavior of indicator vario

grams for a bivariate normal distribution, MS- thehcsis, Branner
Earth Sciences Library, Stanford Univ., 51 pp.
Received 6th Decenber 1988

